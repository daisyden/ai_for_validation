


### Merged Result:1551{"issue_number": 1551, "issue_description": "The operator 'symm_mem::fused_scaled_matmul_reduce_scatter' is not currently implemented for the XPU device. This error occurs in the following test cases: test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_0, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_1, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_0, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_1, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_2, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_0, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_1, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_2. The error is raised when running the test cases on the XPU device.", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1550{"issue_number": 1550, "issue_description": "The operator 'aten::_scaled_mm.out' is not currently implemented for the XPU device. This error occurs in the following test cases: test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_1_return_A_False, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_1_return_A_True, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_2_return_A_False, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_2_return_A_True.", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1549{"issue_number": 1549, "issue_description": "AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n......'", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1548{"issue_number": 1548, "issue_description": "AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [2_inference]\n......'", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1547{"issue_number": 1547, "issue_description": "The operator 'symm_mem::fused_matmul_reduce_scatter' is not currently implemented for the XPU device", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1545{"issue_number": 1545, "issue_description": "import torchvision\n\n*** RuntimeError: register_fake(...): the operator torchvision::nms already has an DispatchKey::Meta implementation via a pre-existing torch.library or TORCH_LIBRARY registration. Please either remove that registration or don't call register_fake.", "reporter": "githubsgi", "assignee": "", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1543{"issue_number": 1543, "issue_description": "In summary, during fine-tuning process, XPU reserved 8GB VRAM more than CUDA. \n\nReproduce steps:\n1. git clone https://github.com/songhappy/torchtune/tree/debug_memory\n2. install the library, prepare the datasets/models as [instructions](https://pytorch.org/torchtune/main/install.html)\n3. \n```\ntune run lora_dpo_single_device --config recipes/configs/llama3_1/8B_lora_dpo_single_device_my.yaml device=xpu profiler.enabled=False max_steps_per_epoch=1 2>&1\n```", "reporter": "airMeng", "assignee": "guangyey", "resolution": "", "root_cause": "The extra 8GB allocation is recorded but not reasonable. The XPU allocator is modified to record each time of the memory allocation, but the recorded memory allocation is not reasonable. The extra memory allocation is not due to the model or the data, but due to the XPU allocator itself.", "state": "open"}


### Merged Result:1537{"issue_number": 1537, "issue_description": "With 2025.0 got accuracy gap when check optimizer state dict, PVC 1100, XELINK, 2 ranks.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "open"}



### Merged Result:1535{"issue_number": 1535, "issue_description": "error\uff1aRuntimeError: Process 0 terminated or timed out after 300.09047198295593 seconds\ncases:\ntest/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_gather_object_cpu\ntest/distributed/test_c10d_object_collectives.py::TestObjectCollectivesCPU::test_scatter_object_list_cpu\ntest/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_gather_object_xpu\ntest/distributed/test_c10d_object_collectives.py::TestObjectCollectivesXPU::test_scatter_object_list_xpu\nlog:\n_________________________________ TestObjectCollectivesCPU.test_gather_object_cpu __________________________________\nTraceback (most recent call last):\n  File \"/home/sdp/penghuic/pytorch/torch/testing/_internal/common_distributed.py\", line 632, in wrapper\n    self._join_processes(fn)\n  File \"/home/sdp/penghuic/pytorch/torch/testing/_internal/common_distributed.py\", line 872, in _join_processes\n    self._check_return_codes(elapsed_time)\n  File \"/home/sdp/penghuic/pytorch/torch/testing/_internal/common_distributed.py\", line 926, in _check_return_codes\n    raise RuntimeError\nRuntimeError: Process 0 terminated or timed out after 300.09047198295593 seconds\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1535. The reporter of the issue is PenghuiCheng, and the assignee is ratnampa, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "ratnampa", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:1533{"issue_number": 1533, "issue_description": "pytorch build got permission issue for windows", "reporter": "mengfei25", "assignee": "chunhuanMeng", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1532{"issue_number": 1532, "issue_description": "The operator 'torchvision::deform_conv2d' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1532. The reporter of the issue is jerryzhou0624, and the assignee is xytintel, and the state of the issue is open.", "reporter": "jerryzhou0624", "assignee": "xytintel", "resolution": "\n", "root_cause": "The reporter jerryzhou0624 is facing an issue when using the file https://github.com/sczhou/ProPainter/blob/main/model/recurrent_flow_completion.py during the process to forward_bidirect_flow(self, masked_flows_bi, masks). The error message is not provided in the comments. The reporter is using ipex and intel-extension-for-pytorch==2.6.10+xpu. The reporter is not able to find the 2.7 wheel and is suggested to use the nightly wheel instead.", "state": "open"}


### Merged Result:1527{"issue_number": 1527, "issue_description": "torch._dynamo.exc.InternalTorchDynamoError: AttributeError: __enter__\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1527. The reporter of the issue is PenghuiCheng, and the assignee is zhangxiaoli73, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "zhangxiaoli73", "resolution": "\nhttps://github.com/pytorch/pytorch/pull/150405 merged, some of tests still failed with `RuntimeError: UR backend failed. UR backend returns:40 (UR_RESULT_ERROR_OUT_OF_RESOURCES`", "root_cause": "The error message in this ticket is `torch._dynamo.exc.InternalTorchDynamoError: AttributeError: enter`, but the code seems to get some runtime error like ", "state": "closed"}


### Merged Result:1526{"issue_number": 1526, "issue_description": "error: RuntimeError: UR backend failed. UR backend returns:40 (UR_RESULT_ERROR_OUT_OF_RESOURCES)\n\ncases: test_dynamo_distributed.py::TestMultiProc::test_get_pg_attr\ntest/distributed/test_functional_api.py::TestCollectivesWithDistributedBackendXPU::test_tracing_xpu\ntest/distributed/test_functional_api.py::TestDistributedBackendCollectivesWithWorldSize4XPU::test_tracing\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1526. The reporter of the issue is PenghuiCheng, and the assignee is zhangxiaoli73, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "zhangxiaoli73", "resolution": "\n", "root_cause": "Failure only happens in dynamo case", "state": "open"}


### Merged Result:1525{"issue_number": 1525, "issue_description": "ValueError: trying to initialize the default process group twice!\n\nThis error occurs when trying to initialize the default process group twice in the test cases of the test_c10d_functional_native.py file. The error is raised when the test cases are executed, and the error message indicates that the default process group is being initialized more than once. The error is not specific to any particular test case, but it affects multiple test cases in the file. The error is caused by the fact that the process group is being initialized in the setUp method of the test cases, and it is being initialized again in the test cases themselves. This causes the error as the process group can only be initialized once.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1525. The reporter of the issue is PenghuiCheng, and the assignee is Chao1Han, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "\n", "root_cause": "The process group is being initialized in the setUp method of the test cases, and it is being initialized again in the test cases themselves. This causes the error as the process group can only be initialized once.", "state": "open"}


### Merged Result:1521{"issue_number": 1521, "issue_description": "PyTorch Flex Attention  fails on XPU, see below,  on XPU fails with errors, see below. Currently enabling PyTorch Flex Attention, see below,  on XPU fails with errors, see below. error: AssertionError: Torch not compiled with CUDA enabled\nFlexAttention is not enabled on XPU yet. we target to enable it on torch-2.8. The draft pr can be found https://github.com/pytorch/pytorch/pull/143553", "reporter": "githubsgi", "assignee": "liangan1", "resolution": "\n", "root_cause": "FlexAttention is not enabled on XPU yet", "state": "open"}


### Merged Result:1520{"issue_number": 1520, "issue_description": "Expected zero exit code but got -11 for pid. \n\nlog:\n\uff08Please check attachment for detail\uff09\n[c10d_functinal_native_new.log](https://github.com/user-attachments/files/19482576/c10d_functinal_native_new.log)\nAssertionError: Scalars are not equal!\nExpected 0 but got -11.\nAbsolute difference: 11\nRelative difference: inf\nExpected zero exit code but got -11 for pid: 2718941\n\nerror cases:\nFAILED [15.9430s] test_c10d_functional_native.py::TestWithNCCL::test_all_gather_into_tensor_coalesced\nFAILED [16.9512s] test_c10d_functional_native.py::TestWithNCCL::test_all_gather_into_tensor_single\nFAILED [15.6386s] test_c10d_functional_native.py::TestWithNCCL::test_all_reduce_coalesced\nFAILED [16.5396s] test_c10d_functional_native.py::TestWithNCCL::test_all_reduce_coalesced_\nFAILED [16.0305s] test_functional_api.py::TestCollectivesWithDistributedBackendXPU::test_all_gather_into_tensor_coalesced_xpu\nFAILED [16.1296s] test_functional_api.py::TestDistributedBackendCollectivesWithWorldSize4XPU::test_all_gather_into_tensor_coalesced\nThe reporter of the issue is PenghuiCheng, and the assignee is ratnampa, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "ratnampa", "resolution": "\nThe cases passed on the main branch of torch-xpu-ops.", "root_cause": "The reporter PenghuiCheng used oneCCL bound MPI with Intel MPI, and the PyTorch commit is 44d55b9, the oneccl commit is 445b002423aea9be7496c7dfac41750cd562b529, and the Intel MPI version is 2021.15  Build 20250213 (id: d233448).", "state": "closed"}


### Merged Result:1519{"issue_number": 1519, "issue_description": "Ge, Qinling found 2 coredump issues in IPEX2.7, the two cases also failed in Pytroch2.7 without IPEX, please help to check: nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_max_pool_nan_inf_xpu_float32, test_ops_xpu.py::TestCommonXPU::test_dtypes__refs_nn_functional_pdist_xpu", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1518{"issue_number": 1518, "issue_description": "Using nightly build PT2.8, this sample code will return wrong output. The output of scaled_dot_product_attention API is wrong. While using stock stable pytorch 2.6 version, the output is correct.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1518. The reporter of the issue is kaixuanliu, and the assignee is LuFinch, and the state of the issue is open.", "reporter": "kaixuanliu", "assignee": "LuFinch", "resolution": "\n", "root_cause": "The issue seems to be related to a difference between SDPA math & MKL path, and it could be a page fault issue. The reporter and assignee have provided error logs, but the exact root cause has not been determined yet. The error logs show unexpected page faults from the GPU, which might be due to a wrong pointer access. The reporter suggests checking the output of [L766](https://github.com/huggingface/transformers/blob/v4.50.1/src/transformers/models/hubert/modeling_hubert.py#L766) first.", "state": "open"}


### Merged Result:1513{"issue_number": 1513, "issue_description": "We plan to add Inductor UT test into nightly test, but met this issue when adding cases. Github action stage: https://github.com/intel/torch-xpu-ops/blob/refs/heads/ruijie/Inductor_UT/.github/workflows/_linux_ut.yml#L204 Test results: You can check the log: https://github.com/intel/torch-xpu-ops/actions/runs/14032267388", "reporter": "RUIJIEZHONG66166", "assignee": "RUIJIEZHONG66166", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1512{"issue_number": 1512, "issue_description": "first run take long time on windows bmg/arc (not test lnl etc), but proper time on Linux\ncuda takes 6s for first run with 9th gen core i7 + Geforce RTX 2060", "reporter": "ZhaoqiongZ", "assignee": "LuFinch", "resolution": "\n", "root_cause": "cuda driver or runtime version issue", "state": "open"}


### Merged Result:1510{"issue_number": 1510, "issue_description": "Some test cases in test/xpu will be hang, such as test_linspace_xpu_complex128. Once 1 case got failed, all the next will be also failed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1510. The reporter of the issue is mengfei25, and the assignee is Stonepia, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "Stonepia", "resolution": "\nempty_cache", "root_cause": "The issue is related to long-time run tests and the failure of some tests.", "state": "open"}


### Merged Result:1509{"issue_number": 1509, "issue_description": "backward failed.\n\nlog:\nFile \"/home/penghuic/pytorch/test/distributed/test_multi_threaded_pg.py\", line 336, in test_bwd_sees_fwd_pg\n    x.sum().backward()\nRuntimeError: Data corruption detected\n\nreproduce command:\npytest -v test/distributed/test_multi_threaded_pg.py\n\nIn max 1550 device, this case will result in segmentation fault error.", "reporter": "PenghuiCheng", "assignee": "zhangxiaoli73", "resolution": "\n", "root_cause": "Oneccl not  perfect  support multi-thread. So skip it first.", "state": "open"}


### Merged Result:1508{"issue_number": 1508, "issue_description": "torch.ops._c10d_functional.reduce_scatter_tensor_coalesced RuntimeError: result type Float can't be cast to the desired output type Long.\n\nreproduce command:\npytest -vs test/distributed/test_c10d_functional_native.py\n\nerror cases:\ntest/distributed/test_c10d_functional_native.py TestWithNCCL.test_reduce_scatter_tensor_coalesced\ntest/distributed/test_c10d_functional_native.py TestWithNCCL.test_reduce_scatter_tensor_single\nRuntimeError: oneCCL: sycl_coll_base.hpp:517 invoke_scaleout: EXCEPTION: unsupported datatype INT64", "reporter": "PenghuiCheng", "assignee": "ratnampa", "resolution": "\nThe issue is resolved by changing the torch tensor dtype to torch.int32", "root_cause": "INT64 is not currently supported by SYCL collectives", "state": "open"}


### Merged Result:1507{"issue_number": 1507, "issue_description": "OffsetBasedRNGTracker didn't support XPU device. Log: torch/testing/_internal/common_distributed.py:748] RuntimeError: OffsetBasedRNGTracker instantiation requires the presence of CUDA/CUDA-like device. Got xpu instead. reproduce command: python test/distributed/tensor/parallel/test_tp_random_state.py\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1507. The reporter of the issue is PenghuiCheng, and the assignee is , and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "", "resolution": "\nhttps://github.com/pytorch/pytorch/pull/148360 was merged.", "root_cause": "The issue was fixed in https://github.com/pytorch/pytorch/pull/148360", "state": "closed"}


### Merged Result:1506{"issue_number": 1506, "issue_description": "E2E (hf & timm) models got fail_accuracy", "reporter": "libohao1201", "assignee": "", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1505{"issue_number": 1505, "issue_description": "14 Timm models got fail_accuracy on ARC-WSL.\n3 HF models also got fail_accuracy.", "reporter": "libohao1201", "assignee": "", "resolution": "\n", "root_cause": "3 HF models also got fail_accuracy.", "state": "open"}


### Merged Result:1504{"issue_number": 1504, "issue_description": "On PVC 1550 with two tiles we found test/distributed/fsdp have 30% cases failed with accuracy issue. Verified on 1100 machine with ZE_AFFINITY_MASK=0,1 set, the cases also got accuracy failures. The test cases failed with the following errors: 1. Scalars are not close! Expected -0.16682696342468262 but got -0.16529536247253418. Absolute difference: 0.0015316009521484375 (up to 1e-05 allowed). Relative difference: 0.009180775821289282 (up to 1.3e-06 allowed). 2. Scalars are not close! Expected 511442.53125 but got 515376.9375. Absolute difference: 3934.40625 (up to 1e-05 allowed). Relative difference: 0.0076927631348610095 (up to 1.3e-06 allowed). 3. Tensor-likes are not close! Mismatched elements: 528 / 640 (82.5%). Greatest absolute difference: 0.00045857205986976624 at index (1, 45) (up to 1e-05 allowed). Greatest relative difference: 0.20614512264728546 at index (8, 26) (up to 1.3e-06 allowed). 4. Tensor-likes are not close! Mismatched elements: 2 / 2 (100.0%). Greatest absolute difference: 0.04993332177400589 at index (1,) (up to 1e-05 allowed). Greatest relative difference: 0.3449627757072449 at index (1,) (up to 1.3e-06 allowed). 5. Tensor-likes are not close! Mismatched elements: 100 / 100 (100.0%). Greatest absolute difference: 0.0000000000000000 at index (0,) (up to 1e-05 allowed). Greatest relative difference: 0.0000000000000000 at index (0,) (up to 1.3e-06 allowed).\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1504. The reporter of the issue is daisyden, and the assignee is zhangxiaoli73, and the state of the issue is open. The issue title is [distributed] fsdp accuracy gaps with 2 ranks, and the issue body includes the test case results and the cases to be checked.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1504. The reporter of the issue is daisyden, and the assignee is zhangxiaoli73, and the state of the issue is open.", "reporter": "daisyden", "assignee": "zhangxiaoli73", "resolution": "\n\n", "root_cause": "a new regression issue from oneCCL", "state": "open"}


### Merged Result:1503{"issue_number": 1503, "issue_description": "this is the conda list of the conda env which contains intel-sycl-rt etc with this env, if we build from source and activate oneapi with call \"C:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\env\\vars.bat\" call \"C:\\Program Files (x86)\\Intel\\oneAPI\\ocloc\\latest\\env\\vars.bat\" the compilation will trigger redefinition error\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1503. The reporter of the issue is ZhaoqiongZ, and the assignee is , and the state of the issue is open.", "reporter": "ZhaoqiongZ", "assignee": "", "resolution": "\n", "root_cause": "This issue also happens when a user compiles a PyTorch extension.", "state": "open"}


### Merged Result:1502{"issue_number": 1502, "issue_description": "WSL will crash when running torchbench. cp torch-xpu-ops/.github/scripts/inductor_xpu_test.sh pytorch cd pytorch # change iterations (-n) to 20 in inductor_xpu_test.sh bash inductor_xpu_test.sh torchbench float32 inference accuracy xpu 0 static 1 0 basic_gnn_gin", "reporter": "libohao1201", "assignee": "", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1500{"issue_number": 1500, "issue_description": "The operator 'aten::_slow_conv2d_forward' is not currently implemented for the XPU device.\nI run into this error when executing above sample code. Will you add support for this OP?", "reporter": "kaixuanliu", "assignee": "", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:1498{"issue_number": 1498, "issue_description": "5 extended uts failed with **RuntimeError: Native API failed. Native API returns: 29 (UR_RESULT_ERROR_INVALID_KERNEL_NAME) **.", "reporter": "libohao1201", "assignee": "gaopengff", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1497{"issue_number": 1497, "issue_description": "RoIAlign autocast test got failed\nThe reporter of the issue is mengfei25, and the assignee is chunhuanMeng, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "chunhuanMeng", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:1496{"issue_number": 1496, "issue_description": "When running E2E inductor on LNL, the following error appears randomly: The memory could not be read. This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1496. The reporter of the issue is libohao1201, and the assignee is , and the state of the issue is open.\nThis might be because of the driver that enables overcommit feature. Then the writing becomes invalid. We are still tracking this in GSD-10905 . If there is a fix, we could switch back to re-test this.", "reporter": "libohao1201", "assignee": "", "resolution": "\n", "root_cause": "The driver that enables overcommit feature causes the writing to become invalid.", "state": "open"}


### Merged Result:1483{"issue_number": 1483, "issue_description": "python benchmarks/dynamo/torchbench.py --performance --float16 -d xpu -n10 --inference --only sam --backend=inductor --cold-start-latency\n\nTesting model sam\nloading model: 0it [00:06, ?it/s]\nxpu  eval  sam\nIn file included from /usr/include/string.h:535,\n                 from /usr/include/c++/11/cstring:42,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/detail/string.hpp:9,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/exception.hpp:16,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/detail/array.hpp:25,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/buffer.hpp:13,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/accessor.hpp:15,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/detail/core.hpp:21,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/sycl.hpp:25,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/lib/python3.10/site-packages/triton/backends/intel/include/sycl_functions.h:16,\n                 from /tmp/tmpr0wjdmws/main.cpp:23:\nIn function \u2018char* strncat(char*, const char*, size_t)\u2019,\n    inlined from \u2018PyObject* load_binary(PyObject*)\u2019 at /tmp/tmpr0wjdmws/main.cpp:291:12:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:138:34: warning: \u2018char* __builtin___strncat_chk(char*, const char*, long unsigned int, long unsigned int)\u2019 specified bound depends on the length of the source argument [-Wstringop-overflow=]\n  138 |   return __builtin___strncat_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n  139 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nIn file included from /usr/include/c++/11/ios:40,\n                 from /usr/include/c++/11/ostream:38,\n                 from /usr/include/c++/11/iostream:39,\n                 from /tmp/tmpr0wjdmws/main.cpp:10:\n/usr/include/c++/11/bits/char_traits.h: In function \u2018PyObject* load_binary(PyObject*)\u2019:\n/usr/include/c++/11/bits/char_traits.h:399:32: note: length computed here\n  399 |         return __builtin_strlen(__s);\n      |                ~~~~~~~~~~~~~~~~^~~~~\nSegmentation fault from GPU at 0xffc00000ffc0a000, ctx_id: 1 (CCS) type: 0 (NotPresent), level: 4 (PML5), access: 0 (Read), banned: 1, aborting.\nSegmentation fault from GPU at 0xffc00000ffc0a000, ctx_id: 1 (CCS) type: 0 (NotPresent), level: 4 (PML5), access: 0 (Read), banned: 1, aborting.\nAbort was called at 269 line in file:\n./shared/source/os_interface/linux/drm_neo.cpp\ninductor_xpu_test.sh: line 62: 1234146 Aborted                 (core dumped) \nsam_fast got same issue, and they are passed on CUDA", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\n", "root_cause": "sdpa", "state": "open"}


### Merged Result:1480{"issue_number": 1480, "issue_description": "SDPBackend.FLASH_ATTENTION , SDPBackend.EFFICIENT_ATTENTION are missing and should be added a quickly as possible.\nThe reporter of the issue is githubsgi, and the assignee is LuFinch, and the state of the issue is open.", "reporter": "githubsgi", "assignee": "LuFinch", "resolution": "\n", "root_cause": "The backward pass is still missing.", "state": "open"}


### Merged Result:1478{"issue_number": 1478, "issue_description": "When adding pytorch/test/test_xpu.py in torch-xpu-ops windows CI, we found these failure: FAILED [1.7263s] test_xpu.py::TestXpuXPU::test_lazy_init_xpu - subprocess.CalledProcessError: Command '['C:\\Users\\Devcloud\\.conda\\envs\\windows_ci\\python.exe', 'C:\\Users\\Devcloud\\.conda\\envs\\windows_ci\\lib\\site-packages\\torch_xpu\\test\\test_xpu.py']' FAILED [0.0043s] test_xpu.py::TestXpuXPU::test_mem_get_info_xpu - RuntimeError: The device (Intel(R) Arc(TM) Graphics) doesn't support querying the available free memory \u2014\u2014 **known issue: https://github.com/intel/torch-xpu-ops/issues/1384** FAILED [1.8017s] test_xpu.py::TestXpuXPU::test_wrong_xpu_fork_xpu - AssertionError: Regex didn't match: 'Cannot re-initialize XPU in forked subprocess.' not found in 'PYTORCH_API_USAGE'\nAn attempt has been made to start a new process before the current process has finished its bootstrapping phase.", "reporter": "RUIJIEZHONG66166", "assignee": "LuFinch", "resolution": "\nCreate PR\uff1ahttps://github.com/pytorch/pytorch/pull/150520", "root_cause": "The issue is related to the use of fork to start child processes. The solution is to add `if __name__ == '__main__':` for Windows, and to skip the test case on Linux.", "state": "open"}


### Merged Result:1475{"issue_number": 1475, "issue_description": "When do the preci test for the branch daisyden/fsdp_test I found some cases of test_fsdp_core.py got random failures, such as: test_transformer_no_grad_mixed_precision_True_xpu, test_transformer_no_grad_mixed_precision_False_xpu. The failure is caused by the assertion that the exit code of the processes should match, but the exit code of the process 1 is -11 while the exit code of the process 0 is 0. The root cause is that the test code uses sys.exit() to simulate failures and in those cases, we can't have an exit code of 0, but we still want to ensure we didn't run into any other errors.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1475. The reporter of the issue is daisyden, and the assignee is zhangxiaoli73, and the state of the issue is open.", "reporter": "daisyden", "assignee": "zhangxiaoli73", "resolution": "\n", "root_cause": "The test code uses sys.exit() to simulate failures and in those cases, we can't have an exit code of 0, but we still want to ensure we didn't run into any other errors.", "state": "open"}


### Merged Result:1472{"issue_number": 1472, "issue_description": "We have listed the following cases that need to be optimized at the operator level.\nAt the practical level, We need to use a specific shape and stride to track their performance status.\n\n- [ ] BatchNorm\n- [ ] GroupNorm\n- [ ] LayerNorm", "reporter": "xytintel", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1468{"issue_number": 1468, "issue_description": "With oneAPI 2025.1 int16/int32/int64 argmin result is incorrect, and int8 does not have the issue. pytest -o cache_dir=F:\\daisy_cache  -v .\\test_ops_xpu.py -k test_compare_cpu_argmin_xpu_int64 With following inputs p sample.input tensor([[[[-4,  8]], \n         [[-3, -9]]], \n         \n        **[[[ 7,  1]],** \n         [[ 5,  9]]], \n         \n        [[[-6, -3]], \n         [[-6, -3]]]], device='xpu:0') p op.name 'argmin' p sample.args () p sample.kwargs {'dim': -1, 'keepdim': False} XPU result is incorrect p cuda_results tensor([[[0], \n         [1]], \n        **[[0],** \n         [0]], \n        [[0], \n         [0]]], device='xpu:0') p cpu_results tensor([[[0], \n         [1]], \n        **[[1],** \n         [0]], \n        [[0], \n         [0]]])", "reporter": "daisyden", "assignee": "Stonepia", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1465{"issue_number": 1465, "issue_description": "RuntimeError: Non-uniform work-groups are not supported by the target device", "reporter": "daisyden", "assignee": "", "resolution": "", "root_cause": "This issue is related to the non-uniform work-groups support in the target device. The reporter has reported this issue on github and it is still open.", "state": "open"}


### Merged Result:1461{"issue_number": 1461, "issue_description": "When to build the torch xpu ops in a isolated python virtual environment, it failed because it uses a different python. The pytorch root cmake is using the `Python_EXECUTABLE` ([Here](https://github.com/pytorch/pytorch/blob/420a9be743f8dd5d6296a32a1351c1baced12f1f/tools/setup_helpers/cmake.py#L310)) but the xpu cmake is using a different macro `PYTHON_EXECUTABLE`. After I quick change it to align it with the torch naming, the issue fixed.", "reporter": "chengjunlu", "assignee": "", "resolution": "After I quick change it to align it with the torch naming, the issue fixed.", "root_cause": "The pytorch root cmake is using the `Python_EXECUTABLE` ([Here](https://github.com/pytorch/pytorch/blob/420a9be743f8dd5d6296a32a1351c1baced12f1f/tools/setup_helpers/cmake.py#L310)) but the xpu cmake is using a different macro `PYTHON_EXECUTABLE`.", "state": "closed"}




### Merged Result:1453{"issue_number": 1453, "issue_description": "The BMG machine will crash when running hunggingface performance mode. The issue will go when adding parameter --batch-size=2. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is open.\nThe model crashes when using the same batch size with accuracy mode.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\n", "root_cause": "The system crashes due to memory issues when the model is too large to fit into the dedicated GPU memory. The shared GPU memory is taken into use, and the model still could run. However, after a few models, the memory is not correctly released, leading to the crash.", "state": "open"}


### Merged Result:1444{"issue_number": 1444, "issue_description": "FlexAttention Accuracy issues during running FlexDecoding UT", "reporter": "hoshibara", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1438{"issue_number": 1438, "issue_description": "On LNL and BMG, the xpu.memory_stats() have no output:\n\n```Python\n>>>  start_mem = torch.xpu.memory_stats()\n>>> for k,v in start_mem.items():\n...     print(k, v)\n...\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1438. The reporter of the issue is Stonepia, and the assignee is LuFinch, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "LuFinch", "resolution": "\nClose it as unnecessary.", "root_cause": "The reproducer is wrong. PVC print empty too. Even CUDA prints empty after change `xpu` to `cuda`.", "state": "closed"}


### Merged Result:1437{"issue_number": 1437, "issue_description": "These cases failed on PVC CI test_meta_xpu.py::TestMetaXPU::test_dispatch_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_bfloat16 test_meta_xpu.py::TestMetaXPU::test_dispatch_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_float16 test_meta_xpu.py::TestMetaXPU::test_dispatch_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_float32 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_all_strides_nn_functional_max_unpool3d_grad_xpu_float32 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_all_strides_nn_functional_max_unpool3d_xpu_float32 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_all_strides_nn_functional_scaled_dot_product_attention_xpu_float32 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_bfloat16 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_float16 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_float32\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1437. The reporter of the issue is daisyden, and the assignee is LuFinch, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "LuFinch", "resolution": "\nfixed", "root_cause": "The reporter of the issue is daisyden, and the assignee is LuFinch, and the state of the issue is closed.", "state": "closed"}


### Merged Result:1432{"issue_number": 1432, "issue_description": "SDPA cases failed after XPU enabled in stock pytorch, see the CI failures in https://github.com/intel/torch-xpu-ops/actions/runs/13645060798/job/38146067831. The failures are in the following tests: test_transformers_xpu.py::TestTransformersXPU::test_multiheadattention_fastpath_attn_mask_attn_mask_dim_2_key_padding_mask_dim_2_bool_xpu, test_transformers_xpu.py::TestTransformersXPU::test_multiheadattention_fastpath_attn_mask_attn_mask_dim_3_key_padding_mask_dim_2_bool_xpu, test_transformers_xpu.py::TestTransformersXPU::test_transformerencoder_fastpath_use_torchscript_False_enable_nested_tensor_False_use_autocast_False_d_model_12_xpu, test_transformers_xpu.py::TestTransformersXPU::test_transformerencoder_fastpath_use_torchscript_False_enable_nested_tensor_False_use_autocast_True_d_model_12_xpu, test_transformers_xpu.py::TestTransformersXPU::test_transformerencoder_fastpath_use_torchscript_False_enable_nested_tensor_True_use_autocast_False_d_model_12_xpu, test_transformers_xpu.py::TestTransformersXPU::test_transformerencoder_fastpath_use_torchscript_False_enable_nested_tensor_True_use_autocast_True_d_model_12_xpu. The root cause is that the pytorch commit c21dc11a179b2714509cd901483138adacdce212 introduced a bug in the scaled_dot_product_attention function, which caused the output shape to be incorrect.\nSDPA outputs NaN for fully masked rows.", "reporter": "daisyden", "assignee": "LuFinch", "resolution": "\nOneDNN support", "root_cause": "The pytorch commit c21dc11a179b2714509cd901483138adacdce212 introduced a bug in the scaled_dot_product_attention function, which caused the output shape to be incorrect.", "state": "open"}


### Merged Result:1431{"issue_number": 1431, "issue_description": "import torch\nnt = torch.nested.nested_tensor([], device=\"xpu\")\nempty = torch.nested.to_padded_tensor(nt, 4)\nRuntimeError: to_padded_tensor: at least one constituent tensor should have non-zero numel", "reporter": "weishi-deng", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "open"}




### Merged Result:1428{"issue_number": 1428, "issue_description": "import torch\n\n\ndef test_quantize_per_channel(dtype=torch.float):\n    src_cpu = torch.randn(1, 3, 2, 2)\n    src_gpu = src_cpu.to(\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1428. The reporter of the issue is weishi-deng, and the assignee is yucai-intel, and the state of the issue is open.", "reporter": "weishi-deng", "assignee": "yucai-intel", "resolution": "\n", "root_cause": "The XPU device is not found in checkZeroPoints(). The corresponding PR has been submitted to pytorch. However, after adding the device, the automatically called 'aten::dequantize.self' has not been registered on the QuantizedXPU backend, which needs further implementation.", "state": "open"}


### Merged Result:1426{"issue_number": 1426, "issue_description": "AssertionError: The values for attribute 'shape' do not match: torch.Size([4, 2, 2, 12]) != torch.Size([4, 2, 8, 12]).\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1426. The reporter of the issue is weishi-deng, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "weishi-deng", "assignee": "xytintel", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/1487", "root_cause": "https://github.com/intel/torch-xpu-ops/blob/main/test/xpu/test_native_mha_xpu.py", "state": "closed"}


### Merged Result:1423{"issue_number": 1423, "issue_description": "binary add become slower than 2.6\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1423. The reporter of the issue is jianyizh, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "jianyizh", "assignee": "xytintel", "resolution": "\n", "root_cause": "Register File Size Per Thread increased from 128 to 256", "state": "closed"}


### Merged Result:1422{"issue_number": 1422, "issue_description": "When building PyTorch without sourcing MKL, the PyTorch can't find the one in conda env, so that cause LAPACK support failed. Those commands will have the following output when built: -- LAPACK requires BLAS -- Cannot find a library with LAPACK API. Not using LAPACK. --   USE_BLAS              : 0 --   USE_LAPACK            : 0 So when testing, it will throw the following error: svd: LAPACK library not found in compilation. A workaround is to source oneMKL before the build. This is what current CI is doing. With this, the LAPACK could be successfully included.\nIt's not our expected behavior, right? @CuiYifeng. Firstly, we don't enable the USE_ONEMKL=1, it shouldn't use oneMKL. Secondly, if we set USE_ONEMKL=1, and we source the compiler, we don't need source mkl any more.", "reporter": "Stonepia", "assignee": "CuiYifeng", "resolution": "\nThe issue is resolved by adding the following line to the build environment: `if defined CMAKE_PREFIX_PATH (set CMAKE_PREFIX_PATH=%CONDA_PREFIX%\nLibrary;%CMAKE_PREFIX_PATH%) else (set CMAKE_PREFIX_PATH=%CONDA_PREFIX%\nLibrary)`", "root_cause": "When building PyTorch without sourcing MKL, the PyTorch can't find the one in conda env, so that cause LAPACK support failed.", "state": "open"}


### Merged Result:1401{"issue_number": 1401, "issue_description": "test_weight_norm.py::TestNNMethod::test_weight_norm_different_type, FAILED, E       AssertionError: Tensor-likes are not close!, 1.980\n\nself = <test_weight_norm.TestNNMethod testMethod=test_weight_norm_different_type>\n\ndef test_weight_norm_different_type(self):\n    v = torch.randn(8193  8193).requires_grad_(True)\n    g = torch.randn(8193).to(torch.float).requires_grad_(True)\n    gw = torch.randn(8193  8193)\n    w  n = torch._weight_norm_interface(v  g  dim=0)\n    w.backward(gw)\n    v_xpu = v.detach().clone().to(\"xpu\").requires_grad_(True)\n    g_xpu = g.detach().clone().to(\"xpu\").requires_grad_(True)\n    w_xpu  n_xpu = torch._weight_norm_interface(v_xpu  g_xpu  dim=0)\n    w_xpu.backward(gw.to(\"xpu\"))\n    self.assertEqual(w  w_xpu.cpu()  atol=1e-3  rtol=1e-5)\n    self.assertEqual(n  n_xpu.cpu()  atol=1e-1  rtol=1e-5)\n    self.assertEqual(v.grad  v_xpu.grad.cpu()  atol=1e-3  rtol=1e-5)\n>       self.assertEqual(g.grad  g_xpu.grad.cpu()  atol=1e-3  rtol=1e-5)\nE       AssertionError: Tensor-likes are not close!\nE       \nE       Mismatched elements: 1 / 8193 (0.0%)\nE       Greatest absolute difference: 0.5634427070617676 at index (3813 ) (up to 0.001 allowed)\nE       Greatest relative difference: 2.646775960922241 at index (3813 ) (up to 1e-05 allowed)\nE       \nE       To execute this test  run the following from the base repo dir:\nE           python test_weight_norm.py TestNNMethod.test_weight_norm_different_type\nE       \nE       This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0", "reporter": "huaiyuzh", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "open"}




### Merged Result:1399{"issue_number": 1399, "issue_description": "According to 25ww07 weekly test, we check the torch2.6 found some ops have perf regression compared with 25ww06 weekly results. [loss.soft_margin_loss] and [loss.soft_margin_loss_backward] are the ops that have perf regression. The reporter of the issue is huaiyuzh, and the assignee is xytintel, and the state of the issue is open. The reporter found this regression and file an issue to track this perf regression on torch-xpu-ops. The issue was found by Ruijie. Due to the size limit of the attachment, I only posted part of the data. If you need more data or information, please contact Ruijie or me.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1399. The reporter of the issue is huaiyuzh, and the assignee is xytintel, and the state of the issue is open.", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "\n", "root_cause": "", "state": "open"}




### Merged Result:1391{"issue_number": 1391, "issue_description": "python benchmarks/dynamo/torchbench.py --accuracy --float32 -d xpu -n10 --inference --only moondream --backend=inductor\n\neager_two_runs_differ\n\n\nThe reporter of the issue is kaileiyx, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "kaileiyx", "assignee": "jianyizh", "resolution": "\nThe issue was resolved as the latest test and local test both passed.", "root_cause": "", "state": "closed"}


### Merged Result:1390{"issue_number": 1390, "issue_description": "DebertaForQuestionAnswering amp_bf16/amp_fp16 training/inference accuracy/performance got failed, the error message is RuntimeError: value cannot be converted to type at::BFloat16 without overflow\nThe test PASSED on latest PyTorch.", "reporter": "kaileiyx", "assignee": "etaf", "resolution": "\nThe test PASSED on latest PyTorch.", "root_cause": "", "state": "closed"}


### Merged Result:1389{"issue_number": 1389, "issue_description": "DebertaForMaskedLM  amp_bf16/amp_fp16  training/inference accuracy/performance got failed, the error message is RuntimeError: value cannot be converted to type at::BFloat16 without overflow\nThe test PASSED on latest PyTorch.", "reporter": "kaileiyx", "assignee": "etaf", "resolution": "\nThe test PASSED on latest PyTorch.", "root_cause": "", "state": "closed"}


### Merged Result:1385{"issue_number": 1385, "issue_description": "Most of E2E models failed with  torch._inductor.exc.InductorError: RuntimeError: Triton Error [ZE]: 0x78000011 on BMG windows.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1385. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\n", "root_cause": "A bug in the driver", "state": "closed"}


### Merged Result:1384{"issue_number": 1384, "issue_description": "On integrated platforms (like LNL, MTL), the following test will failed:\n\n```Python\n>>> import torch\n>>> torch.xpu.mem_get_info()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Users\\sdp\\miniforge3\\envs\\tongsu_stock_pt\\lib\\site-packages\\torch\\xpu\\memory.py\", line 194, in mem_get_info\n    return torch._C._xpu_getMemoryInfo(device)\nRuntimeError: The device does not have the ext_intel_free_memory aspect\n>>> torch.version.xpu\n'20250000' \n```\nThe root cause of this issue is that the driver does not support it now. See the GSD-10758 for the internal track. The UR_DEVICE_INFO_GLOBAL_MEM_FREE needs device modules from Sysman being reported which are not, hence not supported return codes from UR. UR detects 0 modules being reported and returns this error.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nDuplicate with #1352", "root_cause": "The device does not have the ext_intel_free_memory aspect", "state": "closed"}


### Merged Result:1382{"issue_number": 1382, "issue_description": "_scaled_dot_product_attention_math caused test_transformer.py::TestTorchMethod::test_transformerencoderlayer\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1382. The reporter of the issue is huaiyuzh, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:1381{"issue_number": 1381, "issue_description": "molan performance regression up to 76%, which is caused by pad_sequence and gru.input.", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1380{"issue_number": 1380, "issue_description": "Sort has performance regression in model pointnet-atlas(~5% on single tile and ~10% on scaling up)", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1354{"issue_number": 1354, "issue_description": "Bfloat16 GroupNorm 4x slower than fp32\nGroupNorm with vectorization enabled", "reporter": "jianyizh", "assignee": "xytintel", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/1357", "root_cause": "bf16 enter the vectorize kernel, fp32 not", "state": "closed"}


### Merged Result:1352{"issue_number": 1352, "issue_description": "The device does not have the ext_intel_free_memory aspect. This issue only happens on iGPU on Windows. It could pass on BMG. The error message is The device does not have the ext_intel_free_memory aspect. The version of torch-xpu is 20250000.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1352. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is open.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nThe root cause of this issue is that the driver does not support it now. See the **GSD-10758** for the internal track.", "root_cause": "UR_DEVICE_INFO_GLOBAL_MEM_FREE needs device modules from Sysman being reported which are not, hence not supported return codes from UR. UR detects 0 modules being reported and returns this error.", "state": "open"}


### Merged Result:1350{"issue_number": 1350, "issue_description": "Tested on Windows with nightly wheel UT will be hung\nPASSED with PyTorch: 2.7.0.dev20250310+xpu / cdb42bd8cc05bef0ec9b682b274c2acb273f2d62", "reporter": "mengfei25", "assignee": "chunhuanMeng", "resolution": "\nPASSED", "root_cause": "", "state": "closed"}


### Merged Result:1347{"issue_number": 1347, "issue_description": "The operator torch_ipex::prepare_4d_causal_attention_mask is currently not implemented for Intel GPUs (XPU) in the Intel Extension for PyTorch (IPEX). This limitation leads to fallbacks to the CPU, resulting in performance degradation during model inference. The operator is not implemented for XPU, and fallbacks to the CPU are used, leading to performance degradation. The issue has been identified in the context of the following context: NotImplementedError: The operator 'torch_ipex::prepare_4d_causal_attention_mask' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable PYTORCH_ENABLE_XPU_FALLBACK=1 to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1347. The reporter of the issue is tcconnally, and the assignee is , and the state of the issue is closed.", "reporter": "tcconnally", "assignee": "", "resolution": "closed\n", "root_cause": "The operator torch_ipex::prepare_4d_causal_attention_mask is not implemented for XPU in the Intel Extension for PyTorch (IPEX). This leads to fallbacks to the CPU, resulting in performance degradation during model inference.", "state": "closed"}


### Merged Result:1343{"issue_number": 1343, "issue_description": "In preci [https://productionresultssa2.blob.core.windows.net/actions-results/2e1df8ff-092e-4984-bfc1-e2198761ddf4/workflow-job-run-b517ebdf-8d4a-5aac-8de3-54a76fd6597d/logs/job/job-logs.txt?rsct=text%2Fplain&se=2025-02-07T05%3A45%3A32Z&sig=842FnVtTF71nNRr5nqyoymo2RDpAo6Y589R%2Bg%2B6fvqI%3D&ske=2025-02-07T13%3A37%3A06Z&skoid=ca7593d4-ee42-46cd-af88-8b886a2f84eb&sks=b&skt=2025-02-07T01%3A37%3A06Z&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skv=2025-01-05&sp=r&spr=https&sr=b&st=2025-02-07T05%3A35%3A27Z&sv=2025-01-05](url), we observed some cases are failed and have no test result. 2025-02-06T13:10:41.6500848Z test_nn_xpu.py,test_native_functions_xpu.py,nn/test_init_xpu.py,test_linalg_xpu.py,test_torch_xpu.py,test_comparison_utils_xpu.py,nn/test_convolution_xpu.py have failures. And there are following errors: 2025-02-06T11:37:46.4534724Z ==================================== ERRORS ==================================== 2025-02-06T11:37:46.4535033Z ______ ERROR collecting third_party/torch-xpu-ops/test/xpu/test_nn_xpu.py ______ 2025-02-06T11:37:46.4535311Z Traceback (most recent call last): 2025-02-06T11:37:46.4535716Z   File ", "reporter": "daisyden", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1338{"issue_number": 1338, "issue_description": "erfcx_xpu and ndtri_xpu cause an IPEX UT fail. In IPEX2.6, we override this Ops with IPEX implementation to make this UT pass.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1338. The reporter of the issue is huaiyuzh, and the assignee is chunhuanMeng, and the state of the issue is closed.", "reporter": "huaiyuzh", "assignee": "chunhuanMeng", "resolution": "closed\nThis is because ipex adds support for BFloat16 data type for these two ops, but torch-xpu-ops does not have this support, and stock pytorch also does not have this support. Therefore, if you really need it, you can raise a PR in pytorch, and then torch-xpu-ops will make corresponding changes. We ensure that our code is consistent with the design of stock pytorch.", "root_cause": "The reporter of the issue is huaiyuzh, and the assignee is chunhuanMeng, and the state of the issue is closed.", "state": "closed"}


### Merged Result:1337{"issue_number": 1337, "issue_description": "fractional_max_pool2d and fractional_max_pool3d cause an IPEX UT fail.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1337. The reporter of the issue is huaiyuzh, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "In IPEX2.6, we override this Ops with IPEX implementation to make this UT pass.\nI have fixed the issue by updating the case in IPEX2.7. We can close it.", "root_cause": "The issue is caused by the fractional_max_pool2d and fractional_max_pool3d operations, which cause an IPEX UT fail. In IPEX2.6, we override this Ops with IPEX implementation to make this UT pass.", "state": "closed"}


### Merged Result:1336{"issue_number": 1336, "issue_description": "index_copy_xpu not implemented for 'Float8_e4m3fn'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1336. The reporter of the issue is huaiyuzh, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "\n", "root_cause": "The issue is closed and the root cause is not explicitly stated in the comments.", "state": "closed"}


### Merged Result:1335{"issue_number": 1335, "issue_description": "CMake will create a very long command for linkage (more than 32767 characters) if all XPU libraries are combined into one `libtorch_xpu.so`. The length of this command is related to the prefix of build path and the number of objects to be linked.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1335. The reporter of the issue is CuiYifeng, and the assignee is CuiYifeng, and the state of the issue is closed.", "reporter": "CuiYifeng", "assignee": "CuiYifeng", "resolution": "\nClose this issue since long command has been bypassed with intermediate libraries #1243.", "root_cause": "The length of the command is related to the prefix of build path and the number of objects to be linked.", "state": "closed"}


### Merged Result:1334{"issue_number": 1334, "issue_description": "timm_regnet BF16 gor fail accuracy recently, the last known good is https://github.com/intel/torch-xpu-ops/commit/b6786e31c36b31bb2cc18e2325451a3198832cb8 + torch a7c2d85\n```\npython benchmarks/dynamo/torchbench.py --accuracy --bfloat16 -d xpu -n10 --training --only timm_regnet --backend=inductor\n```\nxpu  train timm_regnet \nE0204 15:16:48.599000 2195001 site-packages/torch/_dynamo/utils.py:2751] RMSE (res-fp64): 0.01081, (ref-fp64): 0.00091 and shape=torch.Size([]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.001000, use_larger_multiplier_for_smaller_tensor: 0\nfail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1334. The reporter of the issue is mengfei25, and the assignee is jianyizh, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nFall back fp64 to cpu", "root_cause": "fail on pytorch 2.6, can pass on current main 1433bc145526949c84acf4ba5eaa1687cc2d72fe", "state": "open"}


### Merged Result:1332{"issue_number": 1332, "issue_description": "As title, compilation with XPU support fails with the issue below. Compiling CPU succeeds.\n\n```\n/opt/intel/oneapi/compiler/2025.0/bin/compiler/../../include/sycl/detail/builtins/builtins.hpp:235:1: warning: multi-line comment [-Wcomment]\n  235 | // clang++ -[DU]__SYCL_DEVICE_ONLY__ -x c++ math_functions.inc  \n      | ^\nIn file included from /usr/include/c++/12/functional:59,\n                 from /root/pytorch/c10/util/string_view.h:6,\n                 from /root/pytorch/c10/util/StringUtil.h:6,\n                 from /root/pytorch/c10/util/Exception.h:8,\n                 from /root/pytorch/aten/src/ATen/BlasBackend.h:3,\n                 from /root/pytorch/aten/src/ATen/Context.h:3:\n/usr/include/c++/12/bits/std_function.h: In static member function ~@~Xstatic _Res std::_Function_handler<_Res(_ArgTypes ...), _Functor>::_M_invoke(const sstd::_Any_data&, _ArgTypes&& ...) [with _Res = void; _Functor = sycl::_V1::handler::ResetHostKernel<at::native::xpu::VectorizedElementwiseKernel<8, at::native::xpu::SignbitFunctor<c10::BFloat16>, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int> >, sycl::_V1::nd_item<1>, 1>(const at::native::xpu::VectorizedElementwiseKernel<8, at::native::xpu::SignbitFunctor<c10::BFloat16>, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int> >&)::NormalizedKernelType; _ArgTypes = {const sycl::_V1::nd_item<1>&}]~@~Y:\n/usr/include/c++/12/bits/std_function.h:292:7: error: unrecognizable insn:\n  292 |       }\n      | ^\n(insn 21 20 22 4 (set (reg:V2SI 87 [ vect__71.47795 ])\n        (lshiftrt:V2SI (subreg:V2SI (subreg:V2SF (reg:V2SI 118 [ vect__69.47793 ]) 0) 0)\n            (const_int 31 [0x1f]))) \"/usr/include/c++/12/cmath\":662:29 -1\n     (nil))\nduring RTL pass: vregs\n/usr/include/c++/12/bits/std_function.h:292:7: internal compiler error: in extract_insn, at recog.cc:2791\n0x1b3ed3a internal_error(char const*, ...)\n        ???:0\n0x6a22ba fancy_abort(char const*, int, char const*)\n        ???:0\n0x67affc _fatal_insn(char const*, rtx_def const*, char const*, int, char const*)\n        ???:0\n0x67b01e _fatal_insn_not_found(rtx_def const*, char const*, int, char const*)\n        ???:0\nPlease submit a full bug report, with preprocessed source (by using -freport-bug).\nPlease include the complete backtrace with any bug report.\nSee <file:///usr/share/doc/gcc-12/README.Bugs> for instructions.\nCMake Error at torch_xpu_ops_sycl_unary_binary_kernels_generated_UnarySignKernels.cpp.o.Release.cmake:145 (message):\n  Error generating file\n  /root/pytorch/build/caffe2/aten_xpu/src/CMakeFiles/torch_xpu_ops_sycl_unary_binary_kernels.dir/ATen/native/xpu/sycl/./torch_xpu_ops_sycl_unary_binary_kernels_generated_UnarySignKernels.cpp.o\n...\nThe error occurs with the default `-O2` optimization level but not with `-O1`. Further investigation revealed that the error occurs with GCC 12.3 installed via `sudo apt install gcc-12`, but not with GCC 12.4 built from source. Additionally, using `-O2` optimization with the `-fno-tree-loop-vectorize` flag avoids the error. This suggests that the issue is specific to GCC 12.3 and is related to the loop vectorization optimization.", "reporter": "jingxu10", "assignee": "xytintel", "resolution": "\n", "root_cause": "The issue is specific to GCC 12.3 and is related to the loop vectorization optimization.", "state": "open"}


### Merged Result:1331{"issue_number": 1331, "issue_description": "build the pytorch2.6.0 natively with B580. The log shows `ats-m150`\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1331. The reporter of the issue is alanzhai219, and the assignee is , and the state of the issue is closed.", "reporter": "alanzhai219", "assignee": "", "resolution": "\nThis is because AOT support multiple targets. Let's close the issue.", "root_cause": "can specify the target without building others?", "state": "closed"}


### Merged Result:1329{"issue_number": 1329, "issue_description": "The operator 'quantized::linear_dynamic' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1329. The reporter of the issue is gurwinderintel, and the assignee is ZhiweiYan-96, and the state of the issue is open.", "reporter": "gurwinderintel", "assignee": "ZhiweiYan-96", "resolution": "\n", "root_cause": "The quantization operation is still under development and targeted for PT 2.8, the reporter is advised to wait until PT 2.8 is out.", "state": "open"}


### Merged Result:1328{"issue_number": 1328, "issue_description": "The operator 'fsdp::all_gather_copy_in' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators.\nfsdp::all_gather_copy_in not currently implemented for the XPU device\nNo backend type associated with device type xpu\nThe reporter of the issue is saforem2, and the assignee is Chao1Han, and the state of the issue is closed.", "reporter": "saforem2", "assignee": "Chao1Han", "resolution": "\nclosed\nEnvironment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` was set\nclosed", "root_cause": "The operator 'fsdp::all_gather_copy_in' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators.", "state": "closed"}


### Merged Result:1325{"issue_number": 1325, "issue_description": "So far, https://github.com/intel/torch-xpu-ops/pull/526 implemented the first MKL related operator, `aten.fft_c2c`. The PR introduced MKL building system in torch-xpu-ops. The MKL SDK introduced for building bases on oneAPI package. The potential issue is we would recommend using `pip install mkl-dpcpp` for runtime. There would be potential API breaking issue when MKL version in oneAPI package for building has gap with MKL in Pypi. We need to unify recommended MKL package for building and runtime.\nIs there a plan to add other oneMKL APIs such as those from Sparse BLAS in `torch-xpu-ops`? I'm seeing things like #1330 which appear to start adding Sparse CSR support but aren't (or maybe can't yet be?) using oneMKL.", "reporter": "fengyuan14", "assignee": "CuiYifeng", "resolution": "\n", "root_cause": "The potential issue is we would recommend using `pip install mkl-dpcpp` for runtime. There would be potential API breaking issue when MKL version in oneAPI package for building has gap with MKL in Pypi.", "state": "open"}


### Merged Result:1324{"issue_number": 1324, "issue_description": "We found that when running models and the model is OOM, we get the UR Error, and this UR Error will break tensor context. \n\n1. First fill all the GPU memory\n2. Fill another tensor, this is OOM, expected. This x4 should not be created.\n3. Re-access the tensor, it gets UR Error with OUT_OF_RESOURCES. This is unexpected, the tensor context of x1 should be normal.\n4. Re-access, it gets UR Error without any useful information\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1324. The reporter of the issue is Stonepia, and the assignee is guangyey, and the state of the issue is open.", "reporter": "Stonepia", "assignee": "guangyey", "resolution": "\n", "root_cause": "It seems like a driver bug.", "state": "open"}


### Merged Result:1315{"issue_number": 1315, "issue_description": "Without FALLBACK, and aligned the tests with CUDA, we got 693 failures on test_ops.py. I assigned owners for them, @xytintel @CuiYifeng  @ZhiweiYan-96  please have a check and feel free to reassign them. The detailed table is at [link](https://intel-my.sharepoint.com/:x:/p/daisy_deng/EWrUhd2WO2BCj0GOE_NccoQBTDG1OG_LF_dN57GlkWSwUQ?e=Z4L7eF)", "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1305{"issue_number": 1305, "issue_description": "Models got fail accuracy on BMG but passed on PVC\nThe accuracy gap for the models `fbnetv3_b` and `gernet_l` is not yet determined. The root cause of the accuracy gap for `fbnetv3_b` is an unalignment with CUDA's test behavior, where some models are fallback to SGD optimizer. To resolve this, the `fbnetv3_b` model is being adjusted to use the same optimizer behavior as CUDA. The root cause for `gernet_l` is yet to be determined and a PR is being submitted to align with CUDA's optimizer.", "reporter": "mengfei25", "assignee": "Stonepia", "resolution": "\nThe accuracy gap for the models `fbnetv3_b` and `gernet_l` is being addressed. The root cause for `fbnetv3_b` is an unalignment with CUDA's test behavior, and the root cause for `gernet_l` is yet to be determined.", "root_cause": "The root cause for `fbnetv3_b` is an unalignment with CUDA's test behavior, where some models are fallback to SGD optimizer. To resolve this, the `fbnetv3_b` model is being adjusted to use the same optimizer behavior as CUDA. The root cause for `gernet_l` is yet to be determined.", "state": "open"}


### Merged Result:1296{"issue_number": 1296, "issue_description": "Torchbench demucs training got fail accuracy, xpu  train demucs \nE0115 03:33:56.683000 2457704 torch/_dynamo/utils.py:2263] Accuracy failed: allclose not within tol=0\neager_two_runs_differ\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1296. The reporter of the issue is mengfei25, and the assignee is jianyizh, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nput fp64 ref on xpu can solve this issue", "root_cause": "currently cpu and xpu lstm have different implementation", "state": "open"}


### Merged Result:1290{"issue_number": 1290, "issue_description": "Currently, XPU supports `torchvision::nms` and `torchvision::roi_align`, but additional ops in the torchvision repo are not supported. My proposal is to add SYCL kernels for the following ops:\n- [x] `ps_roi_align, roi_pool, ps_roi_pool` #1291 \n- [x] `deform_conv2d` #1362 \n\n### Alternatives\n_No response_\n\n### Additional context\n_No response_", "reporter": "frost-intel", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}




### Merged Result:1279{"issue_number": 1279, "issue_description": "Just compile the pytorch XPU on latest viable/strict (https://github.com/pytorch/pytorch/commit/68dad26b950) will gives the following error. I tried on Windows but I think this bug should apply to all OS.\n\n```\n[136/3592] Generating ../../../xpu/ATen/XPUFunctions.h, ../../../xpu/ATen/RegisterXPU.cpp, ../../../xpu/ATen/RegisterSparseXPU.cpp, D:/pytorch/torch/csrc/inductor/aoti_torch/generated/extend/c_shim_xpu.h, D:/pytorch/torch/csrc/inductor/aoti_torch/generated/extend/c_shim_xpu.cpp\nFAILED: xpu/ATen/XPUFunctions.h xpu/ATen/RegisterXPU.cpp xpu/ATen/RegisterSparseXPU.cpp D:/pytorch/torch/csrc/inductor/aoti_torch/generated/extend/c_shim_xpu.h D:/pytorch/torch/csrc/inductor/aoti_torch/generated/extend/c_shim_xpu.cpp D:/pytorch/build/xpu/ATen/XPUFunctions.h D:/pytorch/build/xpu/ATen/RegisterXPU.cpp D:/pytorch/build/xpu/ATen/RegisterSparseXPU.cpp \nC:\\WINDOWS\\system32\\cmd.exe /C \"cd /D D:\\pytorch && C:\\Users\\Yi\\miniforge3\\envs\\yi\\python.exe -m torchgen.gen --source-path D:/pytorch/third_party/torch-xpu-ops/yaml/ --install-dir D:/pytorch/build/xpu/ATen/ --per-operator-headers --static-dispatch-backend --backend-whitelist XPU SparseXPU --xpu --update-aoti-c-shim --extend-aoti-c-shim --aoti-install-dir=D:/pytorch/torch/csrc/inductor/aoti_torch/generated/extend && type D:\\pytorch\\third_party\\torch-xpu-ops\\src\\ATen\\native\\xpu\\XPUFallback.template >> D:\\pytorch\\build\\xpu\\ATen\\//RegisterXPU.cpp && C:\\Users\\Yi\\miniforge3\\envs\\yi\\python.exe D:/pytorch/third_party/torch-xpu-ops/tools/codegen/remove_headers.py --register_xpu_path D:/pytorch/build/xpu/ATen//RegisterXPU.cpp && C:\\Users\\Yi\\miniforge3\\envs\\yi\\python.exe D:/pytorch/third_party/torch-xpu-ops/tools/codegen/remove_headers.py --register_xpu_path D:/pytorch/build/xpu/ATen//RegisterSparseXPU.cpp\"\nTraceback (most recent call last):\n  File \"D:\\pytorch\\third_party\\torch-xpu-ops\\tools\\codegen\\remove_headers.py\", line 31, in <module>\n    replace_op_headers()\n  File \"D:\\pytorch\\third_party\\torch-xpu-ops\\tools\\codegen\\remove_headers.py\", line 18, in replace_op_headers\n    with open(args.register_xpu_path, 'r') as fr:\nFileNotFoundError: [Errno 2] No such file or directory: 'D:/pytorch/build/xpu/ATen//RegisterSparseXPU.cpp'\n```", "reporter": "DDEle", "assignee": "", "resolution": "", "root_cause": "pytorch/pytorch#144364", "state": "closed"}


### Merged Result:1278{"issue_number": 1278, "issue_description": "Detectron2 inference accuracy got failed, the model detectron2_fasterrcnn_r_101_c4, detectron2_fasterrcnn_r_101_dc5, detectron2_fasterrcnn_r_101_fpn, detectron2_fasterrcnn_r_50_c4, detectron2_fasterrcnn_r_50_dc5, detectron2_fasterrcnn_r_50_fpn, detectron2_maskrcnn_r_101_c4, detectron2_maskrcnn_r_101_fpn, detectron2_maskrcnn_r_50_c4, detectron2_maskrcnn_r_50_fpn all got failed in inference accuracy, the issue is reported by mengfei25 and assigned to jianyizh, the state of the issue is open.\nroi_align_forward_kernel_xpu not implemented for bf16. Is this the regression because previously we fall back to cpu?", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nThe issue is related to the implementation of roi_align_forward_kernel_xpu for bf16. It seems to be a regression as previously it would fallback to cpu.", "root_cause": "The issue is related to the implementation of roi_align_forward_kernel_xpu for bf16. It seems to be a regression as previously it would fallback to cpu.", "state": "open"}


### Merged Result:1277{"issue_number": 1277, "issue_description": "Llava BF16 and FP16 inference accuracy got out of memory, and the reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.\nllava training is not enabled.... The fail is in load model stage.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:1276{"issue_number": 1276, "issue_description": "Hf_T5_base inference got out of memory but training pass, xpu  eval  hf_T5_base\nIt failed in `nn.functional.softmax` in eager mode, which cannot be fused to SDPA operator.", "reporter": "mengfei25", "assignee": "LuFinch", "resolution": "\n", "root_cause": "It seems that non-fused SDPA should only takes additional 768M (or 2x/3x of it), which should be fine to the platform with 48G memory. There might be other problems that results in the OOM.", "state": "open"}


### Merged Result:1275{"issue_number": 1275, "issue_description": "xpu  train eca_halonext26ts                    E0104 12:46:08.845000 967154 site-packages/torch/_dynamo/utils.py:2353] RMSE (res-fp64): 0.01146, (ref-fp64): 0.00222 and shape=torch.Size([512]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.010000, use_larger_multiplier_for_smaller_tensor: 0\nE0104 12:46:08.845000 967154 site-packages/torch/_dynamo/utils.py:2223] Accuracy failed for key name stages.3.0.post_attn.running_var\nfail_accuracy\nfbnetv3_b got same issue", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\nMarking as a known issue", "root_cause": "Natural numeric issue caused by the bf16 type", "state": "open"}


### Merged Result:1274{"issue_number": 1274, "issue_description": "Convnext_base BF16 training accuracy got failed\nThe root mean square error is very large, and I suspect onednn has some invalid memory access issues...", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nUpdate onednn to main (7a741297e018707b21fb6a280b4399929503bbd7) will solve this issue, but there is small chance to meet gpu page fault and this large rmse again...", "root_cause": "The root cause is suspected to be onednn's invalid memory access issues.", "state": "open"}


### Merged Result:1273{"issue_number": 1273, "issue_description": "Soft_actor_critic BF16 inference got fail accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1273. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nPassed with latest code base", "root_cause": "", "state": "closed"}


### Merged Result:1264{"issue_number": 1264, "issue_description": "Vision_maskrcnn RuntimeError: roi_align_backward_kernel_xpu does not have a deterministic implementation, and issue body Content of #1264 is : ### \ud83d\udc1b Describe the bug\n\n```python\npython benchmarks/dynamo/torchbench.py --accuracy --float32 -d xpu -n10 --training  --only vision_maskrcnn --backend=inductor\n\nxpu  train vision_maskrcnn \n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2751, in validate_model\n    self.model_iter_fn(model, example_inputs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 462, in forward_and_backward_pass\n    self.grad_scaler.scale(loss).backward()\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_tensor.py\", line 648, in backward\n    torch.autograd.backward(\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: roi_align_backward_kernel_xpu does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4886, in run\n    ) = runner.load_model(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 372, in load_model\n    self.validate_model(model, example_inputs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2753, in validate_model\n    raise RuntimeError(\"Eager run failed\") from e\nRuntimeError: Eager run failed\n\neager_fail_to_run\n```\nExisting `roi_align` implementation in torchvision disables the non-deterministic implementation and replaces the op with a pure python implementation which complies to a lower memory version. Supporting XPU with the same approach requires `is_compile_supported` in `torch/_dynamo/utils.py` to add XPU support.", "reporter": "mengfei25", "assignee": "frost-intel", "resolution": "You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.\nSupporting XPU with the same approach requires `is_compile_supported` in `torch/_dynamo/utils.py` to add XPU support.", "root_cause": "roi_align_backward_kernel_xpu does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application.", "state": "closed"}


### Merged Result:1263{"issue_number": 1263, "issue_description": "xpu  train LearningToPaint                                                                                                 Traceback (most recent call last):                                                                                                                                                                                                                                 File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4886, in run                                                                                                                                                                                                                                 ) = runner.load_model(                                                                                                                                                                                                                                                                                File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 325, in load_model                                                                                                                                                                                                                                 model, example_inputs = benchmark.get_module(                                                                                                                                                                                                                                                                            File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/LearningToPaint/__init__.py\", line 81, in get_module                                                                                                                                                                                                                                 action = self.agent.select_action(                                                                                                                                                                                                                                                                                File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/LearningToPaint/baseline/DRL/ddpg.py\", line 211, in select_action                                                                                                                                                                                                                                 action = to_numpy(action)                                                                                                                                                                                                                                                                                File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/LearningToPaint/baseline/utils/util.py\", line 40, in to_numpy                                                                                                                                                                                                                                 return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()                                                                                                                                                                                                                                                                            TypeError: can't convert xpu:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.                                                                                                                                                                                                                              eager_fail_to_run\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1263. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "Use Tensor.cpu() to copy the tensor to host memory first.\nPassed with latest code base", "root_cause": "The error occurs because the tensor is of xpu type and cannot be directly converted to numpy. The solution is to use Tensor.cpu() to copy the tensor to host memory first.", "state": "closed"}


### Merged Result:1262{"issue_number": 1262, "issue_description": "Hf_Reformer got different accuracy results 2 eager runs\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1262. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nPassed with latest code base", "root_cause": "", "state": "closed"}


### Merged Result:1261{"issue_number": 1261, "issue_description": "Stable_diffusion_unet OutOfMemoryError: XPU out of memory, fp16 & bf16 inference are pass_due_to_skip but others throw out of memory error\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1261. The reporter of the issue is mengfei25, and the assignee is tye1, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "tye1", "resolution": "\n", "root_cause": "need to submit request to oneDNN for fp32", "state": "open"}


### Merged Result:1260{"issue_number": 1260, "issue_description": "Nvidia_deeprecommender got failed on XPU device. Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4886, in run\n    ) = runner.load_model(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 325, in load_model\n    model, example_inputs = benchmark.get_module(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/nvidia_deeprecommender/__init__.py\", line 48, in get_module\n    return self.model.rencoder, (self.model.toyinputs,)\nAttributeError: 'DeepRecommenderTrainBenchmark' object has no attribute 'rencoder'\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1260. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nPassed with latest code base", "root_cause": "", "state": "closed"}


### Merged Result:1256{"issue_number": 1256, "issue_description": "The following models got 'eager_two_runs_differ'\n|     |            |                                                                                                           eager_two_runs_diff                                                                                                          |\n|-----|------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| LNL | HF         |                                                                                                                                                                                                                                        |\n|     | Timm       |                                                                                                                                                                                                                                        |\n|     | Torchbench | Super_SloMo   (train_eager_fp32/amp_bf16)      pytorch_CycleGAN_and_pix2pix (train_eager_fp32/bf16/amp_bf16)                                                                                                                           |\n| BMG | HF         |                                                                                                                                                                                                                                        |\n|     | Timm       |                                                                                                                                                                                                                                        |\n|     | Torchbench | Super_SloMo   (train_eager_fp32)      pytorch_CycleGAN_and_pix2pix (train_eager_fp32/bf16)                                                                                                                                             |\n| ARC | HF         | DistilBertForMaskedLM(train_fp16_eager)                                                                                                                                                                                                |\n|     | Timm       | convnext_base   (train_eager)      jx_nest_base (train_eager)      swin_base_patch4_window7_224 (train_eager)      twins_pcpvt_base (train_eager)      coat_lite_mini (train)      convit_base      mobilevit_s      tnt_s_patch16_224 |\n|     | Torchbench | hf_Reformer(train_eager)      timm_regnet (train_eager_fp32) \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1256. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is open.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\n", "root_cause": "The issue is related to atomic operations causing non-deterministic behavior. The reporter and assignee have discussed the need to set deterministic algorithms and avoid atomic operations to resolve the issue. However, the issue still persists on some models like Super_SloMo and pytorch_CycleGAN_and_pix2pix.", "state": "open"}


### Merged Result:1255{"issue_number": 1255, "issue_description": "The following models got fail_accuracy\n|     |            |                                                                                  fail_accuracy                                                                                 |\n|-----|------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| LNL | HF         |                                                                                                                                                                                |\n|     | Timm       |                                                                                                                                                                                |\n|     | Torchbench | hf_Roberta_base   (train_eager_fp16/bf16)      hf_BigBird (train_eager_amp_fp16/bf16)      hf_Reformer (train_eager_amp_fp16/bf16)      hf_Whisper (train_eager_amp_fp16/bf16) |\n| BMG | HF         |                                                                                                                                                                                |\n|     | Timm       |                                                                                                                                                                                |\n|     | Torchbench | hf_Reformer   (train_eager_amp_bf16/fp16)      hf_Whisper (train_eager_amp_fp16/bf16)                                                                                          |\n| ARC | HF         |                                                                                                                                                                                |\n|     | Timm       | coat_lite_mini(train_eager_fp32)                                                                                                                                               |\n|     | Torchbench | hf_Whisper   (train_eager_amp_bf16/fp16)                                                                                                                                       \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1255. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\nThose passed on latest client acceptance test. Thus closed.", "root_cause": "", "state": "closed"}


### Merged Result:1254{"issue_number": 1254, "issue_description": "During the test, we witnessed the following accuracy failures in `test_torchinductor_opinfo.py`:\ntest_comprehensive_masked_mean_xpu_float16\ntest_comprehensive_masked_mean_xpu_float32\ntest_comprehensive_masked_mean_xpu_float64\ntest_comprehensive_nn_functional_pairwise_distance_xpu_float16\nThe test PASSED on latest PyTorch.", "reporter": "Stonepia", "assignee": "etaf", "resolution": "\nThe test PASSED on latest PyTorch.", "root_cause": "", "state": "closed"}


### Merged Result:1253{"issue_number": 1253, "issue_description": "Just compile the Pytorch XPU with cmake 3.31. Examples can be found in CI logs (e.g. https://github.com/intel/torch-xpu-ops/actions/runs/12632141068/job/35195239288#step:5:1034). Spamming if `BUILD_SEPARATE_OPS=ON`", "reporter": "DDEle", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1252{"issue_number": 1252, "issue_description": "Critical issue tracking\n\n### Functional issue:\n\n| Operator | Timeline  | Description | PRs | Flag |\n| ----------------  | ---------------- | ---------------- | ---------------- | ---------------- |\n| index_reduce | 1/8/2025~ | Meet page fault in test_torch_xpu | https://github.com/intel/torch-xpu-ops/pull/1156 | WIP |\n| batch_norm | 1/8/2025~ | Meet page fault in cosmic_tagging_train.h5 with channels_last | | WIP |\n\n### Performance issue:\n\n| Operator | Timeline | Description | PRs | Flag |\n| ----------------  | ---------------- | ---------------- | ---------------- | ---------------- |\n| BN/GN/LN | 1/8/2025~ | Using the IPEX norm backbone to apply the Welford algorithm | | WIP |\n| scatter_gather | ~1/7/2025 | Perf gap with IPEX | https://github.com/intel/torch-xpu-ops/pull/1112 | Merged |\n| batch_norm | ~1/7/2025 | Perf gap with IPEX | https://github.com/intel/torch-xpu-ops/pull/933 | Merged |\n| upsample_bilinear2d | ~1/7/2025 | Perf gap with IPEX | https://github.com/intel/torch-xpu-ops/pull/950 | Merged |\n| max_pool2d_with_indices | ~1/7/2025 | Perf gap with IPEX | https://github.com/intel/torch-xpu-ops/pull/1127 | Merged |\n| group_norm | ~1/7/2025 | Perf gap with IPEX | https://github.com/intel/torch-xpu-ops/pull/1116 | Merged |", "reporter": "xytintel", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1251{"issue_number": 1251, "issue_description": "With rhel and suse container, the 2 models accuracy failed. python benchmarks/dynamo/huggingface.py --accuracy --amp_fp16 -d xpu -n10 --training --only AlbertForMaskedLM --backend=inductor\nCannot reproduce on ubuntu container, is it only in rhel and suse container?", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:1246{"issue_number": 1246, "issue_description": "Nightly rolling test] [UT] XPU OP UT terminate when running test_meta_xpu.py, test would terminate and I found it would fail in different cases, but all the nightly rolling test jobs will be terminated when the entire \nNightly rolling test [UT] XPU OP UT terminate when running test_meta_xpu.py\ntest_meta_xpu.py::TestMetaXPU::test_meta_outplace_nn_functional_adaptive_max_pool3d_xpu_float16\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1246. The reporter of the issue is RUIJIEZHONG66166, and the assignee is PenghuiCheng, and the state of the issue is closed.", "reporter": "RUIJIEZHONG66166", "assignee": "PenghuiCheng", "resolution": "\nThe issue has been fixed in the latest version of the code.\nclosed\nIssue fixed.", "root_cause": "The problem was caused by a bug in the test_meta_xpu.py file, which caused the test to terminate unexpectedly. The bug has been fixed in the latest version of the code.", "state": "closed"}


### Merged Result:1245{"issue_number": 1245, "issue_description": "Since the build logs and test logs are way too long, how about we consider redirecting them to different files. For example, for the following build command: python setup.py bdist_wheel we could direct them in the following way: Linux: python setup.py bdist_wheel 1> logs\test.log 2> logs\test.err Windows: python setup.py bdist_wheel 1>.\test.log 2>.\test.err The above would redirect the output to different files. Normally, we just need to check about the output from 1 and go to the error.log for details. Please go to the Additional Context part for example output of build. For the pytest, we could also do the same.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1245. The reporter of the issue is Stonepia, and the assignee is RUIJIEZHONG66166, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "RUIJIEZHONG66166", "resolution": "\nClose as already done.", "root_cause": "$env:CL=\"/D_CRT_SECURE_NO_WARNINGS\" could also help to reduce warnings in Windows", "state": "closed"}


### Merged Result:1237{"issue_number": 1237, "issue_description": "MLT Windows multihead_attention float16 got accuracy issue\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1237. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\nThis case passed with the latest driver 6647 + 0309 nightly torch whl.", "root_cause": "", "state": "closed"}


### Merged Result:1236{"issue_number": 1236, "issue_description": "nn\\test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_max_pool_nan_inf_xpu_float64 test failed with RuntimeError: Native API failed. Native API returns: 2147483646 (UR_RESULT_ERROR_UNKNOWN). To execute this test, run the following from the base repo dir: python test\nn\test_pooling.py TestPoolingNNDeviceTypeXPU.test_max_pool_nan_inf_xpu_float64\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1236. The reporter of the issue is daisyden, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "Stonepia", "resolution": "\nThe issue was resolved by testing on the following environment and it passed: Pytorch: '2.7.0a0+git924a247', Driver: 32.0.101.6647.", "root_cause": "", "state": "closed"}


### Merged Result:1235{"issue_number": 1235, "issue_description": "nn\\test_embedding_xpu.py::TestEmbeddingNNDeviceTypeXPU::test_embedding_max_norm_device_xpu_float32 failed with an assertion error on Windows MLT with nightly build 20241230. The error message is: tensor(False, device='xpu:0') is not true. The test is supposed to check if the norm of the output tensor is less than or equal to 1, but the actual result is False.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1235. The reporter of the issue is daisyden, and the assignee is gaopengff, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "gaopengff", "resolution": "\nClose it due to MTL windows is low priority", "root_cause": "MTL windows is low priority", "state": "closed"}


### Merged Result:1234{"issue_number": 1234, "issue_description": "The operator 'customflash::custom_flash_aligned' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nSame as https://github.com/intel/torch-xpu-ops/issues/714", "reporter": "mengfei25", "assignee": "xytintel", "resolution": "\nduplicate", "root_cause": "https://github.com/intel/torch-xpu-ops/issues/714", "state": "closed"}


### Merged Result:1231{"issue_number": 1231, "issue_description": "The operator 'aten::_thnn_fused_lstm_cell' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\ndemucs and DLND have the same issue with the XPU device not supporting the LSTM operator.\nThis operator has already been cherry-picked to release/2.6: https://github.com/intel/torch-xpu-ops/pull/1233", "reporter": "mengfei25", "assignee": "", "resolution": "\nclosed\nThe issue has been resolved and the operator has been cherry-picked to the release version.", "root_cause": "The XPU device does not support the LSTM operator, leading to a NotImplementedError.", "state": "closed"}


### Merged Result:1229{"issue_number": 1229, "issue_description": "python benchmarks/dynamo/torchbench.py --accuracy --bfloat16 -d xpu -n10 --training --only yolov3 --backend=inductor\n\nTraceback (most recent call last):\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4886, in run\n) = runner.load_model(\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 312, in load_model\nbenchmark = benchmark_cls(\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 24, in call\nobj = type.call(cls, *args, **kwargs)\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/yolov3/init.py\", line 58, in init\nself.training_loop, self.model, self.example_inputs = prepare_training_loop(\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/yolov3/yolo_train.py\", line 630, in prepare_training_loop\ndevice = torch_utils.select_device(\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/yolov3/yolo_utils/torch_utils.py\", line 31, in select_device\nassert torch.cuda.is_available(), 'CUDA unavailable, invalid device %s requested' % device # check availablity\nAssertionError: CUDA unavailable, invalid device xpu requested\n\nhttps://github.com/pytorch/benchmark/blob/766a5e3a189384659fd35a68c3b17b88c761aaac/torchbenchmark/models/yolov3/yolo_utils/torch_utils.py#L28-L31\n\ndef select_device(device='', apex=False, batch_size=None):\n    # device = 'cpu' or '0' or '0,1,2,3'\n    cpu_request = device.lower() == 'cpu'\n    if device and not cpu_request:  # if device requested other than 'cpu'\n        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable\n        assert torch.cuda.is_available(), 'CUDA unavailable, invalid device %s requested' % device # check availablity\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1229. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nSkipped in CI & Nightly test https://github.com/intel/torch-xpu-ops/pull/1230", "root_cause": "The issue is caused by the fact that the environment variable CUDA_VISIBLE_DEVICES is set to 'xpu' which is not a valid CUDA device. The assert statement in the select_device function raises an AssertionError when the CUDA is unavailable for the device 'xpu' requested. The solution is to set the CUDA_VISIBLE_DEVICES to a valid CUDA device or to remove the environment variable if it is not needed.", "state": "closed"}


### Merged Result:1222{"issue_number": 1222, "issue_description": "Torchbench models got fail_accuracy when using bfloat16 or amp_bf16. The models that failed are listed in the summary.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1222. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nFixed", "root_cause": "https://github.com/intel/torch-xpu-ops/commit/e035f6b3fc8aea782d57bfe90e64fb43cf5ffe55", "state": "closed"}


### Merged Result:1221{"issue_number": 1221, "issue_description": "python benchmarks/dynamo/torchbench.py --accuracy --amp --amp-dtype float16 -d xpu -n10 --inference  --only torchrec_dlrm  --backend=inductor\n\nxpu  eval  torchrec_dlrm                      \nERROR:common:\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 3095, in check_accuracy\n    new_result = optimized_model_iter_fn(model_copy, example_inputs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 576, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/common.py\", line 2786, in run_n_iterations\n    def run_n_iterations(self, mod, inputs):\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 751, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1186, in forward\n    return compiled_fn(full_args)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 322, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 671, in inner_fn\n    outs = compiled_fn(args)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 489, in wrapper\n    return compiled_fn(runtime_args)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_inductor/output_code.py\", line 465, in __call__\n    return self.current_callable(inputs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_inductor/utils.py\", line 2186, in run\n    return model(new_inputs)\n  File \"/tmp/tmpr_qbdtso/lg/clgfoxqdtecedwclatggeagcrpitieaxizihdimxzd2wppd3fjkx.py\", line 1411, in call\n    extern_kernels.addmm(arg87_1, buf171, reinterpret_tensor(buf172, (512, 1728), (1, 512), 0), alpha=1, beta=1, out=buf173)\nRuntimeError: self and mat2 must have the same dtype, but got Float and BFloat16\nTorchDynamo optimized model failed to run because of following error\nfail_to_run\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1221. The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "self and mat2 must have the same dtype, but got Float and BFloat16", "state": "open"}




### Merged Result:1219{"issue_number": 1219, "issue_description": "cannot import name 'cached_download' from 'huggingface_hub' (/global/panfs01/users/mengfei25/miniforge3/envs/test/lib/python3.10/site-packages/huggingface_hub/__init__.py)\nDowngrading huggingface-hub to 0.25.0", "reporter": "mengfei25", "assignee": "", "resolution": "\nDowngrading huggingface-hub to 0.25.0", "root_cause": "Unknown", "state": "closed"}




### Merged Result:1216{"issue_number": 1216, "issue_description": "Failed dtype: float32, float16 and bfloat16. AMP passed\npython benchmarks/dynamo/huggingface.py --accuracy --float32 -d xpu -n10 --training--only DebertaV2ForQuestionAnswering --backend=inductor\nxpu  train DebertaV2ForQuestionAnswering\nE1220 16:43:35.601000 756971 site-packages/torch/_dynamo/utils.py:2307] RMSE (res-fp64): 0.53515, (ref-fp64): 0.01636 and shape=torch.Size([]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.010000, use_larger_multiplier_for_smaller_tensor: 0\nfail_accuracy\nThe issue is caused by a commit in pytorch/pytorch, and upgrading transformers to the latest version will fix the issue.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nUpgrading transformers to the latest version will fix the issue.", "root_cause": "A commit in pytorch/pytorch caused the issue.", "state": "open"}


### Merged Result:1214{"issue_number": 1214, "issue_description": "In preci test, there are random cases will fail, need root cause. The cases that have appeared are as follows: test/xpu/test_ops_xpu.py: test_python_ref__refs_exp_xpu_complex128 test_python_ref__refs_sigmoid_xpu_complex128 test_python_ref_executor__refs_log2_executor_aten_xpu_complex128 test_python_ref_executor__refs_exp_executor_aten_xpu_complex128 test_python_ref_torch_fallback__refs_log2_xpu_complex128 test_python_ref_torch_fallback__refs_log10_xpu_complex128 test_python_ref_torch_fallback__refs_sigmoid_xpu_complex128 workaround PR: https://github.com/intel/torch-xpu-ops/pull/1211 More random failures to be added to skiplist: - [ ] TestCommonXPU.test_python_ref_executor__refs_sigmoid_executor_aten_xpu_complex128 - [ ] TestCommonXPU.test_compare_cpu_nn_functional_local_response_norm_xpu_bfloat16 - [ ] test_ops_xpu.py::TestCommonXPU::test_python_ref__refs_log10_xpu_complex128\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1214. The reporter of the issue is PenghuiCheng, and the assignee is daisyden, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "\n", "root_cause": "New random failure with release/2.7 RC2 pre release wheel test_foreach_xpu.py::TestForeachXPU::test_parity__foreach_div_fastpath_outplace_xpu_complex128", "state": "open"}


### Merged Result:1213{"issue_number": 1213, "issue_description": "Support `aten::split_with_sizes_copy.out`/`aten::_chunk_cat`/`aten::_chunk_cat.out` to align with CUDA as a fast pass.\nThe issue is about the performance of the torch_xpu_ops library, specifically the problem with the `torch_xpu_ops.matmul` function. The reporter of the issue is zhangxiaoli73, and the assignee is fengyuan14. The issue is closed, and the resolution is that the problem is caused by a bug in the `torch_xpu_ops.matmul` function, which has been fixed in the latest version of the library. The root cause is that the function does not handle certain edge cases properly, leading to performance degradation.", "reporter": "zhangxiaoli73", "assignee": "fengyuan14", "resolution": "\nThe problem is caused by a bug in the `torch_xpu_ops.matmul` function, which has been fixed in the latest version of the library.", "root_cause": "The function does not handle certain edge cases properly, leading to performance degradation.", "state": "closed"}


### Merged Result:1210{"issue_number": 1210, "issue_description": "The feature, motivation and pitch\n\nGoogleFnet in torch dynamo benchmarks uses fft, we need support it.\n\nAlternatives\n\n_No response_\n\nAdditional context\n\n_No response_,", "reporter": "jianyizh", "assignee": "CuiYifeng", "resolution": "testing", "root_cause": "unknown", "state": "open"}


### Merged Result:1209{"issue_number": 1209, "issue_description": "Need tf32 for matmul, we should add APIs like torch.backends.cuda.matmul.allow_tf32, torch.backends.cudnn.allow_tf32, torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction", "reporter": "jianyizh", "assignee": "ZhiweiYan-96", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1200{"issue_number": 1200, "issue_description": "Release 2.6 got failures because the cpu \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1200. The reporter of the issue is daisyden, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "\n[Rel](https://github.com/pytorch/pytorch/commit/a52d9f6f4c9c839b0312fbe02925b38cd1f40758c3b3e579c277cdfe26af18a2#diff-fef2ee1307d8d82cd8ccfa4188ab3808635370a8c3b3e579c277cdfe26af18a2)", "root_cause": "", "state": "closed"}


### Merged Result:1199{"issue_number": 1199, "issue_description": "Seems that there should be a dtype conversion but we didn't do it:\n\n```PYTORCH_TEST_WITH_SLOW=1 python test\\xpu\\test_tensor_creation_ops_xpu.py TestTensorCreationXPU.test_block_diag_scipy_xpu\n``` \n\nWe get the following error:\n\n```Traceback (most recent call last):\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 59, in testPartExecutor\n    yield\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 591, in run\n    self._callTestMethod(testMethod)\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 549, in _callTestMethod\n    method()\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_utils.py\", line 3099, in wrapper\n    method(*args, **kwargs)\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_device_type.py\", line 460, in instantiated_test\n    result = test(self, **param_kwargs)\n  File \"C:\\pt26_ww48\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\test_tensor_creation_ops_xpu.py\", line 408, in test_block_diag_scipy\n    self.assertEqual(scipy_result.dtype, scipy_type)\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_utils.py\", line 4007, in assertEqual\n    raise error_metas.pop()[0].to_error(\nAssertionError: Object comparison failed: torch.int64 != torch.int32\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1199. The reporter of the issue is Stonepia, and the assignee is LuFinch, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "LuFinch", "resolution": "\nThe issue should be related to the environment.", "root_cause": "There should be a dtype conversion but we didn't do it.", "state": "closed"}


### Merged Result:1198{"issue_number": 1198, "issue_description": "We get the dtype mismatch for the `pow` op because the cast is failed. From the msg, seems that there should be a type conversion but failed.\nThe issue is related to the environment.", "reporter": "Stonepia", "assignee": "gaopengff", "resolution": "\nCorrectly setup the environment of pip packages", "root_cause": "The issue is related to the environment.", "state": "closed"}


### Merged Result:1197{"issue_number": 1197, "issue_description": "When running the following test:\n\n```PYTORCH_TEST_WITH_SLOW=1 python test\\quantization\\core\\test_workflow_ops.py TestFakeQuantizeOpsXPU.test_learnable_forward_per_channel_cpu_xpu```\n\n```Traceback (most recent call last):\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 59, in testPartExecutor\n    yield\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 591, in run\n    self._callTestMethod(testMethod)\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 549, in _callTestMethod\n    method()\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_utils.py\", line 3099, in wrapper\n    method(*args, **kwargs)\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_device_type.py\", line 460, in instantiated_test\n    result = test(self, **param_kwargs)\n  File \"C:\\pt26_ww48\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\quantization\\core\\test_workflow_ops.py\", line 807, in test_learnable_forward_per_channel\n    qparams=hu.qparams(dtypes=torch.quint8)))\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\hypothesis\\core.py\", line 1758, in wrapped_test\n    raise the_error_hypothesis_found\n  File \"C:\\pt26_ww48\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\quantization\\core\\test_workflow_ops.py\", line 815, in test_learnable_forward_per_channel\n    self._test_learnable_forward_per_channel(\n  File \"C:\\pt26_ww48\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\quantization\\core\\test_workflow_ops.py\", line 802, in _test_learnable_forward_per_channel\n    self.assertTrue(\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 687, in assertTrue\n    raise self.failureException(msg)\nAssertionError: False is not true : Expected kernel forward function to have results match the reference forward function```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1197. The reporter of the issue is Stonepia, and the assignee is LuFinch, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "LuFinch", "resolution": "The issue is fixed in the latest version of the codebase.\nThe issue should be related to the environment.", "root_cause": "The issue is caused by a bug in the kernel implementation of the forward function. The fix is to update the kernel implementation to ensure that the results of the forward function match the reference forward function.", "state": "closed"}


### Merged Result:1196{"issue_number": 1196, "issue_description": "XPU does not support FP8 tests for now. We need to skip them.\n\nPYTORCH_TEST_WITH_SLOW=1 python test\test_matmul_cuda.py TestFP8MatmulCudaXPU.test_zero_dim_tensorwise_which_dim_zero_0_use_torch_compile_False_xpu\nPYTORCH_TEST_WITH_SLOW=1 python test\test_matmul_cuda.py TestFP8MatmulCudaXPU.test_zero_dim_tensorwise_which_dim_zero_0_use_torch_compile_True_xpu\nPYTORCH_TEST_WITH_SLOW=1 python test\test_matmul_cuda.py TestFP8MatmulCudaXPU.test_zero_dim_tensorwise_which_dim_zero_1_use_torch_compile_False_xpu\nPYTORCH_TEST_WITH_SLOW=1 python test\test_matmul_cuda.py TestFP8MatmulCudaXPU.test_zero_dim_tensorwise_which_dim_zero_1_use_torch_compile_True_xpu\nPYTORCH_TEST_WITH_SLOW=1 python test\test_matmul_cuda.py TestFP8MatmulCudaXPU.test_zero_dim_tensorwise_which_dim_zero_2_use_torch_compile_False_xpu\nPYTORCH_TEST_WITH_SLOW=1 python test\test_matmul_cuda.py TestFP8MatmulCudaXPU.test_zero_dim_tensorwise_which_dim_zero_2_use_torch_compile_True_xpu\n\nA typical error msg would be like:\n\n_ TestFP8MatmulCudaXPU.test_zero_dim_tensorwise_which_dim_zero_2_use_torch_compile_True_xpu _\nTraceback (most recent call last):\n  File \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1196. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nPR https://github.com/intel/torch-xpu-ops/pull/1123", "root_cause": "XPU does not support FP8 tests for now.", "state": "closed"}


### Merged Result:1195{"issue_number": 1195, "issue_description": "We get nan when the dtype is complex. The tests are:\n\n```py\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal__refs_sin_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal__refs_sin_xpu_complex64\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_nn_functional_tanhshrink_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_nn_functional_tanhshrink_xpu_complex64\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sin_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sin_xpu_complex64\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sinh_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sinh_xpu_complex64\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large__refs_exp_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large__refs_exp_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large__refs_sin_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large_exp_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large_exp_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large_sin_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_normal_sigmoid_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_acos_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_asin_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_asinh_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_atan_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_atanh_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_sigmoid_xpu_complex32\n\nThere should be related to bugs with the compiler. We will discuss on how to co-work with the compiler team for this issue.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\n", "root_cause": "Compiler bugs", "state": "open"}




### Merged Result:1193{"issue_number": 1193, "issue_description": "UT cases which failed on rolling driver and passed on lts driver:\ntest_distributions_xpu.py::TestDistributionsXPU::test_gamma_gpu_sample_xpu\ntest_ops_xpu.py::TestCommonXPU::test_python_ref__refs_div_trunc_rounding_xpu_float64\nfor test_gamma_gpu_sample_xpu, it will also pass on rolling driver to add \"clang::optnone\" label.\n![image](https://github.com/user-attachments/assets/c449e637-7521-4c7d-8ae7-848a83efcc48)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1193. The reporter of the issue is PenghuiCheng, and the assignee is , and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "", "resolution": "add \"clang::optnone\" label\nDuplicated issue, close it.", "root_cause": "unknown", "state": "closed"}




### Merged Result:1191{"issue_number": 1191, "issue_description": "When Running the test with test_ops_xpu.py::TestCommonXPU::test_compare_cpu_grid_sampler_2d_xpu_float64. Got the following error: Fatal Python error: Illegal instruction\nFatal Python error: Illegal instruction, when running pytest with torch-xpu-ops, the error is raised, the reporter is Stonepia, the assignee is Stonepia, the state of the issue is closed.\nduplicate with https://github.com/intel/torch-xpu-ops/issues/1173", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "The issue is closed\nThe issue is closed.\n", "root_cause": "The error is caused by an illegal instruction, which is a hardware error and cannot be fixed by software. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "state": "closed"}




### Merged Result:1172{"issue_number": 1172, "issue_description": "Got this error on LNL Windows with 1202 wheel. subprocess.CalledProcessError: Command '['where', 'cl']' returned non-zero exit status 1.\nVS2022INSTALLDIR environment variable not set", "reporter": "daisyden", "assignee": "", "resolution": "\nSetting VS2022INSTALLDIR environment variable to the correct path", "root_cause": "Incorrect path set for VS2022INSTALLDIR environment variable", "state": "closed"}


### Merged Result:1171{"issue_number": 1171, "issue_description": "On LNL Windows with 1202 nightly wheel we got this error. No such problem on linux.\n\n### \ud83d\udc1b Describe the bug\n\nOn LNL Windows with 1202 nightly wheel we got this error. No such problem on linux.\n\n### \ud83d\udccb To reproduce\n\nTo execute this test, run the following from the base repo dir:\n    python test\nnn\test_pooling.py TestPoolingNNDeviceTypeXPU.test_MaxUnpool_index_errors_case2_xpu\n\n### \ud83d\udcc4 Expected behavior\n\nNo error.\n\n### \ud83d\udcc4 Actual behavior\n\nAssertion `maxind >= 0 && maxind < outputImageSize` failed\n\n### \ud83d\udcc4 Steps to reproduce\n\n1. Run the test on LNL Windows with 1202 nightly wheel\n2. Run the test on Linux\n3. Compare the results\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1171. The reporter of the issue is daisyden, and the assignee is gaopengff, and the state of the issue is open.", "reporter": "daisyden", "assignee": "gaopengff", "resolution": "\n", "root_cause": "compiler issue", "state": "open"}


### Merged Result:1170{"issue_number": 1170, "issue_description": "After use stock PT, OOB for HF models on PVC have OOM issue compared with IPEX 2.3. issue type: 1. RuntimeError: UR backend failed. UR backend returns:40 (UR_RESULT_ERROR_OUT_OF_RESOURCES) 2. torch.OutOfMemoryError: XPU out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 64.00 GiB\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1170. The reporter of the issue is 1pikachu, and the assignee is , and the state of the issue is closed.", "reporter": "1pikachu", "assignee": "", "resolution": "\nThis seems reasonable. The stock PyTorch may have larger op and exceed the memory capacity (because of too many kernels fused together).", "root_cause": "The stock PyTorch may have larger op and exceed the memory capacity (because of too many kernels fused together).", "state": "closed"}


### Merged Result:1169{"issue_number": 1169, "issue_description": "torch.nextafter has an incorrect result for bf16 on XPU", "reporter": "guangyey", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1166{"issue_number": 1166, "issue_description": "Please check this test, you can find some UT failures in log, but the result of preci is passed.\nThis should already done.", "reporter": "daisyden", "assignee": "RUIJIEZHONG66166", "resolution": "\nThis should already done.", "root_cause": "", "state": "closed"}


### Merged Result:1165{"issue_number": 1165, "issue_description": "Add a test of PyTorch XPU with Huggingface Transformers, Catch regressions coming from PyTorch XPU backend which affect Transformers, Catch new features coming from Transformers which require implementation efforts in PyTorch XPU, Design approach is to be as close to Transformers ci environment as possible, See [Dockerfile](https://github.com/huggingface/transformers/blob/v4.47.0/docker/transformers-pytorch-gpu/Dockerfile), see [self-push.yml](https://github.com/huggingface/transformers/blob/v4.47.0/.github/workflows/self-push.yml) for the references, Setup the following test triggers, Use `linux.idc.xpu` runners, Use Ubuntu based hosts (22.04 or later), Install: `apt-get install git-lfs && git lfs install`, Install: `apt-get install espeak-ng`, Install: `apt-get install pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libavfilter-dev libswscale-dev libswresample-dev`, Use Conda virtual environment with python 3.10, Clone Transformers `v4.47.0`, Install Transformers, Install Transformers test dependencies, Install XPU device specific test configuration file `TRANSFORMERS_TEST_DEVICE_SPEC=spec.py`, Run Transformers tests as follows, Use pytest options: `-rsf --make-reports=<testgroup>`, Run: `python -m pytest tests/*.py`, Run: `python -m pytest tests/benchmark`, Run: `python -m pytest tests/generation`, Run: `python -m pytest tests/models`, Run: `python -m pytest tests/models -k backbone`, Run: `python -m pytest tests/pipelines`, Run: `python -m pytest tests/trainer`, Run: `python -m pytest tests/utils`, Mark test as passed or failed according to baseline expectations defined for each test group, At the moment we still have some features not implemented for PyTorch XPU backend affecting Transformers tests, plus some porting is needed in tests themselves, For convenience we are breaking tess into groups defining baseline expectations for each group separately, In the future we will likely switch to running just `python -m pytest tests`, Baseline expecations are, Test should check baseline as follows, For groups with 0/0 expectations - check pytest return status code (expect to be 0), For groups with non-zero failed cases - ignore pytest return status code and check, Number of errors should match (be 0), Number of failed cases should match, One-line `failures_line.txt` outputs from `--make-reports` (or failed cases) should match, The following artifacts should be made available after test execution, List of PyPI packages installed in Conda environment and their versions, List of available GPU device IDs, Logs running each pytest command, Archived reports from `--make-reports` command, Table report with annotations, Table report with number of passed/failed/skipped cases, Table report with failed cases, Table report with skipped cases and reasons, \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1165. The reporter of the issue is dvrogozh, and the assignee is RUIJIEZHONG66166, and the state of the issue is open.", "reporter": "dvrogozh", "assignee": "RUIJIEZHONG66166", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:1164{"issue_number": 1164, "issue_description": "Observed with torchbench training performance on Rolling driver and LTS driver, looks like Rolling is slower than LTS. Overall it is ~20% gap. The following is the < 50% models\nHW CPU frequency settings, not related with SW", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "HW CPU frequency settings, not related with SW", "state": "closed"}


### Merged Result:1163{"issue_number": 1163, "issue_description": "torch._standard_gamma() has accuracy gap compared to scipy and torch.cpu\nTried cpu _standard_gamma tensor, no such issue.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/1161", "root_cause": "Seems this issue exists or not on different driver versions, investigation is WIP.", "state": "open"}


### Merged Result:1160{"issue_number": 1160, "issue_description": "When the two tensors are the same, what is the expected result? It is 1.0 or a number close to 1.0? This will lead to different result when apply trunc, lead to the UT failures.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "", "root_cause": "The issue is caused by the difference in the result of torch.div and torch.trunc(torch.div) when the divisor and dividend are the same. The torch.div function returns a number close to 1.0, while the torch.trunc function returns 0.", "state": "open"}


### Merged Result:1159{"issue_number": 1159, "issue_description": "Huggingface model DebertaForQuestionAnswering && DebertaV2ForMaskedLM failed with RuntimeError: value cannot be converted to type at::BFloat16 without overflow. Script: `python benchmarks\\dynamo\\huggingface.py --accuracy -d xpu -n10 --inference --backend=eager --cold-start-latency --amp --amp-dtype float16 --only DebertaForQuestionAnswering`. Error Log Info: xpu eval DebertaForMaskedLM Traceback (most recent call last): File \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1159. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is open.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\n", "root_cause": "This is the issue from transformers. See the discussion at https://github.com/huggingface/transformers/pull/35336 .", "state": "open"}


### Merged Result:1158{"issue_number": 1158, "issue_description": "Script: `python benchmarks\\dynamo\\huggingface.py --accuracy -d xpu -n10 --inference --backend=eager --cold-start-latency --float32 --only BlenderbotSmallForCausalLM ` failed with UR Error. The error occurred during the deepcopy process of the model. The error message is: RuntimeError: UR error. The issue is reported by libohao1201 and assigned to Stonepia. The state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1158. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is open.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "default\n", "root_cause": "default", "state": "open"}


### Merged Result:1157{"issue_number": 141539, "issue_description": "install pytorch 2.6 nightly on windows Arc 770 machines,\npython test.py: \n\ngot the error \n\n File \"C:\\Users\\huiyanca\\.conda\\envs\\torch-xpu-nightly\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\huiyanca\\.conda\\envs\\torch-xpu-nightly\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 1124, in forward\n    result = _VF.lstm(\nNotImplementedError: The operator 'aten::_thnn_fused_lstm_cell' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\n\n>set PYTORCH_ENABLE_XPU_FALLBACK=1\ndoesn't help. \n\nrelated PR:  https://github.com/pytorch/pytorch/issues/141539\nRelated PR: https://github.com/intel/torch-xpu-ops/pull/926\n\nthank you!\nThis operator has already been cherry-picked to release/2.6", "reporter": "yinghu5", "assignee": "", "resolution": "\nThis operator has already been cherry-picked to release/2.6", "root_cause": "", "state": "closed"}


### Merged Result:1152{"issue_number": 1152, "issue_description": "When input is nan, PVC and CPU return nan while ARC returns 0. However I write a small case and it also returns nan on ARC, looks not a compiler issue. Need more investigations.\nverified passed on 1222 wheel", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:1151{"issue_number": 1151, "issue_description": "Failed to build Pytorch XPU on Windows Server\nThe issue is related to the long file path and the OS version is Windows 10 1607, which might be too old. Thus we will try on a newer OS machine.", "reporter": "DDEle", "assignee": "Stonepia", "resolution": "\nclose issue", "root_cause": "The issue is because of the long file path and the OS version is Windows 10 1607, which might be too old.", "state": "closed"}


### Merged Result:1150{"issue_number": 1150, "issue_description": "Some operators UT fails on XPU with \"Kernel is incompatible with all devices\" error.\nKernel is incompatible with all devices in devs", "reporter": "PenghuiCheng", "assignee": "fengyuan14", "resolution": "\nChange the scalar or immediate value from double to float", "root_cause": "Calling sycl::get_kernel_bundle before sycl::queue::submit", "state": "open"}


### Merged Result:1147{"issue_number": 1147, "issue_description": "topk calculation gives wrong result when on xpu. I find the issue when using both bfloat16 and float16 but not on float32. Following code results with a different results. If the .to('xpu') is removed, the answer is 0.\nWhen using torch.topk, the **indices** of tied elements are not guaranteed to **be stable** and may vary across different invocations. And if you check the calculated **values** by following code, you will find the difference between two devices is zero.", "reporter": "maciek226", "assignee": "xytintel", "resolution": "\nThe issue is closed.", "root_cause": "The indices of tied elements are not guaranteed to be stable and may vary across different invocations.", "state": "closed"}


### Merged Result:1141{"issue_number": 1141, "issue_description": "Support NestedTensor for XPU device", "reporter": "min-jean-cho", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1137{"issue_number": 1137, "issue_description": "Run stable-diffusion-inf at gpu-models repo, got an error. Need more investigation to confirm whether it is caused by stable-diffusion.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1137. The reporter of the issue is daisyden, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "Stonepia", "resolution": "\nShould be related to Driver problem, we will re-test it with the new driver.", "root_cause": "Driver problem", "state": "closed"}


### Merged Result:1136{"issue_number": 1136, "issue_description": "IPEX model resnet50 main.py called torch.xpu.set_fp32_math_mode(torch.xpu.FP32MathMode.FP32) but stock pytorch does not have the API. We could also need to understand the impact of this setting to resnet50.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1136. The reporter of the issue is daisyden, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "Stonepia", "resolution": "closed\nThis should be related to the IPEX-specific API, since we discussed to use OOB model scripts rather than IPEX scripts, we will close this issue.", "root_cause": "", "state": "closed"}


### Merged Result:1135{"issue_number": 1135, "issue_description": "Run ssd-mobilenetv1 on LNL Windows with https://github.com/intel-innersource/frameworks.ai.pytorch.gpu-models/SSD-MobileNetv1 on stock PyTorch got error\n\nfail(AttributeError: module 'torch.xpu' has no attribute 'locations_to_boxes').\n\nThis is because the model used an ipex customized op at [link](https://github.com/intel-innersource/frameworks.ai.pytorch.gpu-models/blob/daisyden/stock_pt/SSD-MobileNetv1/vision/ssd/ssd.py#L94).\n\nDo we need to support it or use an official version on ssd-mobilenet in torch_vision?\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1135. The reporter of the issue is daisyden, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "Stonepia", "resolution": "\nSince we are going to use OOB model scripts instead of the IPEX scripts, we will close this issue.", "root_cause": "", "state": "closed"}


### Merged Result:1133{"issue_number": 1133, "issue_description": "Performance enhancement [not exposed]\n- [batch_normalization: Introduce vectorization optimization in the batch norm elementwise kernel](https://github.com/intel/torch-xpu-ops/pull/933)\n- [upsample_biliear2d channel last backward optimization](https://github.com/intel/torch-xpu-ops/pull/950)\n- [group norm forward vectorization optimization](https://github.com/intel/torch-xpu-ops/pull/1116)\n- [max pool operator level optimization](https://github.com/intel/torch-xpu-ops/pull/1127)", "reporter": "xytintel", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1129{"issue_number": 1129, "issue_description": "inductor may pad mm for some shapes i.e replace matmul with zeros, cat, matmul and slice. On A100 fp16 fp_GPT2, this feature can improve 30% performance. Check if it's useful on XPU.\nGeneral guideline from onednn team: pad to a multiple of 64 bytes (1 cache line) in the unit-stride dimension, but try to avoid multiples of large powers of 2 (say 2048 bytes). So for bf16, 37 elements in contiguous dimension = 74 bytes -> pad to 128 bytes, but 1023 elements = 2046 bytes -> pad to 2048 + 64 bytes to avoid a multiple of 2048. It's enough to pad the tensor strides; you do not need to pad the tensor size itself if you don't want to.", "reporter": "jianyizh", "assignee": "jianyizh", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:1128{"issue_number": 1128, "issue_description": "Current sdpa will go into math path, which will always use fp32 even inputs are 16 bit. Compare to cuda, more patterns can be matched and this will cause low performance before we have sdpa kernel.\nFor example, GPT 2 can have 35% improvement if we don't match sdpa. https://github.com/pytorch/pytorch/blob/v2.5.1/torch/_inductor/fx_passes/fuse_attention.py#L815", "reporter": "jianyizh", "assignee": "jianyizh", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1125{"issue_number": 1125, "issue_description": "The reporter of the issue is daisyden, and the assignee is xytintel, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1125. The reporter of the issue is daisyden, and the assignee is xytintel, and the state of the issue is open.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:1124{"issue_number": 1124, "issue_description": "Precision issues depend on oneAPI. For extreme value processing, Numpy and XPU results are inconsistent, std operations get different behavior on std::complex operarands for extremal cases. 'test_reference_numerics_extremal_exp2_xpu_complex128', 'test_reference_numerics_extremal_exp2_xpu_complex64', 'test_reference_numerics_normal_nn_functional_tanhshrink_xpu_complex64', 'test_reference_numerics_extremal__refs_acos_xpu_complex64', 'test_reference_numerics_extremal__refs_acosh_xpu_complex64', 'test_reference_numerics_extremal__refs_asin_xpu_complex64', 'test_reference_numerics_extremal__refs_log_xpu_complex64', 'test_reference_numerics_extremal__refs_tanh_xpu_complex128', 'test_reference_numerics_extremal__refs_tanh_xpu_complex64', 'test_reference_numerics_extremal_acos_xpu_complex64', 'test_reference_numerics_extremal_acosh_xpu_complex64', 'test_reference_numerics_extremal_asin_xpu_complex64', 'test_reference_numerics_extremal_log_xpu_complex64', 'test_reference_numerics_extremal_tanh_xpu_complex128', 'test_reference_numerics_extremal_tanh_xpu_complex64', 'test_reference_numerics_large__refs_acosh_xpu_complex64', 'test_reference_numerics_large_acosh_xpu_complex64', 'test_reference_numerics_large_tanh_xpu_complex32' are fixed.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "", "root_cause": "Precision issues depend on oneAPI. For extreme value processing, Numpy and XPU results are inconsistent, std operations get different behavior on std::complex operarands for extremal cases.", "state": "open"}


### Merged Result:1122{"issue_number": 1122, "issue_description": "We got a report that on Ubuntu 24.10, the installation following https://dgpu-docs.intel.com/driver/client/overview.html#installing-client-gpus-on-ubuntu-desktop-24-10 will fail. `lspci` and `clinfo` will hang.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1122. The reporter of the issue is Stonepia, and the assignee is , and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "", "resolution": "The user need to upgrade kernel using the following command. \nsudo apt-get upgrade linux-generic linux-headers-generic linux-image-generic\n\nThis should be related to the driver installation unaligned with the kernel. So close this.", "root_cause": "There may some misalignment about the kernel and driver package version. According to report, default 6.11.0-8 kernel version will fail. and 6.11.0-9 should pass. The user need to upgrade kernel using the above command. However, we need to make sure that the default installation instructions work OOB. We need to verify that.", "state": "closed"}


### Merged Result:1121{"issue_number": 1121, "issue_description": "The straight forward thought is kernel bundle is not device specific under a specific platform context (Like GPU platform). So we should not use `dev` (Existing WA for the https://github.com/intel/llvm/issues/15127) as a hint. The code snippet is as follows: auto kid = ::sycl::get_kernel_id<KernelClass>(); auto kbundle = ::sycl::get_kernel_bundle<::sycl::bundle_state::executable>(ctx, {dev}, {kid}); ::sycl::kernel k = kbundle.get_kernel(kid); return k.get_info<::sycl::info::kernel_device_specific::work_group_size>(dev);", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1120{"issue_number": 1120, "issue_description": "FP8 matmul compute wrong result in OneDNN 3.5 when the matrix contains fp8 maximum or minimum. This bug is fixed in oneDNN 3.6. This OP will be suspended until stock pytorch update oneDNN.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "This bug is fixed in oneDNN 3.6", "root_cause": "This bug is fixed in oneDNN 3.6", "state": "closed"}


### Merged Result:1113{"issue_number": 1113, "issue_description": "Execute Triton XPU tutorial:\nhttps://raw.githubusercontent.com/intel/intel-xpu-backend-for-triton/refs/heads/main/python/tutorials/01-vector-add.py\npython 01-vector-add.py\n\nResult:\nterminate called after throwing an instance of 'sycl::_V1::exception'\n  what():  Native API failed. Native API returns: 37 (UR_RESULT_ERROR_UNINITIALIZED)\nAborted (core dumped)\nThe issue is no longer reproducible with the latest nightly wheels, closing this issue. Will need a separate issue for `AttributeError`.", "reporter": "pbchekin", "assignee": "ratnampa", "resolution": "\nThe issue is no longer reproducible with the latest nightly wheels.", "root_cause": "UR_RESULT_ERROR_UNINITIALIZED", "state": "closed"}


### Merged Result:1109{"issue_number": 1109, "issue_description": "There are some models in torch benchmarks using lstm, rnn... CUDA uses cuDNN for these ops. Can we add them using oneDNN?\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1109. The reporter of the issue is jianyizh, and the assignee is xytintel, and the state of the issue is open.", "reporter": "jianyizh", "assignee": "xytintel", "resolution": "\n", "root_cause": "The reporter jianyizh suggests using thnn_fused_lstm_cell_forward to check if it can solve the problem.", "state": "open"}


### Merged Result:1108{"issue_number": 1108, "issue_description": "Evaluate the following operators in performance:\n- [x] batch_norm https://github.com/intel/torch-xpu-ops/pull/933\n- [x] scatter_gather https://github.com/intel/torch-xpu-ops/pull/1112\n- [ ] grid_sampler\n- [x] group_norm https://github.com/intel/torch-xpu-ops/pull/1116", "reporter": "xytintel", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1094{"issue_number": 1094, "issue_description": "We find a failure when running on Client GPU:\n\n```\n_____________________________ TestNN.test_no_grad ______________________________\nTraceback (most recent call last):\n  File \"/home/zhouyi/work/pytorch/third_party/torch-xpu-ops/test/xpu/../../../../test/test_nn.py\", line 274, in test_no_grad\n    output.backward(torch.ones(1, 5, 10, 10))\n  File \"/home/zhouyi/miniforge3/envs/xpu_op_0/lib/python3.10/site-packages/torch/_tensor.py\", line 581, in backward\n    torch.autograd.backward(\n  File \"/home/zhouyi/miniforge3/envs/xpu_op_0/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/home/zhouyi/miniforge3/envs/xpu_op_0/lib/python3.10/site-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: DNNL does not support bf16/f16 backward on the platform with avx2_vnni_2\n\nTo execute this test, run the following from the base repo dir:\n    PYTORCH_TEST_WITH_SLOW=1 python test/test_nn.py TestNN.test_no_grad\n\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\n```\nclosed as current CI does not have the same issue.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nclosed as current CI does not have the same issue.", "root_cause": "DNNL does not support bf16/f16 backward on the platform with avx2_vnni_2", "state": "closed"}


### Merged Result:1093{"issue_number": 1093, "issue_description": "with oneAPI 2025.0.1 upgrade, Compiler: [l_compiler_p_2025.0.1.1242](https://af-satgoneapi.devtools.intel.com/ui/native/satgoneapi-or-local/products/compiler/2025.0.1/packages/l_compiler_p_2025.0.1.1242/)\nWe got fail on this case:\ntest_reductions_xpu.py::TestReductionsXPU::test_mode_large_xpu_float32\nIt can be reproduced on this case, we expect to get 1 in each row of value variable, but got 0.\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='xpu:0')\n\nimport torch\na=torch.FloatTensor([[  1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.],\n            [  1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.],\n                    [  1.,   1.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.],\n                            [  1.,   1.,  33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.],\n                                    [  1.,   1.,  43.,  44.,  45.,  46.,  47.,  48.,  49.,  50.],\n                                            [  1.,   1.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.],\n                                                    [  1.,   1.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.],\n                                                            [  1.,   1.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.],\n                                                                    [  1.,   1.,  83.,  84.,  85.,  86.,  87.,  88.,  89.,  90.],\n                                                                            [  1.,   1.,  93.,  94.,  95.,  96.,  97.,  98.,  99., 100.]]).xpu()\nvalue, indice = torch.mode(a, -1, False)\nprint(value)", "reporter": "daisyden", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1092{"issue_number": 1092, "issue_description": "When running the example code on XPU, a segmentation fault error occurs. The debug message with gdb shows that the error is caused by the conversion from fp32 to bf16. The reporter of the issue is faaany, and the assignee is Stonepia. The state of the issue is closed.\nThe issue is related to the installation and usage of `pytorch-triton-xpu` and `triton` packages, which caused a `Segmentation fault (core dumped)` error. The root cause is a bug in the Triton's latest commit, which was fixed by the commit `9b5b553c7c90b917eed839d69f9516087ebb970d`.", "reporter": "faaany", "assignee": "Stonepia", "resolution": "\nClose this as already implemented.", "root_cause": "The segmentation fault error is caused by the conversion from fp32 to bf16, which is a known issue with Triton and Intel's BF16 conversion.", "state": "closed"}


### Merged Result:1080{"issue_number": 1080, "issue_description": "OneDNN upgrade introduces new failures when testing UT and E2E (huggingface models). Details of new failures are as follows: - E2E: OneDNN new Failures (vs Baseline) |         E2E   Cases         |                        Error Type                        |                                                                                   oneDNN       (Failed model)                                                                                   |                                                                  Baseline      (Failed model)                                                                 |\n|:---------------------------:|:--------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| hf_n10_inference_eager_fp32 | Native API   returns: -999       (Unknown PI error)      |                                      BlenderbotForCausalLM      DebertaV2ForMaskedLM      DebertaV2ForQuestionAnswering      MBartForConditionalGeneration                                      |                                       BlenderbotForCausalLM      DebertaV2ForMaskedLM      DebertaV2ForQuestionAnswering                                      |\n|                             | eager_two_runs_differ                                    |                                                                 PegasusForConditionalGeneration      RobertaForQuestionAnswering                                                                |                                                                                                                                                               |\n|                             | Quit without pass                                        |                                                                                                                                                                                                 | MBartForConditionalGeneration                                                                                                                                 |\n| hf_n10_inference_eager_fp16 | eager_two_runs_differ                                    | CamemBert      PegasusForConditionalGeneration                                                                                                                                                  |                                                                                                                                                               |\n| hf_n10_inference_eager_bf16 | eager_two_runs_differ                                    | ConditionalGeneration      M2M100ForConditionalGeneration      MBartForConditionalGeneration      XGLMForCausalLM                                                                               |                                                                                                                                                               |\n|   hf_n10_train_eager_fp32   | Native API returns: -999       (Unknown PI error)        | BartForConditionalGeneration      BlenderbotForCausalLM      DebertaV2ForMaskedLM      DebertaV2ForQuestionAnswering     MBartForConditionalGeneration      OPTForCausalLM      XGLMForCausalLM | BartForConditionalGeneration      BlenderbotForCausalLM      DebertaV2ForMaskedLM      MBartForConditionalGeneration      OPTForCausalLM      XGLMForCausalLM |\n|                             | Native API returns: -5       (PI_ERROR_OUT_OF_RESOURCES) |                                                                                                                                                                                                 | DebertaV2ForQuestionAnswering                                                                                                                                 |\n|                             | eager_two_runs_differ                                    | BertForMaskedLM      BlenderbotSmallForConditionalGeneration                                                                                                                                    |                                                                                                                                                               |\n|   hf_n10_train_eager_fp16   | eager_two_runs_differ                                    | MegatronBertForCausalLM      PLBartForConditionalGeneration                                                                                                                                     |                                                                                                                                                               |\n|   hf_n10_train_eager_bf16   | eager_two_runs_differ                                    | AllenaiLongformerBase      MegatronBertForQuestionAnswering      OPTForCausalLM                                                                                                                 |                                                                                                                                                               |\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1080. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is closed.\nTraker[Windows] OneDNN upgrade introduces new failures when testing UT and E2E (huggingface models)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1080. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\nclosed\nclosed\nAfter triaging, there should not be related to oneDNN upgrade issue. There is no regression. We will track those issues in other thread. Close this issue.", "root_cause": "https://github.com/intel/torch-xpu-ops/issues/1080#issuecomment-892244440", "state": "closed"}


### Merged Result:1078{"issue_number": 1078, "issue_description": "Run TestFakeTensor with xpu we got a lot of errors of shapes are not equal. For example:\n\n======================================================================\nERROR: test_fake_crossref_backward_amp_nn_functional_multilabel_soft_margin_loss_xpu_float32 (__main__.TestFakeTensorXPU)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/gta/miniforge3/envs/daisy_upstream/lib/python3.10/site-packages/torch/_subclasses/fake_utils.py\", line 182, in __torch_dispatch__\n    torch._prims.utils.compare_tensor_meta(\n  File \"/home/gta/miniforge3/envs/daisy_upstream/lib/python3.10/site-packages/torch/_prims_common/__init__.py\", line 156, in compare_tensor_meta\n    raise AssertionError(msg)\nAssertionError: Shapes torch.Size([0]) and torch.Size([5]) are not equal!\n\nAnother issue is in backward:\n======================================================================\nERROR: test_fake_crossref_backward_amp_nn_functional_multilabel_soft_margin_loss_xpu_float32 (__main__.TestFakeTensorXPU)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/gta/miniforge3/envs/daisy_upstream/lib/python3.10/site-packages/torch/_subclasses/fake_utils.py\", line 182, in __torch_dispatch__\n    torch._prims.utils.compare_tensor_meta(\n  File \"/home/gta/miniforge3/envs/daisy_upstream/lib/python3.10/site-packages/torch/_prims_common/__init__.py\", line 156, in compare_tensor_meta\n    raise AssertionError(msg)\nAssertionError: Shapes torch.Size([0]) and torch.Size([5]) are not equal!\n\nTo reproduce, change test/test_ops.py instantiate_device_type_tests(TestFakeTensor, globals()) ==> instantiate_device_type_tests(TestFakeTensor, globals(), allow_xpu=True) and run\n```PYTORCH_TEST_WITH_SLOW=1 python  test_ops.py -k TestTags```\nThe results for real tensors are aligned with CUDA, but not for fake tensors. The root cause is that there is some CUDA specific codes in aten::log_sigmoid_forward.", "reporter": "daisyden", "assignee": "chunhuanMeng", "resolution": "\npr link:https://github.com/pytorch/pytorch/pull/141333", "root_cause": "The reporter of the issue is daisyden, and the assignee is chunhuanMeng, and the state of the issue is closed.", "state": "closed"}


### Merged Result:1077{"issue_number": 1077, "issue_description": "It is found that the vectorized kernels are not performing well. For instance, copy_() is just utilizing only 40-50% of the theoretical bandwidth.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1077. The reporter of the issue is cfgfung, and the assignee is cfgfung, and the state of the issue is closed.", "reporter": "cfgfung", "assignee": "cfgfung", "resolution": "\n", "root_cause": "The root cause is that when we get an API to get the vector_size / SIMD lane from the LevelZero runtime, the runtime returns just half of the vector width. e.g.: For float32, it returns width 2 instead of width 4.", "state": "closed"}


### Merged Result:1071{"issue_number": 1071, "issue_description": "Sometimes, there is an error 'AssertionError: \"Simulate error\" does not match \"grad can be implicitly created only for scalar outputs\"' in case: test_autograd_xpu.py::TestAutogradDeviceTypeXPU::test_reentrant_parent_error_on_cpu_xpu. reproduce cmd: PYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test_autograd_xpu.py -k test_reentrant_parent_error_on_cpu_xpu\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1071. The reporter of the issue is PenghuiCheng, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "guangyey", "resolution": "\nClosed as completed.", "root_cause": "The issue is caused by the timing of autograd reentrant feature, and it can't be reproduced on the local machine.", "state": "closed"}


### Merged Result:1061{"issue_number": 1061, "issue_description": "extended/test_ops_xpu.py:\n- [x] test_compare_cpu_grid_sampler_2d_xpu_bfloat16\n- [x] test_compare_cpu_grid_sampler_2d_xpu_float16\n- [x] test_compare_cpu_nn_functional_grid_sample_xpu_bfloat16\n- [x] test_compare_cpu_nn_functional_grid_sample_xpu_float16\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1061. The reporter of the issue is xytintel, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "xytintel", "assignee": "daisyden", "resolution": "\ncompleted", "root_cause": "", "state": "closed"}


### Merged Result:1059{"issue_number": 1059, "issue_description": "Existing: https://github.com/intel/torch-xpu-ops/blob/b1582e14f9e27bcbc0666ff636389d2be61783e4/src/comm/DeviceProperties.h#L14-L28\nTODO: https://github.com/intel/llvm/pull/15650\n", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1056{"issue_number": 1056, "issue_description": "Support aten::_convert_weight_to_int4pack.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1056. The reporter of the issue is fengyuan14, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "closed\nPR ready https://github.com/intel/torch-xpu-ops/pull/1035", "root_cause": "", "state": "closed"}


### Merged Result:1055{"issue_number": 1055, "issue_description": "We need op record_stream that is widely used in DDP/FSDP, and issue body Content of #1055 is : ### \ud83d\ude80 The feature, motivation and pitch\nAs titled.\nCould we implement op `aten::record_stream`?\n\ncc @zhangxiaoli73 \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1055. The reporter of the issue is guangyey, and the assignee is , and the state of the issue is closed.", "reporter": "guangyey", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:1054{"issue_number": 1054, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1054. The reporter of the issue is LuFinch, and the assignee is daisyden, and the state of the issue is closed. This is the github issue title glu_backward fp16 has accuracy issues compared with CPU output.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1054. The reporter of the issue is LuFinch, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "LuFinch", "assignee": "daisyden", "resolution": "\ntorch-xpu-ops will align with cuda.", "root_cause": "cuda and xpu does not use accumulate dtype for bfloat16 and flat16, while cpu used.", "state": "closed"}


### Merged Result:1053{"issue_number": 1053, "issue_description": "index_select_xpu cause an IPEX UT fail.  In IPEX2.5, we override this Ops with IPEX implementation to make this UT pass. \n\nipex/tests/gpu/example/test_fp8_index_select.py::TestTorchMethod::test_index_select \n\n'index_select_xpu' not implemented for 'Float8_e4m3fn'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1053. The reporter of the issue is LuFinch, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "LuFinch", "assignee": "daisyden", "resolution": "\nmerged", "root_cause": "created a PR for index_select, https://github.com/intel/torch-xpu-ops/pull/1081/files", "state": "closed"}


### Merged Result:1052{"issue_number": 1052, "issue_description": "Embedding_bag_out does not have boundary check and causes IPEX UT fail\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1052. The reporter of the issue is LuFinch, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "LuFinch", "assignee": "daisyden", "resolution": "In IPEX2.5, we override this Ops with IPEX implementation to make this UT pass\npr merged", "root_cause": "Ops in torch-xpu-ops do not have this check and hence fail in this UT", "state": "closed"}


### Merged Result:1048{"issue_number": 1048, "issue_description": "With 2025 bundle, we got failed on this case \"PYTORCH_TEST_WITH_SLOW=1 pytest -vs test_ops_xpu.py -k test_non_standard_bool_values_index_put_xpu_bool\". The test will transform the input sample with convert_boolean_tensors() and compare the output with transformed input and original input. However, with torch.randint() to evaluate true_vals it failed, while with torch.ones() to evalute true_vals it passed. Is it a problem related to c10 load? @xytintel", "reporter": "daisyden", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}




### Merged Result:1025{"issue_number": 1025, "issue_description": "Clarify branch policy of torch-xpu-ops repo - what's viable/strict branch?, In particular, I see that PyTorch main branch currently points to the https://github.com/intel/torch-xpu-ops/tree/viable/strict of torch-xpu-ops while I was thought it should point to `main` unless we stabilize code prior to release in which case it would point to one of release branches.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1025. The reporter of the issue is dvrogozh, and the assignee is EikanWang, and the state of the issue is open.", "reporter": "dvrogozh", "assignee": "EikanWang", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:1023{"issue_number": 1023, "issue_description": "To port upstream UT to XPU backend we requires the xpu backend support for these APIs : * torch.cuda.amp.autocast for test_ops.py test_fake_crossref_backward_amp * torch.testing._internal.common_device_type._has_sufficient_memory for test_nn.py::TestNNDeviceType::test_avg_pool_large_tensor\nClose as implemented.", "reporter": "daisyden", "assignee": "riverliuintel", "resolution": "\nClose as implemented.", "root_cause": "", "state": "closed"}


### Merged Result:1022{"issue_number": 1022, "issue_description": "The code provided in the issue is not working as expected. It seems that the sort function is not behaving as expected for bool type tensor. The output is not as expected and the code is not sorting the tensor as expected. The issue is reproducible on both xpu and cpu devices.", "reporter": "guizili0", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1016{"issue_number": 1016, "issue_description": "We are using kernel specific max work group size to avoid platform compatibility issue. The routine is,\n```\n  auto kid = ::sycl::get_kernel_id<KernelClass>();\n  auto kbundle = ::sycl::get_kernel_bundle<::sycl::bundle_state::executable>(\n      ctx, {dev}, {kid});\n  sycl::kernel k = kbundle.get_kernel(kid);\n  int max_work_group_size =  k.get_info<::sycl::info::kernel_device_specific::work_group_size>(dev); \n``` \nsycl::get_kernel_bundles gets severe host overhead. The data is as below,\n![image](https://github.com/user-attachments/assets/50788d43-8598-4167-b031-766de9487044)\nImpacts: All kernels in torch-xpu-ops launched with kernel specific max work group are impacted.\n1. 40us overhead is not acceptable for some single batch inference cases, since latency of kernels might be less than 10us.\n2. CUDA runtime usually spends ~6us for a kernel launch.\nhttps://github.com/intel/llvm/issues/15824", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1013{"issue_number": 1013, "issue_description": "Import torch not assert in windows, if install torch XPU on a host without driver installed.\nDone in stock PT main branch.", "reporter": "riverliuintel", "assignee": "ratnampa", "resolution": "\nDone in stock PT main branch.", "root_cause": "", "state": "closed"}


### Merged Result:1012{"issue_number": 1012, "issue_description": "Logcumsumexp has different results between CPU and XPU on BF16/Complex64/Complex128, the issue is closed, the mismatched elements are 2 / 125 (1.6%) for BF16, 2 / 125 (1.6%) for Complex128, 1 / 3 (33.3%) for Complex64, the greatest absolute difference is 0.03125 at index (1, 4, 2) for BF16, 12.566370614359174 at index (3, 3, 0) for Complex128, nan at index (2,) for Complex64, the greatest relative difference is 0.006072998046875 at index (2, 3, 1) for BF16, 1.5103243157406059 at index (3, 4, 0) for Complex128, nan at index (2,) for Complex64, the resolution is not provided, the root cause is not provided.", "reporter": "LuFinch", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:1011{"issue_number": 1011, "issue_description": "KLDivLoss function in pytorch always fallsback to CPU, and issue body Content of #1011 is : ### \ud83d\udc1b Describe the bug\n\nGreetings:\n\nI am trying to use the `torch.nn.KLDivLoss` function on Intel's Max 1550 GPU; however, the function keeps falling back to execute on the CPU.\n\nBelow is a reproducer:\n\n```python\nimport torch\n\nkl_div = torch.nn.KLDivLoss(reduction=\nThe reporter of the issue is jgtong, and the assignee is jgtong, and the state of the issue is closed.", "reporter": "jgtong", "assignee": "jgtong", "resolution": "\nThe issue was resolved by the commit 804a03b76e6b1270327f3f6ddbe58b6ffba5d30e in xpu-ops.", "root_cause": "The issue was caused by a missing op previously. The test passed successfully after the commit.", "state": "closed"}


### Merged Result:1009{"issue_number": 1009, "issue_description": "PyTorch XPU verbose log should be clear and comparable with PyTorch GPU practice. Need to investigate and give the clear implementation requirement on different scenarios. For example, in initialize phase, Torch GPU should give clear software stack, running GPU information. If run Torch XPU in a host with CPU only, it should give the CPU fallback information. When run workload, it should give comparable verbose message as stock Pytorch. When run in error, it should give helpful error message and reminder to report issue or get help message. error message for some other exceptional cases", "reporter": "riverliuintel", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1008{"issue_number": 1008, "issue_description": "Use Complie and driver compression feature to AOT source compile more GPU target into one Torch wheels. It requires: 1) the binary size is comparable with PyTorch GPU wheels. 2) OS coverage: Windows and Linux\n", "reporter": "riverliuintel", "assignee": "fengyuan14", "resolution": "closed\n", "root_cause": "", "state": "closed"}


### Merged Result:1007{"issue_number": 1007, "issue_description": "There are two scenarios for PyTorch XPU release. 1) pip install torch wheels in one host machine without GPU driver installed. Import torch will fallback to CPU. 2) install driver and pip install torch for AI workload run on GPU. 3) install driver, install deep-learning-essential bundle and pip install torch can work well on GPU. 4) install driver, install deep-learning-essential bundle and source build pyTorch.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1007. The reporter of the issue is riverliuintel, and the assignee is chuanqi129, and the state of the issue is closed.", "reporter": "riverliuintel", "assignee": "chuanqi129", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:1006{"issue_number": 1006, "issue_description": "PyTorch CI/CD requires to be enhanced to support Torch/vision/audio distribution in both Windows and Linux. And set up essential Windows/Linux CI to improve the development break frequency.\n\n1. Enabled Torch vision and audio in Windows CD.\n2. Windows CI testing (depends on public client GPU instance ready)\n3. Enabled Torch nightly with e2e nightly in stock PyTorch.\nTorchvision and audio are enabled in Windows CD.", "reporter": "riverliuintel", "assignee": "chuanqi129", "resolution": "\ndone", "root_cause": "", "state": "closed"}


### Merged Result:1005{"issue_number": 1005, "issue_description": "Integrate oneDNN GEMM INT4 kernels, and serves for Torchao LLM usage. It requires pass UT and example workloads usage.", "reporter": "riverliuintel", "assignee": "ZhiweiYan-96", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:1004{"issue_number": 1004, "issue_description": "Analyze Triton kernels data and report to Triton XPU.\n1. Recollect reasonable competitive GPU performance data\n2. Use TorchInductor built-in benchmark tool to detect slower XPU triton kernels.\nscatter op issue: https://github.com/intel/intel-xpu-backend-for-triton/issues/2665", "reporter": "riverliuintel", "assignee": "retonym", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:1003{"issue_number": 1003, "issue_description": "Request INT8 quantization (PT2E) feature on Linux. It requires, implement PT2E infrastructure for Intel GPU path, complete essential oneDNN, Triton quantized INT8 ops, pass benchmark models quantization testing. And complete essential docs changes\nfunc done in PyTorch main branch.", "reporter": "riverliuintel", "assignee": "ZhiweiYan-96", "resolution": "closed\nfunc done in PyTorch main branch.", "root_cause": "", "state": "closed"}


### Merged Result:1002{"issue_number": 1002, "issue_description": "Implement the AOTInuctor and torch.export on Intel GPU Linux. It requires enabling the model binary store/load mechanism, support ABI netural calling and enabling feature UT.\ndone", "reporter": "riverliuintel", "assignee": "etaf", "resolution": "closed\nready in PT2.7", "root_cause": "", "state": "closed"}


### Merged Result:1001{"issue_number": 1001, "issue_description": "PyTorch 2.6 Aten ops coverage support requirement >= 80% and pin to stock PyTorch repo before feature freeze. It requires to pass 100% UT on both Linux and Windows. The platform needs to cover PVC and client GPU.\nThe coverage goal of the operation has been met.", "reporter": "riverliuintel", "assignee": "xytintel", "resolution": "\nThe coverage goal of the operation has been met.", "root_cause": "", "state": "closed"}


### Merged Result:1000{"issue_number": 1000, "issue_description": "Need to enable XPU path in front-end level and implement essential custom kernels for popular PyTorch libaries. It includes:\n1) Redefine the code infrastructure and add xpu path in front-end API support\n2) Implement essential custom kernels by Triton\n3) Set up CI build\n4) Enable XPU build\n5) docs support\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1000. The reporter of the issue is riverliuintel, and the assignee is , and the state of the issue is closed.", "reporter": "riverliuintel", "assignee": "", "resolution": "\nclose this", "root_cause": "", "state": "closed"}


### Merged Result:999{"issue_number": 999, "issue_description": "Windows build log size is too big to open in web, which impact issue triage in CI. Need to enhance windows build log and clean the warning message in Windows.\nThe excessive verbosity was caused by SYCL compiler `/clang:-MD` used with `-fsycl-host-compiler=cl.exe`, which invoked `-E`. `-E` emits the preprocessed source code to stdout, causing the excessive verbosity.", "reporter": "riverliuintel", "assignee": "min-jean-cho", "resolution": "\nThe issue has been resolved with SYCL compiler of oneAPI 2025.", "root_cause": "/clang:-MD used with `-fsycl-host-compiler=cl.exe`, which invoked `-E`.", "state": "closed"}


### Merged Result:998{"issue_number": 998, "issue_description": "Huggingface accuracy have reached reasonable pass rate on PVC Linux on both torch.complie and eager. For windows on Client GPU, it only can support eager accuracy. It requests to test all three benchmarks eager mode and triage the failure reason to reach a reasonable pass rate. Take LNL as an reference platform. Some checkpoints: 1. Fully run huggingface benchmark e2e test result on eager. 2. Set up CI test task (Weekly can be workaround before CI machine ready) 3. Give the investigate report and fix plan within two weeks. 4. Fix bugs, give the fix reason and upstream the code.", "reporter": "riverliuintel", "assignee": "Stonepia", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:997{"issue_number": 997, "issue_description": "Torch-xpu-ops have reached almost 100% pass rate on PVC and only few left know issues. For windows on Client GPU, need to analyze the Torch-xpu-ops UT test result and triage the failure to reach compatible UT pass rate. Take LNL as an reference platform.  Some checkpoints: 1. Start JIRA investigate from Test report and run left all scope of torch-xpu ops UT testing, and provide analysis report and execution plan within one week. 2. Set up CI test task (nightly can be workaround before CI machine ready) 3. Fix UT bugs, give the fix reason and upstream the code\n99% pass rate", "reporter": "riverliuintel", "assignee": "Stonepia", "resolution": "\nDone", "root_cause": "", "state": "closed"}


### Merged Result:987{"issue_number": 987, "issue_description": "Build with new oneAPI will got failed with WERROR=1\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/987. The reporter of the issue is mengfei25, and the assignee is mengfei25, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "mengfei25", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/1070", "root_cause": "https://github.com/intel/torch-xpu-ops/pull/989", "state": "closed"}


### Merged Result:986{"issue_number": 986, "issue_description": "The operator 'c10d::allgather_' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/986. The reporter of the issue is zhiyuan1i, and the assignee is Chao1Han, and the state of the issue is open.", "reporter": "zhiyuan1i", "assignee": "Chao1Han", "resolution": "\n", "root_cause": "zhiyuan1i does not have an Intel multi-GPU environment at hand to test the issue.", "state": "open"}


### Merged Result:982{"issue_number": 982, "issue_description": "Whether `CompositeExplicitAugograd` codegen flag is needed requires further investigation. The reporter of the issue is xytintel, and the assignee is xytintel, and the state of the issue is closed.\nThe issue is about the `CompositeExplicitAutograd` key in the `derivatives.yaml` file, which is used for kernels that work for all backends but require an explicit definition of backward function to support autograd. The most typical use is for delegating functions. The conclusion is that we don't need it.", "reporter": "xytintel", "assignee": "xytintel", "resolution": "closed\nThe issue is resolved and the conclusion is that we don't need it.", "root_cause": "", "state": "closed"}


### Merged Result:979{"issue_number": 979, "issue_description": "Details in https://github.com/intel/torch-xpu-ops/actions/runs/11361002852\n\ndev | name | batch_size | accuracy\n-- | -- | -- | --\nxpu | jx_nest_base | 8 | fail_accuracy\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | pass\nxpu | jx_nest_base | 8 | fail_accuracy\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/979. The reporter of the issue is mengfei25, and the assignee is jianyizh, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\n", "root_cause": "The issue is a random accuracy issue on local, and triton does not support deterministic. The result gets non-deterministic after avg pool [8, 512, 14, 14] -> [8, 512, 1, 1], inductor optimize it into a single aten.mean. However I cannot use UT to reproduce. I suggest accept this variance.", "state": "open"}


### Merged Result:978{"issue_number": 978, "issue_description": "2.5 aten::linear also introduced an additional aten::copy_, that make aten::linear latency dropped from 308us to 426us.\nAutocast difference between IPEX and torch-xpu-ops leads to the additional copy. According to the current requirement, it is not a defect.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nNot a defect", "root_cause": "Autocast difference between IPEX and torch-xpu-ops leads to the additional copy", "state": "closed"}


### Merged Result:977{"issue_number": 977, "issue_description": "2.5 aten::layer_norm introduced 3 aten::copy_, that make the latency dropped from 150us to 401us.\nAdditional three copies are introduced by Autocast. torch-xpu-ops aligns Autocast policy with PyTorch CUDA, where LayerNorm requires FP32 in computation. And in IPEX, LayerNorm could stay on BF16 according to IPEX custom Autocast policy.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nClose won't fix, as currently we follow rules to align with CUDA impl and guarantee accuracy. If we have performance consideration in future, we can file new issue.", "root_cause": "Additional three copies are introduced by Autocast. torch-xpu-ops aligns Autocast policy with PyTorch CUDA, where LayerNorm requires FP32 in computation. And in IPEX, LayerNorm could stay on BF16 according to IPEX custom Autocast policy.", "state": "closed"}


### Merged Result:970{"issue_number": 970, "issue_description": "CPU time as below,\n\noverride             aten::sum         2.52%      24.765ms         4.07%      39.978ms      60.849us \nnon-override         aten::sum         4.49%      46.832ms         5.74%      59.905ms      91.180us\n\nSame root cause, https://github.com/intel/llvm/issues/15824", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "\n", "root_cause": "https://github.com/intel/llvm/issues/15824", "state": "open"}


### Merged Result:969{"issue_number": 969, "issue_description": "Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg      Self XPU    Self XPU %     XPU total  XPU time avg    # of Calls\nnon override         aten::nonzero         5.50%      63.456ms        54.60%     630.302ms     489.365us       5.160ms         4.35%      34.468ms      26.761us          1288\noverride             aten::nonzero         5.40%      58.551ms        52.64%     570.870ms     443.222us       6.688ms         5.55%      34.737ms      26.970us          1288\nThe low performance is caused by SYCL API, which we used to query kernel specific max work group size. We file issue to compiler to track this issue.", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "\nThe low performance is caused by SYCL API, which we used to query kernel specific max work group size. We file issue to compiler to track this issue.", "root_cause": "The low performance is caused by SYCL API, which we used to query kernel specific max work group size.", "state": "open"}


### Merged Result:964{"issue_number": 964, "issue_description": "Unit test: Port all necessary unit tests from test/test_cuda.py, and the current issue is that the reporter xytintel has not provided a resolution or root cause information.", "reporter": "xytintel", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "open"}


### Merged Result:957{"issue_number": 957, "issue_description": "test_autograd_xpu.py::TestAutograd::test_node_ordering_when_none_returned - AssertionError: Torch not compiled with CUDA enabled\n\ntest_linalg_xpu.py::TestLinalgXPU::test__int_mm_errors_xpu - AssertionError: RuntimeError not raised by <lambda>\nreport in https://github.com/intel/torch-xpu-ops/issues/821 and https://github.com/intel/torch-xpu-ops/issues/814", "reporter": "PenghuiCheng", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:956{"issue_number": 956, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/956. The reporter of the issue is PenghuiCheng, and the assignee is , and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:954{"issue_number": 954, "issue_description": "torch-xpu-ops support building with clang as host compiler on linux\nClang compiler fails due to duplicate symbols in torchgen.gen headers and lld linker issues.", "reporter": "gglin001", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "Clang compiler fails due to duplicate symbols in torchgen.gen headers and lld linker issues.", "state": "open"}


### Merged Result:942{"issue_number": 942, "issue_description": "F.scaled_dot_product_attention need XETLA support to avoid the SD and Bert training regression in IPEX 2.5 test. IPEX got this failure: [PVC][PT2.5][Bundle0.5.3.36/2024.2.1] stable-diffusion train 10% perf regression. From oneDNN verbose SD convolution time in 2.5 is the same as 2.3. While there are some additional gemm ops in 2.5 as mentioned by Shufan, and we can see those ops takes a lot of time, like below table. The issue is introduced by F.scaled_dot_product_attention, we will need a patch from <a class=\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/942. The reporter of the issue is daisyden, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "majing921201", "resolution": "\nClose it due to product plan change", "root_cause": "", "state": "closed"}


### Merged Result:941{"issue_number": 941, "issue_description": "ipex convolution switched to stock pytorch, because we don't have tf32 support this case got 46% regresion compared to ipex 2.3 \n\nRVP | resnet50_tf32_train_plain_nhwc | High | 256 | FAIL | 1650.43 | 1672.41 | 902.86 | -46%\n\nHere is a comparison of verbose:\n\nconvolution | jit:ir | forward_training | src_f32::blocked:acdb::f0 wei_f32::blocked:acdb::f0 bia_undef::undef::: dst_f32::blocked:acdb::f0 | attr-scratchpad:user attr-fpmath:tf32 | alg:convolution_direct | mb16_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1 | 0.087158\n\nconvolution | jit:ir | forward_training | src_f32::blocked:acdb::f0 wei_f32::blocked:acdb::f0 bia_undef::undef::: dst_f32::blocked:acdb::f0 | attr-scratchpad:user | alg:convolution_direct | mb16_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1 | 0.196045\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/941. The reporter of the issue is daisyden, and the assignee is ZhiweiYan-96, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:939{"issue_number": 939, "issue_description": "Performance: Improve UpsampleBilinear forward backward performance to be on-par as oneDNN.\nThe reporter of the issue is fengyuan14, and the assignee is majing921201, and the state of the issue is open.", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "\n", "root_cause": "The performance of the channels last kernel is on-par with cuda, but still has a gap with oneDNN.", "state": "open"}


### Merged Result:938{"issue_number": 938, "issue_description": "Performance: Evaluate MaxPool2d forward backward performance gap between IPEX.\nThe performance gap between pointnet_bf16_ipex25_1108 and pointnet_bf16_ipex25_1108_stock_pytorch is due to the use of IPEX's max_pool2d function, which has a special handling path for certain cases that stock PyTorch's max_pool2d does not have. This special handling path in IPEX provides better parallelism.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "The issue is closed.\nThe root cause of the performance gap is that IPEX has a special handling path for certain cases which stock PyTorch's max_pool2d does not have. This special handling path in IPEX provides better parallelism. A pull request to fix this issue has been merged into the main branch.", "root_cause": "IPEX has a special handling path for certain cases which stock PyTorch's max_pool2d does not have. This special handling path in IPEX provides better parallelism.", "state": "closed"}


### Merged Result:937{"issue_number": 937, "issue_description": "Performance: Improve BatchNormalization forward/backward to align with oneDNN implementation. Basing on the consideration of accuracy, we followed the PyTorch CUDA implementation, using Welford algorithm and similar kernel template. Will improve the kernel template with vectorized load/store. _\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/937. The reporter of the issue is fengyuan14, and the assignee is xytintel, and the state of the issue is open.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "\nMerged", "root_cause": "", "state": "open"}


### Merged Result:928{"issue_number": 928, "issue_description": "test_dataloader UT failed in CI\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/928. The reporter of the issue is majing921201, and the assignee is PenghuiCheng, and the state of the issue is closed.", "reporter": "majing921201", "assignee": "PenghuiCheng", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:922{"issue_number": 922, "issue_description": "Unittest: New failures after PyTorch uplift\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/922. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nisin cases are passed on pytorch 41977a05314bbf537e1c5d6cf5916a368d1907d9 and torch-xpu-ops 999094bc948b3afd21162784dead1e765c60a376.", "root_cause": "Pr for unique: https://github.com/intel/torch-xpu-ops/pull/963", "state": "closed"}


### Merged Result:919{"issue_number": 919, "issue_description": "We have witnessed that when running models, there are some warnings from Triton, that would be like this:\n\n(I): Detected 9472 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 512 spills\n(I): Detected 20032 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 10816 spills\n(I): Detected 33600 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 25408 spills\n\nThis is because we didn't set the `grf_mode` in the triton config, and there are register spills exceeding the thresh_hold. Thus it triggers an automatic using large grf mode re-compile for the Triton kernel.\n\nThis is the expected behavior. We have two options:\n1. Set the `grf_mode=auto` in inductor side. So that when there is xpu, the `triton.Config` will have this kwarg.\n2. Discuss with the Triton team about hiding this from end users. These outputs should be treated as warnings.\n\nAfter the offline discussion, we think option 2 is better, because we need to keep from the inductor side,  that there will be no difference between XPU and CUDA/HIP. We wish to always keep the same config for all kinds of devices. The \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/919. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nhttps://github.com/intel/intel-xpu-backend-for-triton/pull/2385", "root_cause": "", "state": "closed"}


### Merged Result:918{"issue_number": 918, "issue_description": "AssertionError: Get None or [] without decomp\n    \"test_comprehensive_nansum_xpu_bool\",\n    \"test_comprehensive_nansum_xpu_int\",\n    \"test_comprehensive_nansum_xpu_uint8\",\n    \"test_quick_nansum_xpu_bool\",\n    \"test_quick_nansum_xpu_int\",\n    \"test_quick_nansum_xpu_uint8\",\n    \"test_comprehensive_nn_functional_binary_cross_entropy_with_logits_xpu\",\nRuntimeError: \"avg_pool2d_xpu\" not implemented for 'Long'\n    \"test_comprehensive_nn_functional_avg_pool1d_xpu_int64\",\n    \"test_comprehensive_nn_functional_local_response_norm_xpu_int64\",\nRuntimeError: Difference from float64 is larger with decomposition nll_loss2d_backward.default than original on output 0. Original max diff: 1.1224856321843946e-05, Decomp max diff: 1.9292721803156054e-05 atol = 1e-07\n    \"test_comprehensive_nn_functional_nll_loss_xpu_float16\"", "reporter": "yuchengliu1", "assignee": "PenghuiCheng", "resolution": "", "root_cause": "", "state": "closed"}




### Merged Result:912{"issue_number": 912, "issue_description": "torchbench accuracy failed\nE2E accuracy issue on Ubuntu 22.04", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nThe issue will be closed in Ubuntu 24.10", "root_cause": "The issue is related to the version of Ubuntu and the specific model being tested.", "state": "closed"}


### Merged Result:911{"issue_number": 911, "issue_description": "Accuracy failed for key name albert.embeddings.token_type_embeddings.weight.grad\nThe issue is related to the cosine similarity test for the huggingface models on Ubuntu 22.04 and Ubuntu 24.04. The test fails on Ubuntu 22.04 but passes on Ubuntu 24.04. The root cause is related to the layer norm backward process, which causes the cosine similarity test to fail. The solution is to modify the patch and compare the result directly, without putting fp64_outputs on xpu and putting new_result and correct_result on cpu to compare. The issue is closed.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nThe issue is closed.", "root_cause": "The root cause is related to the layer norm backward process, which causes the cosine similarity test to fail.", "state": "closed"}


### Merged Result:910{"issue_number": 910, "issue_description": "Failures on ARC windows, total 2127,FP64 related issue: 1910, others: 217\n- AssertionError: 'Assertion `cur_target >= 0 && cur_target < n_classes` failed' not found in 'PYTORCH_API_USAGE'\n- AssertionError: \"Kernel\\ is\\ incompatible\\ with\\ all\\ devices\\ in\\ devs\" does not match \"Required aspect fp64 is not supported on the device\"\n- AssertionError: \"not implemented for\" does not match \"Native API failed. Native API returns: -999 (Unknown PI error) -999 (Unknown PI error)\"\n- AssertionError: RuntimeError not raised\n- AssertionError: Scalars are not close!\n- AssertionError: Tensor-likes are not close!\n- AssertionError: Tensor-likes are not equal!\n- Exception: Caused by sample input at index\n- RuntimeError: Caught RuntimeError in DataLoader worker process 1.\n- RuntimeError: Comparing\n- RuntimeError: could not create a primitive descriptor for a convolution forward propagation primitive\n- RuntimeError: could not create a primitive descriptor for a deconvolution forward propagation primitive\n- RuntimeError: Default context is not supported on XPU on Windows. So we can NOT find its global index of the ATen device.\n- RuntimeError: Kernel is incompatible with all devices in devs\n- RuntimeError: Loader error\n- RuntimeError: Native API failed. Native API returns: -999 (Unknown PI error) -999 (Unknown PI error)\n- RuntimeError: Ninja is required to load C++ extensions\n- RuntimeError: Required aspect fp64 is not supported on the device\n- RuntimeError: Worker error\n- RuntimeError: XPU out of memory, please use `empty_cache` to release all unoccupied cached memory.\n- AssertionError: Scalars are not close!\n- Exception: Caused by sample input at index\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/910. The reporter of the issue is mengfei25, and the assignee is min-jean-cho, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "min-jean-cho", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:907{"issue_number": 907, "issue_description": "The issue is found in codegen PR, where `aten::_assert_async.msg` is called in op multinomial. It affects the uts in `extended/test_ops_xpu.py`\nThe issue is about the performance of the torch_xpu_ops library, specifically the problem with the `torch_xpu_ops.nn.functional.conv2d` function. The reporter of the issue is ZhiweiYan-96, and the assignee is ZhiweiYan-96. The state of the issue is closed. The root cause of the issue is that the convolution operation is not optimized for the XPU hardware, leading to performance degradation. The relevant PR that addresses this issue is PR #955, which has been merged and resolved the problem.", "reporter": "ZhiweiYan-96", "assignee": "ZhiweiYan-96", "resolution": "\nThe convolution operation is now optimized for the XPU hardware, leading to improved performance.", "root_cause": "The convolution operation is not optimized for the XPU hardware, leading to performance degradation.", "state": "closed"}


### Merged Result:906{"issue_number": 906, "issue_description": "scatter_add needs xpu device check in aten operators\nscatter_add needs xpu device check in aten operators\nscatter_add needs xpu device check in aten operators\nThe failed UT is skipped in codegen PR currently", "reporter": "ZhiweiYan-96", "assignee": "ZhiweiYan-96", "resolution": "\nclosed\nclosed\n", "root_cause": "The issue is introduced in codegen pr https://github.com/intel/torch-xpu-ops/pull/310. The FAILED UT throw errors like RuntimeError: scatter_add_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. The affected UTs include test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_add__xpu_complex64 FAILED [ 23%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_add__xpu_float16 FAILED [ 25%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_add__xpu_float32 FAILED [ 26%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_add_mult_index_base_xpu_float32 FAILED [ 28%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_reduce_mean_xpu_bfloat16 FAILED [ 60%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_reduce_mean_xpu_float16 FAILED [ 61%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_reduce_mean_xpu_float32 FAILED [ 63%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_reduce_mean_xpu_float64 FAILED [ 64%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_reduce_mean_xpu_int16 FAILED [ 66%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_reduce_mean_xpu_int32 FAILED [ 67%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_reduce_mean_xpu_int64 FAILED [ 69%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_reduce_mean_xpu_int8 FAILED [ 70%], test_scatter_gather_ops_xpu.py::TestScatterGatherXPU::test_scatter_reduce_mean_xpu_uint8 FAILED [ 71%] ", "state": "closed"}


### Merged Result:905{"issue_number": 905, "issue_description": "Looks like there is a random issue for Super_SloMo, and it will be passed with WHL install from prebuild but failed with source build. In latest weekly, WHL Passed: https://github.com/intel/torch-xpu-ops/actions/runs/10742335908 Source build Failed: https://github.com/intel/torch-xpu-ops/actions/runs/10741560513 And I tested WHL locally multiple times and it is passed randomly.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/905. The reporter of the issue is mengfei25, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:904{"issue_number": 904, "issue_description": "xpu train timm_efficientnet\n[WARNING] Failed to create Level Zero tracer: 2013265921\n(I): Detected 2048 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 1024 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 1024 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 1024 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\nE0907 00:54:25.073000 2172206 site-packages/torch/_dynamo/utils.py:1798] RMSE (res-fp64): 0.00040, (ref-fp64): 0.00010 and shape=torch.Size([4, 96, 1, 1]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000, use_larger_multiplier_for_smaller_tensor: 0\nE0907 00:54:25.074000 2172206 site-packages/torch/_dynamo/utils.py:1670] Accuracy failed for key name blocks.1.0.se.conv_reduce.weight.grad\nfail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/904. The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\nPassed in latest weekly test", "root_cause": "", "state": "closed"}


### Merged Result:901{"issue_number": 901, "issue_description": "Torchbench basic_gnn models performance regression, the performance of basic_gnn_gcn, basic_gnn_gin, basic_gnn_sage, basic_gnn_edgecnn models in release 2.5.0 is lower than the performance in main branch. The regression is more obvious in release 2.5.0 compared to main branch. The regression is more obvious in release 2.5.0 compared to main branch in eager mode. The regression is more obvious in release 2.5.0 compared to main branch in inductor mode.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/901. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nClose as this is too old.", "root_cause": "", "state": "closed"}


### Merged Result:900{"issue_number": 900, "issue_description": "xpu  eval  jx_nest_base                        [WARNING] Failed to create Level Zero tracer: 2013265921\n(I): Detected 1024 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 8192 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 8192 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 4096 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 4096 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\nE0912 00:16:10.029000 3264502 site-packages/torch/_dynamo/utils.py:1802] RMSE (res-fp64): 0.00087, (ref-fp64): 0.00036 and shape=torch.Size([8, 1000]). res.dtype: torch.float16, multiplier: 2.000000, tol: 0.001000, use_larger_multiplier_for_smaller_tensor: 0\nfail_accuracy\nThe reporter of the issue is mengfei25, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nThe issue was resolved locally by jianyizh on pvc 1550.", "root_cause": "The issue was caused by the kernel recompiling using large GRF mode and the kernel has now 0 spills.", "state": "closed"}


### Merged Result:899{"issue_number": 899, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/899. The reporter of the issue is mengfei25, and the assignee is PenghuiCheng, and the state of the issue is closed. Details in https://github.com/intel/torch-xpu-ops/actions/runs/10806301704/job/29974962919. - RuntimeError: \nscatter_reduce_mean is nondeterministic, we need to update threshold.", "reporter": "mengfei25", "assignee": "PenghuiCheng", "resolution": "\nThe issue was fixed by the latest version of torch-xpu-ops.", "root_cause": "scatter_reduce_mean is nondeterministic.", "state": "closed"}


### Merged Result:891{"issue_number": 891, "issue_description": "Reproducing step:\n1. enable `test/inductor/test_torchinductor_opinfo.py` with this PR:\n  https://github.com/pytorch/pytorch/pull/134556\n2. `python test/inductor/test_torchinductor_opinfo.py -k addmm_xpu`\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/891. \nThe reporter of the issue is hoshibara, \nand the assignee is ZhiweiYan-96,\nand the state of the issue is closed.\nThe reporter of the issue is hoshibara, and the assignee is ZhiweiYan-96, and the state of the issue is closed.", "reporter": "hoshibara", "assignee": "ZhiweiYan-96", "resolution": "\nfixed", "root_cause": "https://github.com/pytorch/pytorch/pull/139721", "state": "closed"}


### Merged Result:890{"issue_number": 890, "issue_description": "Unittest: Evaluate remaining unported test suites.\n\n- [ ] test_type_promotion\n\nPlease evaluate if there are more required suites.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/890. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nPR submitted https://github.com/intel/torch-xpu-ops/pull/965", "root_cause": "", "state": "closed"}


### Merged Result:889{"issue_number": 889, "issue_description": "to(torch.int8) will get different result on XPU.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/889. The reporter of the issue is hoshibara, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "hoshibara", "assignee": "majing921201", "resolution": "\nclosed", "root_cause": "The root cause of the issue is the undefined behavior of overflow in XPU, as both IGC and SYCL spec don't define overflow behavior, so different values are expected.", "state": "closed"}


### Merged Result:887{"issue_number": 887, "issue_description": "Unittest: New failures on unfold. The failures are on the following tests: test_dtypes_nn_functional_unfold_xpu, test_non_standard_bool_values_nn_functional_unfold_xpu_bool, test_compare_cpu_nn_functional_unfold_xpu_bool, test_non_standard_bool_values_nn_functional_unfold_xpu_bool, test_nn_unfold_xpu. The issue is reported by fengyuan14 and assigned to daisyden. The issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:884{"issue_number": 884, "issue_description": "For more details, please refer to https://jira.devtools.intel.com/browse/PYTORCHDGQ-5162?\nTwo performance limitations of hardware have been identified: 1. Instruction bound on PVC, 2. Memory/cache efficiency of broadcast case.", "reporter": "xiaowangintel", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "1. Instruction bound on PVC, 2. Memory/cache efficiency of broadcast case.", "state": "closed"}


### Merged Result:882{"issue_number": 882, "issue_description": "For OPTForCausalLM train on stock pytorch, aten::eq cost time on pvc-1100 worse than A100 * ratio.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/882. The reporter of the issue is xiaowangintel, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "xiaowangintel", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:881{"issue_number": 881, "issue_description": "For OPTForCausalLM train on stock pytorch, aten::lt cost time on pvc-1100 worse than A100 * ratio.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/881. The reporter of the issue is xiaowangintel, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "xiaowangintel", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:878{"issue_number": 878, "issue_description": "output of cdist op on XPU device is defferent with CPU op when p=2 and mode=2.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/878. The reporter of the issue is PenghuiCheng, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "xytintel", "resolution": "\nFixed in https://github.com/intel/torch-xpu-ops/pull/873", "root_cause": "", "state": "closed"}


### Merged Result:877{"issue_number": 877, "issue_description": "add conv and matrix multiple related ops in extended UT\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/877. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "state": "open"}


### Merged Result:875{"issue_number": 875, "issue_description": "We found there are ~180 cases got failed specifically on MTL, it can pass on PVC, on ARC the cases will fail in op creation with fp64 cases excluded. MTL: [mtl.log](https://github.com/user-attachments/files/16885899/mtl.log) ARC log for convolution: [arc.log](https://github.com/user-attachments/files/16886459/arc.log) RuntimeError: could not create a primitive descriptor for a convolution forward propagation primitive\noneDNN issues on MTL and ARC, ilated_transposed_has_bias_False_strided_False_contiguous_True_xpu\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/875. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is closed. There are kinds of issues on MTL:\n- [ ] Jacobian mismatch\n- [ ] Tensor-likes are not close, we could adjust threshold\ndifference: 4.11529848906818e-07 at index (0, 0, 7) (up to 1e-07 allowed)\ndeconvolution issue: https://jira.devtools.intel.com/browse/MFDNN-12448\uff0c implemented, waiting for the new oneDNN 3.7 integration to stock pytorch.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\nclosed\nclosed\n\nimplemented", "root_cause": "https://github.com/intel/torch-xpu-ops/issues/875", "state": "closed"}


### Merged Result:861{"issue_number": 861, "issue_description": "On 22.04, this case cause segmentation fault, \"PYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 gdb - -args Python -m pytest -v test_torch_xpu.py -k test_to_with_tensor\".\nUser case defect. Need be aware of async execution and CPU tensor life cycle.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "The reporter of the issue is daisyden, and the assignee is fengyuan14, and the state of the issue is closed.", "state": "closed"}


### Merged Result:849{"issue_number": 849, "issue_description": "Need `getStreamFromExternal` and `stream()` API  in XPUStreamfor AOT Inductor.\n", "reporter": "etaf", "assignee": "guangyey", "resolution": "\nClosed as it is completed.", "root_cause": "", "state": "closed"}


### Merged Result:845{"issue_number": 845, "issue_description": "case list: test_compare_cpu_nn_functional_adaptive_avg_pool3d_xpu_bfloat16, test_compare_cpu_nn_functional_adaptive_avg_pool3d_xpu_float16, failed on xpu for \"AssertionError: Tensor-likes are not close!\", but I found cuda doesn't run these two cases, only run the case \"test_compare_cpu_nn_functional_adaptive_avg_pool3d_xpu_float32\", maybe we should align with cuda and skip these cases if cuda fails too. Please help to check.\ncuda fails too, I think we should skip these two cases.", "reporter": "chunhuanMeng", "assignee": "daisyden", "resolution": "\nskip these two cases", "root_cause": "", "state": "closed"}


### Merged Result:842{"issue_number": 842, "issue_description": "Windows only) Pow operator gives incorrect result in UT test_binary_ufuncs_xpu.py::TestBinaryUfuncsXPU::test_pow_xpu_float16.  #798 is to fix this failure. However, additional overhead was introduced in type cast so we need to record this.  The failure is related to the cast of complex half type in kernel. If we convert with opmath_t{}, other ops like log will also gives incorrect results.\nThis is a bug due to the compiler.", "reporter": "Kanya-Mo", "assignee": "Kanya-Mo", "resolution": "fix\n", "root_cause": "cast of complex half type in kernel", "state": "closed"}


### Merged Result:839{"issue_number": 839, "issue_description": "There is lack of XPU support in of `toAccumulateType`.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/839. The reporter of the issue is fengyuan14, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "closed\nMerged in https://github.com/pytorch/pytorch/pull/134465", "root_cause": "", "state": "closed"}


### Merged Result:827{"issue_number": 827, "issue_description": "A reproducer for the behavior of `index_put_` which is inconsistency with other backends.\n\n```python\nimport torch\n\ninput = torch.randn(4, 4, device=\"xpu\")\nindex = torch.randint(4, (4,), device=\"xpu\").int()\nsrc = torch.randn(4, device=\"xpu\")\n\ntorch.index_put_(input, (index, index), src, True)\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/827. The reporter of the issue is guangyey, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "guangyey", "assignee": "Stonepia", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/597", "root_cause": "Please help check the behavior of other ops which also use `checkIndexTensorTypes`.", "state": "closed"}




### Merged Result:821{"issue_number": 821, "issue_description": "Retriage for PT2.6, old issue is https://github.com/intel/torch-xpu-ops/issues/577\n# XPU supported OP:\n - [x] linalg_vector_norm:\n ```\n# RuntimeError: Fail to enable Kineto Profiler on XPU due to error code: 200\n        \nFor the first two cases, the root cause there are cuda bias codes(https://github.com/pytorch/pytorch/blob/6afcec0c582cb852fcf673ea3b6ce12e4b9da01d/aten/src/ATen/native/ReduceOpsUtils.h#L223), we should avoid explicitly casting low precision inputs to fp32 like cuda, need to raise a pr in stock pytorch for adding condition for xpu.", "reporter": "yuchengliu1", "assignee": "PenghuiCheng", "resolution": "\npr merged", "root_cause": "cuda bias codes(https://github.com/pytorch/pytorch/blob/6afcec0c582cb852fcf673ea3b6ce12e4b9da01d/aten/src/ATen/native/ReduceOpsUtils.h#L223), need to raise a pr in stock pytorch for adding condition for xpu", "state": "closed"}


### Merged Result:817{"issue_number": 817, "issue_description": "bincount and uniform_ ops with hard coded fp64 will cause ARC test failures\nbincount is expected to use double, so we will skip it on ARC on xpu backend.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nhooks added", "root_cause": "sample inputs generated double data", "state": "closed"}


### Merged Result:816{"issue_number": 816, "issue_description": "For LayoutLMForSequenceClassification model on stock pytorch, index_select cost time on pvc-1100 worse than A100 * ratio, please refer to https://jira.devtools.intel.com/browse/PYTORCHDGQ-5080.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/816. The reporter of the issue is xiaowangintel, and the assignee is xiaowangintel, and the state of the issue is closed.", "reporter": "xiaowangintel", "assignee": "xiaowangintel", "resolution": "\nfixed", "root_cause": "xpu performance is not targeted to PT 2.6", "state": "closed"}


### Merged Result:814{"issue_number": 814, "issue_description": "The feature, motivation and pitch\n\nhttps://pytorch.org/docs/stable/cuda.html#tunableop\nPlease make sure we can pass the following UTs.\n\n- test_bmm_tunableop_rocm_xpu_float32\n- test_numeric_check_leak_tunableop_rocm_xpu_float32\n- test_matmul_small_brute_force_tunableop_xpu_float16\n- test_matmul_small_brute_force_tunableop_xpu_float32\n- test_matmul_small_brute_force_tunableop_xpu_float64\n- test_addmm_relu_tunableop_rocm_xpu_float32\n- test_addmm_relu_tunableop_rocm_xpu_float64\n- test_matmul_offline_tunableop_xpu_float16\nNo plan to support tunable in 2.6, close this issue.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\nclosed", "root_cause": "No plan to support tunable in 2.6", "state": "closed"}


### Merged Result:811{"issue_number": 811, "issue_description": "The op is expected to fallback to CPU, see https://github.com/intel/torch-xpu-ops/blob/main/src/ATen/native/xpu/XPUFallback.template#L239, but it is not implemented in CPU backend.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/811. The reporter of the issue is daisyden, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nfixed", "root_cause": "", "state": "closed"}


### Merged Result:809{"issue_number": 809, "issue_description": "test_ops_fwd_gradients_xpu.py::TestFwdGradientsXPU::test_fn_fwgrad_bwgrad_nn_functional_conv3d_xpu_complex128 mFAILED\n\ntest_ops_fwd_gradients_xpu.py::TestFwdGradientsXPU::test_fn_fwgrad_bwgrad_nn_functional_conv3d_xpu_float64 mFAILED \ntest_ops_gradients_xpu.py::TestBwdGradientsXPU::test_fn_gradgrad_nn_functional_conv3d_xpu_complex128\ntest_ops_gradients_xpu.py::TestBwdGradientsXPU::test_fn_gradgrad_nn_functional_conv3d_xpu_float64 mFAILED\nnn/test_convolution_xpu.py::TestConvolutionNN::test_thnn_conv_strided_padded_dilated mFAILED\nThe regression seems to be fixed in onednn 3.7. I have wrote a small case with the same input in pytorch UT, and it passed with onednn3.7. However, pytorch cannot compile with pytorch 3.7.", "reporter": "daisyden", "assignee": "yuchengliu1", "resolution": "\nThe regression seems to be fixed in onednn 3.7.", "root_cause": "oneDNN dependence.", "state": "closed"}


### Merged Result:803{"issue_number": 803, "issue_description": "For LayoutLMForSequenceClassification model on stock pytorch, div cost time on pvc-1100 worse than A100 * ratio.\nxpu performance is not targeted to PT 2.6", "reporter": "xiaowangintel", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "PT 2.6 is not targeted for xpu performance", "state": "open"}


### Merged Result:800{"issue_number": 800, "issue_description": "For LayoutLMForSequenceClassification model on stock pytorch, gelu cost time on pvc-1100 worse than A100 * ratio.\nxpu performance is not targeted to PT 2.6", "reporter": "xiaowangintel", "assignee": "retonym", "resolution": "\nClose as the problem no longer exists", "root_cause": "rerun this test and the perf for gelu is reasonable", "state": "closed"}


### Merged Result:795{"issue_number": 795, "issue_description": "For T5Small model on stock pytorch, inplace add cost time on pvc-1100 worse than A100 * ratio.\nxpu performance is not targeted to PT 2.6", "reporter": "xiaowangintel", "assignee": "", "resolution": "\n", "root_cause": "xpu performance is not targeted to PT 2.6", "state": "open"}


### Merged Result:794{"issue_number": 794, "issue_description": "For Hugging Face benchmark model on stock pytorch, Softmax cost time on pvc-1100 worse than A100 * ratio.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/794. The reporter of the issue is xiaowangintel, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "xiaowangintel", "assignee": "jianyizh", "resolution": "\n", "root_cause": "The issue was closed as the reporter xiaowangintel did not provide a resolution.", "state": "closed"}


### Merged Result:789{"issue_number": 789, "issue_description": "For AllenaiLongformerBase model on stock pytorch, copy cost time on pvc-1100 worse than A100 * ratio.\nxpu performance is not targeted to PT 2.6", "reporter": "xiaowangintel", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "xpu performance is not targeted to PT 2.6", "state": "open"}


### Merged Result:788{"issue_number": 788, "issue_description": "We have witnessed the following E2E models have pagefault because of the oneDNN version v3.4.2:\n\nHuggingface:\n   1. AllenaiLongformerBase\n       a. amp_bf16 & amp_fp16, inference, performance\n\nTorchbench:\n   1. cm3leon_generate\n       a. amp_bf16 & amp_fp16, inference, accuracy & performance\n       b. bfloat16 & float16, inference, accuracy\n   2. hf_Longformer\n       a. amp_bf16 & amp_fp16, inference, accuracy & performance\n\nBy upgrading oneDNN version to v3.5.3, the problem will be solved.\n\nTested oneDNN commit:\n\nv3.4.2: 1137e04ec0b5251ca2b4400a4fd3c667ce843d67\nv3.5.3: 66f0cb9eb66affd2da3bf5f8d897376f04aae6af", "reporter": "Stonepia", "assignee": "", "resolution": "By upgrading oneDNN version to v3.5.3, the problem will be solved.", "root_cause": "The problem is caused by the oneDNN version v3.4.2, upgrading to v3.5.3 solves the problem.", "state": "closed"}


### Merged Result:784{"issue_number": 784, "issue_description": "test_foreach.py::TestForeachCUDA::test_0dim_tensor_overload_exception_cuda is expected to report \"RuntimeError: scalar tensor expected to be on cuda:0 but is on cpu\" while xpu does not have such error message. Please note cuda only report the error when alpha is specified.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/784. The reporter of the issue is daisyden, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nFixed by this pr https://github.com/intel/torch-xpu-ops/pull/1065", "root_cause": "", "state": "closed"}


### Merged Result:783{"issue_number": 783, "issue_description": "PYTORCH_TEST_WITH_SLOW=1 pytest -v test_indexing_xpu.py -k test_advancedindex_xpu_float64 the reference size is 10, so the out of boundary alert should be reported. While on xpu it passed.  In the 2nd time when we call reference[torch.LongTensor([err_idx]).to(device)] there is a core dump. Assertion `index >= -sizes_[i] && index < sizes_[i] && \nNot an issue.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "\nNot an issue.", "root_cause": "The issue is caused by the out of boundary access in the second call of reference[torch.LongTensor([err_idx]).to(device)] on xpu. The assertion failed because the index is out of the expected range.", "state": "closed"}


### Merged Result:781{"issue_number": 781, "issue_description": "Seems both cpu and xpu result are questionable. the output's imag is expected to be \"-1.0020e+23\" but got 1.0020e+23 on cpu, while the xpu result imag is -inf.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/781. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "This should be related to the implementation of the compiler. We push to 2.8", "state": "open"}


### Merged Result:780{"issue_number": 780, "issue_description": "PYTORCH_TEST_WITH_SLOW=1 pytest -v test_ops_xpu.py -k test_compare_cpu_native_layer_norm_xpu_bfloat16 \nReport \"AssertionError: The values for attribute 'dtype' do not match: torch.float32 != torch.bfloat16\".\nFrom the log we can see the 2nd tensor returned does not have dtype specified that means it is torch.float32, while cpu returned torch.bfloat16.\n\n> /home/gta/daisyden/pytorch4/test/test_ops.py(296)test_compare_cpu()\n-> cuda_results = op(sample.input, *sample.args, **sample.kwargs)\n(Pdb) p sample.input\ntensor([[[-4.5000, -5.9062, -0.9531],\n         [-2.3594,  1.5625, -6.2812]]], device='xpu:0', dtype=torch.bfloat16)\n(Pdb) p sample.args\n((1, 2, 3), tensor([[[-6.7188,  5.6875,  0.7930],\n         [ 2.8750, -4.0938,  8.5000]]], device='xpu:0', dtype=torch.bfloat16), tensor([[[ 2.1719, -8.5000, -3.1406],\n         [-7.0000,  3.2656, -2.5000]]], device='xpu:0', dtype=torch.bfloat16), 0.5)\n(Pdb) n\n> /home/gta/daisyden/pytorch4/test/test_ops.py(297)test_compare_cpu()\n-> cpu_results = op(cpu_sample.input, *cpu_sample.args, **cpu_sample.kwargs)\n(Pdb) l\n292             for sample in samples:\n293                 import pdb\n294                 pdb.set_trace()\n295                 cpu_sample = sample.transform(to_cpu)\n296                 cuda_results = op(sample.input, *sample.args, **sample.kwargs)\n297  ->             cpu_results = op(cpu_sample.input, *cpu_sample.args, **cpu_sample.kwargs)\n298  \n299                 # output_process_fn_grad has a very unfortunate name\n300                 # We use this function in linalg extensively to postprocess the inputs of functions\n301                 # that are not completely well-defined. Think svd and muliplying the singular vectors by -1.\n302                 # CPU and CUDA implementations of the SVD can return valid SVDs that are different.\n(Pdb) p cuda_results\ntensor([[[  5.5000, -14.0625,  -2.5625],\n         [ -6.2812,  -3.3125, -11.9375]]], device='xpu:0',\n       dtype=torch.bfloat16), tensor([[[-3.0729]]], device='xpu:0'), tensor([[[0.3469]]], device='xpu:0'))\n(Pdb) n\n> /home/gta/daisyden/pytorch4/test/test_ops.py(304)test_compare_cpu()\n-> cuda_results = sample.output_process_fn_grad(cuda_results)\n(Pdb) p cpu_results\ntensor([[[  5.5000, -14.0625,  -2.5625],\n         [ -6.2812,  -3.3125, -11.9375]]], dtype=torch.bfloat16), tensor([[[-3.0781]]], dtype=torch.bfloat16), tensor([[[0.3477]]], dtype=torch.bfloat16))\ncuda has the same issue, we can skip the case.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "\nskip the case", "root_cause": "cuda issue", "state": "closed"}


### Merged Result:776{"issue_number": 776, "issue_description": "convert float.min to int8 or int16, the output is different with numpy and cuda. Log: (Pdb) p vals (-3.4028234663852886e+38, -2, -1.5, -0.5, 0, 0.5, 1.5, 2) (Pdb) p dtype torch.int8 (Pdb) p device 'xpu:0' (Pdb) p np.array(vals, dtype=np.float32).astype(torch_to_numpy_dtype_dict[dtype]) array([ 0, -2, -1,  0,  0,  0,  1,  2], dtype=int8) (Pdb) n (Pdb) p vals (-3.4028234663852886e+38, -2, -1.5, -0.5, 0, 0.5, 1.5, 2) (Pdb) p dtype torch.int8 (Pdb) p device 'xpu:0' (Pdb) p torch.tensor(vals, device=device, dtype=torch.float).to(dtype) tensor([-128,   -2,   -1,    0,    0,    0,    1,    2], device='xpu:0', dtype=torch.int8) cases: test_tensor_creation_ops_xpu.py::TestTensorCreationXPU::test_float_to_int_conversion_finite_xpu_int8 test_tensor_creation_ops_xpu.py::TestTensorCreationXPU::test_float_to_int_conversion_finite_xpu_int16\nduplicate to https://github.com/intel/torch-xpu-ops/issues/889.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:774{"issue_number": 774, "issue_description": "Cases of XPU supported OP\n - [x] adaptive_max_pool2d: Expected out tensor to have dtype c10::BFloat16/c10::Half/float/double, but got long int instead\n\nThe cases related to `_foreach_norm` have been fixed in main branch. The cases related to `adaptive_max_pool` also passed in main branch. Cases related to `embedding bag` is fixed with this https://github.com/intel/torch-xpu-ops/pull/1018. The pooling related cases are all passed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/774. The reporter of the issue is yuchengliu1, and the assignee is daisyden, and the state of the issue is open.", "reporter": "yuchengliu1", "assignee": "daisyden", "resolution": "\nThe pooling related cases are all passed.\n", "root_cause": "The issue is related to oneDNN's support for certain datatypes and operations, and the reporter and assignee are discussing the need to skip these cases until oneDNN is updated to support them.", "state": "open"}


### Merged Result:772{"issue_number": 772, "issue_description": "Need quantization support, NotImplementedError: Could not run 'aten::_empty_affine_quantized' with arguments from the 'QuantizedXPU' backend.\ncases:\ntest_view_ops_xpu.py::TestOldViewOpsXPU::test_flatten_xpu\ntest_view_ops_xpu.py::TestOldViewOpsXPU::test_ravel_xpu\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/772. The reporter of the issue is PenghuiCheng, and the assignee is fengyuan14, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "fengyuan14", "resolution": "\nLowering the priority of the issue", "root_cause": "The Tensor with QuantizedXPU dispatch key implies quantization information, but in other quantization solutions, the scale and shift are not represented in a Tensor, and the scale and shift are put in a separate Tensor. The operator API or graph will introduce the scale and shift Tensors.", "state": "open"}


### Merged Result:771{"issue_number": 771, "issue_description": "1. RuntimeError: \"avg_pool3d_out_frame\" not implemented for 'BFloat16'\ncases:\n nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_pooling_bfloat16_xpu\n nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_pool_large_size_xpu_bfloat16\n\n2. RuntimeError: \"adaptive_max_pool3d_cpu\" not implemented for 'Half'\ncases:\n nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_AdaptiveMaxPool3d_indices_xpu_float16\n nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_max_pool_nan_inf_xpu_float16\n nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_adaptive_pooling_empty_output_size_xpu_float16\n nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_maxpool_indices_no_batch_dim_xpu_float16\n nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_pool_large_size_xpu_float16\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/771. The reporter of the issue is PenghuiCheng, and the assignee is chunhuanMeng, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "chunhuanMeng", "resolution": "\nCases above have been passed in the main branch ,suggest to close this issue.", "root_cause": "", "state": "closed"}


### Merged Result:768{"issue_number": 768, "issue_description": "Refs op will use the original op dtypes, we can also align the dtypesIfXPU of refs ops with cuda to avoid issues like the below. The two cases are skipped by cuda but not by torch-xpu-ops.\n- [ ] # NameError: name 'nanj' is not defined. Did you mean: 'nan'? \n\n\nThese two cases does not run in current vision", "reporter": "daisyden", "assignee": "yuchengliu1", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:767{"issue_number": 767, "issue_description": "AssertionError in nn/test_packed_sequence_xpu.py\nduplicated https://github.com/intel/torch-xpu-ops/issues/745", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "closed"}






### Merged Result:753{"issue_number": 753, "issue_description": "models\n- [ ] LayoutLMForSequenceClassification amp_fp16\n- [ ] DebertaForQuestionAnswering float16\n- [ ] DebertaV2ForQuestionAnswering float16\n\nxpu  eval  LayoutLMForSequenceClassification \nE0811 04:36:32.669000 134825 torch/_dynamo/utils.py:1555] RMSE (res-fp64): 0.00373, (ref-fp64): 0.00084 and shape=torch.Size([1, 2]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000\nE0811 04:36:32.669000 134825 torch/_dynamo/utils.py:1447] Accuracy failed for key name logits\nfail_accuracy\nxpu  eval  DebertaForQuestionAnswering \nE0811 03:59:14.697000 113914 torch/_dynamo/utils.py:1555] RMSE (res-fp64): 0.00889, (ref-fp64): 0.00107 and shape=torch.Size([]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000\nfail_accuracy\nxpu  eval  DebertaV2ForQuestionAnswering \nE0811 04:00:28.630000 114111 torch/_dynamo/utils.py:1555] RMSE (res-fp64): 0.00952, (ref-fp64): 0.00170 and shape=torch.Size([]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000\nfail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/753. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:752{"issue_number": 752, "issue_description": "Observed E2E performance on MTL, amp will be out of memory and machine will be disconnected.\nOOM and machine disconnect be related to the Driver bug.", "reporter": "mengfei25", "assignee": "Stonepia", "resolution": "\nClose the issue", "root_cause": "Driver bug", "state": "closed"}


### Merged Result:750{"issue_number": 750, "issue_description": "Error message like 'L0 build module failed. Log: error: bf conversion instruction not supported! in kernel: 'triton_poi_fused__to_copy_2' error: backend compiler failed build.' Traceback (most recent call last): File \"/home/gta/actions-runner/actions-runner/_work/torch-xpu-ops/repro.py\", line 34, in <module> async_compile.wait(globals()) File \"/opt/conda/envs/e2e_ci/lib/python3.10/site-packages/torch/_inductor/async_compile.py\", line 261, in wait scope[key] = result.result() File \"/opt/conda/envs/e2e_ci/lib/python3.10/site-packages/torch/_inductor/codecache.py\", line 3745, in result self.kernel.precompile() File \"/opt/conda/envs/e2e_ci/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 234, in precompile compiled_binary, launcher = self._precompile_config( File \"/opt/conda/envs/e2e_ci/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 442, in _precompile_config self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary( RuntimeError: Triton Error [ZE]: 0x70000004\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/750. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nThe issue is verified with latest triton release/2.5.0 branch, and it should work with PyTorch with commit id later than https://github.com/pytorch/pytorch/commit/fbd020fce649ddb44bd9a578dabb5834c5d0f186", "root_cause": "The error is caused by the unsupported bf conversion instruction in the Triton backend. The reporter reported this issue in the Intel XPU backend for Triton repository and the issue is closed.", "state": "closed"}


### Merged Result:746{"issue_number": 746, "issue_description": "new ut failures introduced by new pytorch\n- PYTORCH_TEST_WITH_SLOW=1 python test/test_modules.py TestModuleXPU.test_non_contiguous_tensors_nn_Conv3d_xpu_float32 - need to adjust tolerence\n- PYTORCH_TEST_WITH_SLOW=1 python test/test_dynamic_shapes.py TestSymNumberMagicMethods.test_method_fn_add_first_type_int_second_type_float\n- PYTORCH_TEST_WITH_SLOW=1 python test/test_dynamic_shapes.py TestSymNumberMagicMethods.test_method_fn_mul_first_type_int_second_type_float\n- PYTORCH_TEST_WITH_SLOW=1 python test/test_dynamic_shapes.py TestSymNumberMagicMethods.test_method_fn_sub_first_type_int_second_type_float\nnew ut failures introduced by new pytorch\n4 cases are passed, the 1st one need to adjust tolerence.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\nclosed\nverified", "root_cause": "unknown", "state": "closed"}


### Merged Result:745{"issue_number": 745, "issue_description": "PI_ERROR_INVALID_QUEUE after copying device 0 tensor to device 1\nCannot launch kernel successfully on PVC Tile 1 after querying `info::kernel_device_specific::work_group_size`. Got runtime error.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nThe issue is common for all platform where there are devices more than one. The most important and most common case for us is client case, a client platform/desktop has an iGPU and an dGPU.", "root_cause": "Cannot launch kernel successfully on PVC Tile 1 after querying `info::kernel_device_specific::work_group_size`. Got runtime error.", "state": "closed"}


### Merged Result:737{"issue_number": 737, "issue_description": "XPU data types supported at native are more than CUDA. Like we always support BF16, but CUDA in some operators doesn't support BF16. So we have got a handle in test infrastructure to add BF16 to our claimed data types, backward_dtypesIfXPU = backward_dtypesIfCUDA + bfloat16. But in fft operators, we should claim not supporting BF16. The existing assumption in test infrastructure leads to some fft unit test failures, test_dtypes_fft_fft2_xpu, test_dtypes_fft_fft_xpu, test_dtypes_fft_fftn_xpu, test_dtypes_fft_hfft2_xpu, test_dtypes_fft_hfft_xpu, test_dtypes_fft_hfftn_xpu, test_dtypes_fft_ifft2_xpu, test_dtypes_fft_ifft_xpu, test_dtypes_fft_ifftn_xpu, test_dtypes_fft_ihfft2_xpu, test_dtypes_fft_ihfft_xpu, test_dtypes_fft_ihfftn_xpu, test_dtypes_fft_irfft2_xpu, test_dtypes_fft_irfft_xpu, test_dtypes_fft_irfftn_xpu, test_dtypes_fft_rfft2_xpu, test_dtypes_fft_rfft_xpu, test_dtypes_fft_rfftn_xpu\n", "reporter": "fengyuan14", "assignee": "PenghuiCheng", "resolution": "\ncompleted", "root_cause": "", "state": "closed"}


### Merged Result:731{"issue_number": 731, "issue_description": "TestAutograd::test_profiler UT failure after enable PTI. The test fails with RuntimeError: Fail to enable Kineto Profiler on XPU due to error code: 200. To execute this test, run the following from the base repo dir: PYTORCH_TEST_WITH_SLOW=1 python test/test_autograd.py TestAutograd.test_profiler. This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/731. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "closed"}




### Merged Result:728{"issue_number": 728, "issue_description": "torchbench_amp_fp16_inference\n- [ ] `detectron2_fasterrcnn_r_101_c4`\n- [ ] `detectron2_fasterrcnn_r_101_dc5`\n- [ ] `detectron2_fasterrcnn_r_101_fpn`\n- [ ] `detectron2_fasterrcnn_r_50_c4`\n- [ ] `detectron2_fasterrcnn_r_50_dc5`\n- [ ] `detectron2_fasterrcnn_r_50_fpn`\n- [ ] `detectron2_maskrcnn_r_50_fpn`\n- [ ] `detectron2_maskrcnn_r_101_c4`\n- [ ] `detectron2_maskrcnn_r_101_fpn`\n- [ ] `detectron2_maskrcnn_r_50_fpn`\nWARNING:common:fp64 golden ref were not generated for detectron2_fasterrcnn_r_101_c4. Setting accuracy check to cosine\nWARNING:common:current_device=xpu; error:dets should have the same type as scores\nW0803 06:43:58.821000 3103334 torch/_dynamo/utils.py:1499] Similarity score=0.8771064281463623\nE0803 06:43:58.822000 3103334 torch/_dynamo/utils.py:1450] Accuracy failed for key name pred_classes\nE0803 06:43:58.823000 3103334 torch/_dynamo/utils.py:1450] Accuracy failed for key name instances\nfail_accuracy\nloading model: 0it [00:00, ?it/s]\nloading model: 0it [00:58, ?it/s]\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/728. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "The models are not included in the Meta PyTorch dashboard due to low priority and the failure of detectron2 installation on A100. These models are not included in the Meta dashboard and are not targeted for PT2.6.", "state": "closed"}


### Merged Result:727{"issue_number": 727, "issue_description": "torchbench_amp_bf16_training xpu train tacotron2\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/727. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nclose due to a100 also failed", "root_cause": "torch._dynamo.config.capture_scalar_outputs = True or env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1", "state": "closed"}


### Merged Result:726{"issue_number": 726, "issue_description": "torchbench_amp_bf16_training xpu train hf_distil_whisper Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run  ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 302, in load_model  benchmark = benchmark_cls(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__  obj = type.__call__(cls, *args, **kwargs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/hf_distil_whisper/__init__.py\", line 12, in __init__  raise NotImplementedError(\"Training is not implemented.\") NotImplementedError: Training is not implemented. model_fail_to_load\n\nloading model: 0it [00:00, ?it/s] \nloading model: 0it [00:00, ?it/s]\"} {\nNot an xpu issue", "reporter": "mengfei25", "assignee": "", "resolution": "\nNot an xpu issue", "root_cause": "", "state": "closed"}


### Merged Result:725{"issue_number": 725, "issue_description": "torchbench_amp_bf16_training xpu train detectron2_fcos_r_50_fpn Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run  ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 302, in load_model  benchmark = benchmark_cls(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__  obj = type.__call__(cls, *args, **kwargs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/detectron2_fcos_r_50_fpn/__init__.py\", line 15, in __init__  super().__init__(variant=\"COCO-Detection/fcos_R_50_FPN_1x.py\", test=test, device=device,  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/framework/detectron2/model_factory.py\", line 137, in __init__  raise NotImplementedError(  NotImplementedError: FCOS train is not supported by upstream detectron2. See GH Issue: https://github.com/facebookresearch/detectron2/issues/4369. model_fail_to_load loading model: 0it [00:13, ?it/s] warning: Aten Op fallback from XPU to CPU happends. This may have performance implications. If need debug the fallback ops please set environment variable `PYTORCH_DEBUG_XPU_FALLBACK=1`  (function operator())\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/725. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "FCOS train is not supported by upstream detectron2. See GH Issue: https://github.com/facebookresearch/detectron2/issues/4369.", "state": "open"}




### Merged Result:723{"issue_number": 723, "issue_description": "torchbench_amp_bf16_training xpu train pyhpc_turbulent_kinetic_energy Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run  ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 302, in load_model  benchmark = benchmark_cls(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__  obj = type.__call__(cls, *args, **kwargs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 137, in __init__  self._determine_batch_size(batch_size)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 262, in _determine_batch_size  raise NotImplementedError(  NotImplementedError: Model's DEFAULT_TRAIN_BSIZE is not implemented. model_fail_to_load\n\nloading model: 0it [00:00, ?it/s] \nloading model: 0it [00:07, ?it/s] \n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/723. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nclose due to a100 failed", "root_cause": "Model's DEFAULT_TRAIN_BSIZE is not implemented.", "state": "closed"}


### Merged Result:722{"issue_number": 722, "issue_description": "torchbench_amp_bf16_training\n- [ ] `pyhpc_equation_of_state`\n- [ ] `pyhpc_isoneutral_mixing`\n- [ ] `maml`\n- [ ] `maml_omniglot`\n- [ ] `cm3leon_generate`\n- [ ] `hf_T5_generate`\n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2512, in validate_model\n    self.model_iter_fn(model, example_inputs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 450, in forward_and_backward_pass\n    self.grad_scaler.scale(loss).backward()\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_tensor.py\", line 522, in backward\n    torch.autograd.backward(\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 346, in backward\n    _engine_run_backward(\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/autograd/graph.py\", line 812, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run\n    ) = runner.load_model(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 362, in load_model\n    self.validate_model(model, example_inputs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2514, in validate_model\n    raise RuntimeError(\"Eager run failed\") from e\nRuntimeError: Eager run failed\n\neager_fail_to_run\n\nloading model: 0it [00:00, ?it/s]\nloading model: 0it [00:01, ?it/s]\"} {\nA100 has same issue", "reporter": "mengfei25", "assignee": "", "resolution": "\nclosed", "root_cause": "element 0 of tensors does not require grad and does not have a grad_fn", "state": "closed"}


### Merged Result:721{"issue_number": 721, "issue_description": "torchbench_amp_bf16_training xpu train doctr_reco_predictor Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2512, in validate_model self.model_iter_fn(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 449, in forward_and_backward_pass loss = self.compute_loss(pred)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 432, in compute_loss return reduce_to_scalar_loss(pred)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in <genexpr> return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in <genexpr> return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in <genexpr> return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 124, in reduce_to_scalar_loss raise NotImplementedError(\nA100 is also failed", "reporter": "mengfei25", "assignee": "", "resolution": "\nclose due to a100 failed", "root_cause": "", "state": "closed"}


### Merged Result:720{"issue_number": 720, "issue_description": "torchbench_amp_bf16_training xpu train doctr_det_predictor Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2512, in validate_model self.model_iter_fn(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 449, in forward_and_backward_pass loss = self.compute_loss(pred)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 432, in compute_loss return reduce_to_scalar_loss(pred)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in <genexpr> return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in <genexpr> return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in <genexpr> return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 124, in reduce_to_scalar_loss raise NotImplementedError(\nA100 is also failed", "reporter": "mengfei25", "assignee": "", "resolution": "\nclose due to a100 failed", "root_cause": "", "state": "closed"}




### Merged Result:718{"issue_number": 718, "issue_description": "torchbench_amp_bf16_training xpu train opacus_cifar10 Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2512, in validate_model self.model_iter_fn(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 450, in forward_and_backward_pass self.grad_scaler.scale(loss).backward()  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_tensor.py\", line 522, in backward torch.autograd.backward(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 346, in backward torch.autograd.backward(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/autograd/graph.py\", line 812, in _engine_run_backward return Variable._execution_engine.run_backward(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 98, in __call__ return self.hook(module, *args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/opacus/grad_sample/grad_sample_module.py\", line 328, in capture_backprops_hook activations, backprops = self.rearrange_grad_samples(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/opacus/grad_sample/grad_sample_module.py\", line 384, in rearrange_grad_samples raise ValueError  ValueError: No activations detected for <class 'torch.nn.modules.linear.Linear'>, run forward after add_hooks(model) The above exception was the direct cause of the following exception: Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 362, in load_model self.validate_model(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2514, in validate_model raise RuntimeError(\"Eager run failed\") from e  RuntimeError: Eager run failed eager_fail_to_run loading model: 0it [00:00, ?it/s] loading model: 0it [00:06, ?it/s] \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/718. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nclose the issue, due to a100 also failed.", "root_cause": "No activations detected for <class 'torch.nn.modules.linear.Linear'>, run forward after add_hooks(model)", "state": "closed"}




### Merged Result:716{"issue_number": 716, "issue_description": "torchbench_amp_bf16_inference xpu eval hf_clip Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2512, in validate_model self.model_iter_fn(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 439, in forward_pass return mod(*inputs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl return self._call_impl(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl return forward_call(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 1110, in forward vision_outputs = self.vision_model(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl return self._call_impl(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl return forward_call(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 849, in forward hidden_states = self.embeddings(pixel_values)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl return self._call_impl(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl return forward_call(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 188, in forward batch_size = pixel_values.shape[0] AttributeError: 'str' object has no attribute 'shape' The above exception was the direct cause of the following exception: Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 362, in load_model self.validate_model(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2514, in validate_model raise RuntimeError(\nA100 also has this issue", "reporter": "mengfei25", "assignee": "", "resolution": "\nclose the issue", "root_cause": "The error is caused by the fact that the input to the model is a string instead of a tensor. The model expects a tensor as input, but a string is passed instead. This can happen if the input data is not properly preprocessed or if there is a bug in the data loading process. To fix this, the input data should be converted to a tensor before passing it to the model. ", "state": "closed"}


### Merged Result:715{"issue_number": 715, "issue_description": "torchbench_amp_bf16_inference\n- [ ] `moco`\n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run\n    ) = runner.load_model(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 309, in load_model\n    benchmark = benchmark_cls(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__\n    obj = type.__call__(cls, *args, **kwargs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/moco/__init__.py\", line 80, in __init__\n    raise NotImplementedError(f\"{device} not supported\")\nNotImplementedError: xpu not supported\n\nmodel_fail_to_load\n\nloading model: 0it [00:00, ?it/s]\nYou are using a model of type moondream1 to instantiate a model of type phi. This is not supported for all configurations of models and can yield errors.\n\nloading model: 0it [00:17, ?it/s]\n\nA100 pass", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\nduplicated as https://github.com/intel/torch-xpu-ops/issues/489", "root_cause": "The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is closed.", "state": "closed"}






### Merged Result:712{"issue_number": 712, "issue_description": "The original model code forces the use of CUDA. model_fail_to_load\nsimple_gpt on A100 are failed", "reporter": "mengfei25", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:711{"issue_number": 711, "issue_description": "The eval test only supports CPU. The model fails to load.\nThe model still fails with a new error message after the previous error was resolved. The new error message is: expected scalar type Float but found Half. The previous error message was: expected scalar type Float but found Half. The root cause of the new error is that the model is using a half precision tensor in a place where a float tensor is expected.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nThe model needs to be updated to use float tensors instead of half precision tensors in the relevant places.", "root_cause": "The model is using a half precision tensor in a place where a float tensor is expected.", "state": "open"}


### Merged Result:710{"issue_number": 710, "issue_description": "For now ,when `ord == inf` ,it fallback to cpu, we should have the implementation when `ord == inf` to same as cuda.\nThe issue is about the performance of the torch_xpu_ops module, specifically the problem of the module not being able to run on the XPU device. The reporter of the issue is chunhuanMeng, and the assignee is chunhuanMeng, and the state of the issue is closed.", "reporter": "chunhuanMeng", "assignee": "chunhuanMeng", "resolution": "closed\nThe PR was merged", "root_cause": "The issue was caused by a bug in the torch_xpu_ops module, which was fixed by the PR merged by xytintel.", "state": "closed"}


### Merged Result:708{"issue_number": 708, "issue_description": "Model list:\n- [ ] `convnext_base`\n\nE0804 00:49:00.458000 519441 torch/_dynamo/utils.py:1558] RMSE (res-fp64): nan, (ref-fp64): 0.00512 and shape=torch.Size([128]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.010000\nE0804 00:49:00.458000 519441 torch/_dynamo/utils.py:1450] Accuracy failed for key name stem.0.bias.grad\nfail_accuracy\nlow priority for fp16 training not included in Meta PyTorch dashboard", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nClose the issues, since A100 accuracy also fails", "root_cause": "A100 is also failed", "state": "closed"}


### Merged Result:707{"issue_number": 707, "issue_description": "Model list: - [ ] `fbnetv3_b` E0804 06:29:58.153000 934284 torch/_dynamo/utils.py:1558] RMSE (res-fp64): 0.30015, (ref-fp64): 0.05598 and shape=torch.Size([360]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.040000 E0804 06:29:58.154000 934284 torch/_dynamo/utils.py:1450] Accuracy failed for key name blocks.4.3.bn1.running_var fail_accuracy\nlow priority for ampbf16 training not included in Meta PyTorch dashboard", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nclose the issue, due to A100 also fails", "root_cause": "A100 is also failed", "state": "closed"}




### Merged Result:705{"issue_number": 705, "issue_description": "Model list: - [ ] `resnest101e` E0804 04:47:13.347000 901510 torch/_dynamo/utils.py:1558] RMSE (res-fp64): nan, (ref-fp64): 0.00000 and shape=torch.Size([128]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.010000 E0804 04:47:13.348000 901510 torch/_dynamo/utils.py:1450] Accuracy failed for key name bn1.bias.grad fail_accuracy\nlow priority for ampbf16 training not included in Meta PyTorch dashboard", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\ncheck the latest weekly report, this model pass now.", "root_cause": "", "state": "closed"}


### Merged Result:704{"issue_number": 704, "issue_description": "WARNING:common:fp64 golden ref were not generated for GPTNeoForSequenceClassification. Setting accuracy check to cosine\nWARNING:common:current_device=xpu; error:value cannot be converted to type float without overflow\nE0802 16:58:50.605000 3518147 torch/_dynamo/utils.py:1450] Accuracy failed for key name logits\nfail_accuracy\nThose 2 models belongs to skip list before, commit https://github.com/pytorch/pytorch/commit/8458980bbf78714a0fbe703785c100cad523fade change the skip logic. We'll submit PR to skip them again, but we need to double check whether we have potential real issue in here.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "value cannot be converted to type float without overflow", "state": "closed"}


### Merged Result:703{"issue_number": 703, "issue_description": "WARNING:common:fp64 golden ref were not generated for GPTNeoForCausalLM. Setting accuracy check to cosine\nWARNING:common:current_device=xpu; error:value cannot be converted to type float without overflow\nE0802 18:11:51.919000 3841258 torch/_dynamo/utils.py:1450] Accuracy failed for key name transformer.h.0.attn.attention.k_proj.weight.grad\nfail_accuracy\nThose 2 models belongs to skip list before, commit https://github.com/pytorch/pytorch/commit/8458980bbf78714a0fbe703785c100cad523fade change the skip logic. We'll submit PR to skip them again, but we need to double check whether we have potential real issue in here.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "value cannot be converted to type float without overflow", "state": "closed"}






### Merged Result:698{"issue_number": 698, "issue_description": "Reduction: Computation error in work group reduction on SLM when SIMD=8\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/698. The reporter of the issue is fengyuan14, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "\nClose issue as already been fixed.", "root_cause": "possibly an issue of compiler software stack, SYCL compiler or IGC. The issue is filed for tracking. Will retrieve original logic if the issue is fixed.", "state": "closed"}


### Merged Result:686{"issue_number": 686, "issue_description": "test_ops_xpu.py::TestCommonXPU::test_dtypes_nanmean_xpu - Exception: Caused by sample input at index 23: SampleInput(input=Tensor[size=(3, 2, 1, 2), device=\"xpu:0\", dtype=torch.bool], args=(), kwargs={'dim': '(1,3)', 'keepdim': 'False'}, broadcasts_input=False, name='') - passed in regular test with LTS driver 803.61, - This issue is because the torch-xpu-ops [de744d9](https://github.com/intel/torch-xpu-ops/commit/de744d9a92d5294041127235d1630466128eff1f) used in the test do not enable nanmean, so the backward_dtypes didn't align with cuda in infrstructure. The latest version has support it.\nUT failures with rolling build and LTS launch\nUT failures with rolling build and LTS launch\nUT failures with rolling build and LTS launch\nThe torch-xpu-ops version is out of date, a lot of cases are skipped in latest test suites.", "reporter": "mengfei25", "assignee": "majing921201", "resolution": "The latest version has support it.\n\n\n\nThe torch-xpu-ops version is out of date", "root_cause": "The torch-xpu-ops [de744d9](https://github.com/intel/torch-xpu-ops/commit/de744d9a92d5294041127235d1630466128eff1f) used in the test do not enable nanmean, so the backward_dtypes didn't align with cuda in infrstructure.", "state": "closed"}


### Merged Result:685{"issue_number": 685, "issue_description": "Reduction: Enhance reduction kernel with supporting data type dynamic cast, The existing CUDA implementation in PyTorch supports data type dynamic cast, so that there won't be an extra kernel to align data types of input and output.\nNot an urgent case, as the usage is rare. Lower the priority.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:683{"issue_number": 683, "issue_description": "accuracy issue of test_neg_view_nn_functional_rrelu_xpu_float64 - PASSED on torch              2.6.0a0+git64ccebd\n\ntorch-xpu-ops 3b245e2faeda3982f3147b3216fdee021051985a\n\nRuntimeError: value cannot be converted to type float without overflow\nTestMathBitsXPU has 2 cases with RuntimeError: value cannot be converted to type float without overflow - v2.6\n\"test_conj_view_addbmm_xpu_complex64\",\n\"test_neg_conj_view_addbmm_xpu_complex128\",   - duplicated with #436\nRuntimeError: value cannot be converted to type float without overflow", "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": "\n", "root_cause": "The addbmm implementation of mkldnn has explicit cast to float like 'alpha.to<float>()', when the alpha and beta are complex, it causes the error.", "state": "closed"}


### Merged Result:676{"issue_number": 676, "issue_description": "TestMatmulCudaXPU.test_cublas_addmm_size_1000_xpu_float32 failed with AssertionError: Tensor-likes are not close! Mismatched elements: 9 / 1003002 (0.0%) Greatest absolute difference: 711.126220703125 at index (472, 999) (up to 0.1 allowed) Greatest relative difference: 2.7107455730438232 at index (472, 997) (up to 0.1 allowed)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/676. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\npassed with latest code", "root_cause": "", "state": "closed"}


### Merged Result:674{"issue_number": 674, "issue_description": "Affected total of 21 test: test_fn_fwgrad_bwgrad_nn_functional_pairwise_distance_xpu_float64, test_backward_sgn_xpu_float32, test_forward_ad_sgn_xpu_float32, test_noncontiguous_samples_sgn_xpu_float32, test_variant_consistency_eager_sgn_xpu_float32, test_neg_view_sgn_xpu_float64, test_fn_fwgrad_bwgrad_sgn_xpu_float64, test_forward_mode_AD_sgn_xpu_float64, test_inplace_forward_mode_AD_sgn_xpu_float64, test_forward_mode_AD_sub_xpu_complex128, test_forward_mode_AD_sub_xpu_float64, test_inplace_forward_mode_AD_sub_xpu_complex128, test_inplace_forward_mode_AD_sub_xpu_float64, test_fn_fwgrad_bwgrad_abs_xpu_float64, test_fn_fwgrad_bwgrad_nn_functional_l1_loss_xpu_float64, test_forward_mode_AD_rsub_xpu_complex128, test_forward_mode_AD_rsub_xpu_float64, test_fn_fwgrad_bwgrad_nn_functional_smooth_l1_loss_xpu_float64, test_fn_fwgrad_bwgrad_nn_functional_softsign_xpu_float64, test_fn_fwgrad_bwgrad_special_i0e_xpu_float64, test_fn_fwgrad_bwgrad_special_i1e_xpu_float64\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/674. The reporter of the issue is Stonepia, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "fengyuan14", "resolution": "\nFixing: https://github.com/intel/torch-xpu-ops/pull/702, https://github.com/intel/torch-xpu-ops/pull/689", "root_cause": "", "state": "closed"}


### Merged Result:673{"issue_number": 673, "issue_description": "Affected total of 18 test:\ntest_inplace_forward_mode_AD_add_xpu_complex128\ntest_inplace_forward_mode_AD_add_xpu_float64\ntest_forward_mode_AD_addcdiv_xpu_complex128\ntest_forward_mode_AD_addcdiv_xpu_float64\ntest_inplace_forward_mode_AD_addcdiv_xpu_complex128\ntest_inplace_forward_mode_AD_addcdiv_xpu_float64\ntest_forward_mode_AD_addcmul_xpu_complex128\ntest_forward_mode_AD_addcmul_xpu_float64\ntest_inplace_forward_mode_AD_addcmul_xpu_complex128\ntest_inplace_forward_mode_AD_addcmul_xpu_float64\ntest_forward_mode_AD_addr_xpu_complex128\ntest_forward_mode_AD_addr_xpu_float64\ntest_inplace_forward_mode_AD_addr_xpu_complex128\ntest_inplace_forward_mode_AD_addr_xpu_float64\ntest_forward_mode_AD_index_add_xpu_complex128\ntest_forward_mode_AD_index_add_xpu_float64\ntest_inplace_forward_mode_AD_index_add_xpu_complex128\ntest_inplace_forward_mode_AD_index_add_xpu_float64\n\nRun command:\n\n```Bash\nexport DisableScratchPages=1 \nexport NEOReadDebugKeys=1\n\nexport PYTORCH_TEST_WITH_SLOW=1 \n\npython -m pytest -v test_ops_fwd_gradients_xpu.py -k test_fn_fwgrad_bwgrad_abs_xpu_float64 \n``` \nUse the following command for more detail:\n\n```Bash\nexport SYCL_PI_TRACE=-1\n\nexport ZE_SERIALIZE=2\n\nexport OverrideImmediateCmdListSynchronousMode=1\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/673. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/702, https://github.com/intel/torch-xpu-ops/pull/689", "root_cause": "", "state": "closed"}


### Merged Result:672{"issue_number": 672, "issue_description": "Affected total of 18 test:\ntest_stable_sort_against_numpy_xpu_bfloat16\ntest_stable_sort_against_numpy_xpu_float16\ntest_stable_sort_against_numpy_xpu_float32\ntest_stable_sort_against_numpy_xpu_float64\ntest_stable_sort_against_numpy_xpu_int16\ntest_stable_sort_against_numpy_xpu_int32\ntest_stable_sort_against_numpy_xpu_int64\ntest_stable_sort_against_numpy_xpu_int8\ntest_stable_sort_against_numpy_xpu_uint8\n\nRun command:\n\n```Bash\nexport DisableScratchPages=1 \nexport NEOReadDebugKeys=1\n\nexport PYTORCH_TEST_WITH_SLOW=1 \n\npython -m pytest -v test_ops_fwd_gradients_xpu.py -k test_fn_fwgrad_bwgrad_abs_xpu_float64 \n``` \nUse the following command for more detail:\n\n```Bash\nexport SYCL_PI_TRACE=-1\n\nexport ZE_SERIALIZE=2\nexport OverrideImmediateCmdListSynchronousMode=1\n``` \n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/672. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nFixed by https://github.com/intel/torch-xpu-ops/pull/734 https://github.com/intel/torch-xpu-ops/pull/735", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/672. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "state": "closed"}




### Merged Result:667{"issue_number": 667, "issue_description": "New UT failures on PVC 1550\ntest_non_contiguous_tensors_nn_LazyConvTranspose3d_xpu_float32 is known random issue, others are driver issue", "reporter": "mengfei25", "assignee": "ZhiweiYan-96", "resolution": "\nclosed", "root_cause": "driver issue", "state": "closed"}




### Merged Result:664{"issue_number": 664, "issue_description": "log_softmax operation fails on XPU with \"Kernel is incompatible with all devices\" error on arc a770\nsimilar to https://github.com/intel/torch-xpu-ops/issues/628 and to pull request https://github.com/intel/torch-xpu-ops/pull/511 It must set:\nexport OverrideDefaultFP64Settings=1\nexport IGC_EnableDPEmulation=1\n", "reporter": "zhiyuan1i", "assignee": "daisyden", "resolution": "\naccording to @PenghuiCheng 's verification, let's close the issue.", "root_cause": "", "state": "closed"}






### Merged Result:661{"issue_number": 661, "issue_description": "Test infrastructure: Non CUDA alignment data type tested for signbit in test_unary_ufuncs. Aligned with CUDA implementation. No bool support. The test infra should algin with typesIfCUDA, but seems not.\nCuda can work on bool", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nCuda can work on bool", "root_cause": "CUDA can run native code on bool", "state": "closed"}


### Merged Result:658{"issue_number": 658, "issue_description": "TypeError: TestStateDictHooks.test_register_state_dict_post_hook() missing 1 required positional argument: 'private'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/658. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nThis case is skipped in latest pytorch.", "root_cause": "case is skiped in below version: torch-xpu-ops:fb8e6e9ef0240523c32a856a45220fc5cb55012c pytorch: e5560d10f4ee621b5952f61950761bac1d105afd", "state": "closed"}


### Merged Result:654{"issue_number": 654, "issue_description": "Proposal: Switch to safer data_ptr API\n\nExisting code uses raw pointers like this:\n\n```C++\n// non-template usage. Avoid using this\ntensor.data_ptr();\n```\n\nThe above usage doesn't check whether the `tensor` is initialized or not. When the `tensor`'s storage is not initialized (for example, FakeTensor), this `data_ptr` is null. Then the kernel may try to write to a non-initialized memory, and causing a page fault.\n\nThe template data_ptr will do additional checks. Like `has_storage()` and `storage_initialized()`. Please refer to the PyTorch [TensorImpl.h](https://github.com/pytorch/pytorch/blob/03f49c9523db2f307b749ef3fe1735f522941f78/c10/core/TensorImpl.h#L1561-L1564) for detail.\n\nIn view of that, change to a safer usage for old usage.\n\n```C++\n// template usage. Not encouraged.\ntensor.data_ptr<scalar_t>();\n```\nFor the existing code, I changed the first part in PR : https://github.com/intel/torch-xpu-ops/pull/655. The second part is not solved yet.", "reporter": "Stonepia", "assignee": "", "resolution": "\nThe first part of the issue is resolved by the PR https://github.com/intel/torch-xpu-ops/pull/655.", "root_cause": "The existing code uses raw pointers like this: `tensor.data_ptr();` which doesn't check whether the `tensor` is initialized or not. When the `tensor`'s storage is not initialized (for example, FakeTensor), this `data_ptr` is null. Then the kernel may try to write to a non-initialized memory, and causing a page fault.", "state": "closed"}


### Merged Result:653{"issue_number": 653, "issue_description": "these cases are target 2.5\n - [x] LossNLL2d has no correct assert\n    \"test_cross_entropy_loss_2d_out_of_bounds_class_index_xpu_float16\",\n    \"test_cross_entropy_loss_2d_out_of_bounds_class_index_xpu_float32\" - [Jianghang](https://github.com/intel/torch-xpu-ops/pull/665)\n - [x] native_group_norm : RuntimeError: Expected X.is_contiguous(memory_format) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)\n    \"test_GroupNorm_memory_format_xpu\", - https://github.com/intel/torch-xpu-ops/pull/677\n - [x] upsamplingNearest2d: Failed: Unexpected success\n    \"test_upsamplingNearest2d_launch_fail_xpu\", - grid size check \n - [x] do not aline with cuda #656 fix it\n\"test_upsamplingBiMode2d_consistency\",\n\"test_upsamplingBiLinear2d_consistency_interp_size_bug\",\n - [x] cause by cuda hard code\n\"test_device_mask_xpu\", #656 fix it\n\"test_overwrite_module_params_on_conversion_cpu_device_xpu\",\n(https://github.com/pytorch/pytorch/blob/1fb498d6e34e0e9b43b2c26dc0a18a4fc3a52605/aten/src/ATen/native/cuda/UpSampleNearest2d.cu#L303) is specially for cuda, keep in skip list", "reporter": "yuchengliu1", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:645{"issue_number": 645, "issue_description": "Failed test cases with FP64 emulation feature\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/645. The reporter of the issue is mengfei25, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "daisyden", "resolution": "\n", "root_cause": "The issue is related to MTL and the test cases are failing on MTL. The test cases are passing on ARC and cuda. The issue is tracked and will be fixed in the future.", "state": "closed"}


### Merged Result:644{"issue_number": 644, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/644. The reporter of the issue is yuchengliu1, and the assignee is , and the state of the issue is closed. This is the github issue title aten::_thnn_fused_gru_cell // CPU fallback could not cover it, and issue body Content of #644 is : \nSorry, open the issue by accidental touch.", "reporter": "yuchengliu1", "assignee": "", "resolution": "\n", "root_cause": "Accidental touch when opening the issue.", "state": "closed"}


### Merged Result:640{"issue_number": 640, "issue_description": "Warning: Warning only once for all operators, other operators may also be overrided. Overriding a previously registered kernel for the same operator and the same dispatch key operator: aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!) dispatch key: XPU previous kernel: registered at /home/dvrogozh/git/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30476 new kernel: registered at /home/dvrogozh/git/pytorch/pytorch/build/aten/src/ATen/xpu/RegisterXPU.cpp:7169 (function operator())\nThat's regression after 6eca3940f2a1d1bce884e0c4b929157c0fa3f88a by @yucai-intel, #557.", "reporter": "dvrogozh", "assignee": "", "resolution": "\n", "root_cause": "Should be a typo when resolving conflict.", "state": "closed"}


### Merged Result:636{"issue_number": 636, "issue_description": "aten::embedding_renorm_ requires XPU implementation. Test infrastructure requirement. @huaiyuzh - 2.6\n#splited from #233\nThis issues is used to  retrieve the fine grain cases when embedding_renorm_ added.", "reporter": "daisyden", "assignee": "huaiyuzh", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:632{"issue_number": 632, "issue_description": "Squeezenet1_1 accuracy minify\nThe issues should related to conv + relu + adaptive_avgpool in the backward pattern. However, it is really hard to reproduce in unit test. Still working on this problem.", "reporter": "retonym", "assignee": "retonym", "resolution": "\nFix PR: https://github.com/intel/torch-xpu-ops/pull/668", "root_cause": "pytorch PR: https://github.com/pytorch/pytorch/pull/84541", "state": "open"}




### Merged Result:629{"issue_number": 629, "issue_description": "The operation `log_probs = torch.nn.functional.log_softmax(out[top_p_mask] / temperature[top_p_mask].unsqueeze(1), dim=-1)` is failing with a warning that the `masked_select` operation is falling back to CPU execution. This is due to the lack of native XPU support for the `masked_select` operation, which is causing the operation to be executed on the CPU instead of the XPU.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/629. The reporter of the issue is zhiyuan1i, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "zhiyuan1i", "assignee": "xytintel", "resolution": "Implement native XPU support for the `masked_select` operation to avoid CPU fallback.\nPR ready: https://github.com/intel/torch-xpu-ops/pull/649", "root_cause": "The `masked_select` operation is not fully supported on the XPU backend and is falling back to CPU execution due to the lack of native XPU support.", "state": "closed"}


### Merged Result:628{"issue_number": 628, "issue_description": "I've encountered issue while using the Intel XPU backend with PyTorch:\n\n1. FP64 not supported on XPU device\n\nWhen trying to create a random tensor and convert it to bfloat16, I receive the following error:\n\n```python\nout = torch.randn(batch_size, vocab_size, device='xpu').to(torch.bfloat16)\n# RuntimeError: Required aspect fp64 is not supported on the device\n``` \n\nThis error suggests that the XPU device does not support fp64 operations. However, the `torch.randn()` function seems to be attempting to use fp64 internally before converting to the desired dtype.\n\nProposed solution: Consider modifying the `torch.randn()` implementation to use fp32 as an intermediate type when fp64 is not supported on the device.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/628. The reporter of the issue is zhiyuan1i, and the assignee is riverliuintel, and the state of the issue is closed.\nrandom. Even your operator is FP64 irrelevant, runtime compilation gets failure due to per-source build. Per-source build means all kernels including FP64 involved in the same source file are built when calling one of them. Driver raises building error. AOT build (building machine code statically) should help on this.", "reporter": "zhiyuan1i", "assignee": "riverliuintel", "resolution": "The issue has been resolved and the fix has been merged into the main branch of the repository.\nThe issue was closed by the assignee.\nThis feature has been implemented in PT2.5 and long time no response, close this ticket.", "root_cause": "The XPU device does not support fp64 operations, and the `torch.randn()` function internally uses fp64, leading to the error when converting to bfloat16.", "state": "closed"}


### Merged Result:626{"issue_number": 626, "issue_description": "With both key and value data type being 64 bits, there will be occasional computation issues on MTL machines.\nneed to investigate the sort kernel refinement in 2.6.", "reporter": "xytintel", "assignee": "xytintel", "resolution": "\n", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/626. The reporter of the issue is xytintel, and the assignee is xytintel, and the state of the issue is closed.", "state": "closed"}


### Merged Result:623{"issue_number": 623, "issue_description": "Failure case: test_nextafter_bfloat16_xpu_bfloat16. We aligned CPU and CUDA implementation by using std::nextafter. But got failure, Assertion error: Scalars are not equal! Expected 9.183549615799121e-41 but got 0.0. Absolute difference: 9.183549615799121e-41 Relative difference: 1.0\ncompiler dependency, move to 2.8.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\ncompiler dependency, move to 2.8.", "root_cause": "The issue is caused by the difference in the implementation of std::nextafter between CPU (GCC) and XPU (SYCL). The XPU implementation of std::nextafter is not consistent with the CPU implementation, leading to the failure in the test.", "state": "open"}


### Merged Result:622{"issue_number": 622, "issue_description": "Triage is done\n- [x] test_reference_numerics_normal_polygamma_polygamma_n_1_xpu_float16: cpu issue https://github.com/pytorch/pytorch/issues/132386\n- [x] test_reference_numerics_normal_polygamma_polygamma_n_2_xpu_float16: cpu issue https://github.com/pytorch/pytorch/issues/132386\n- [x] test_reference_numerics_normal_polygamma_polygamma_n_3_xpu_float16: cpu issue https://github.com/pytorch/pytorch/issues/132386\n- [x] test_reference_numerics_normal_polygamma_polygamma_n_4_xpu_float16: cpu issue https://github.com/pytorch/pytorch/issues/132386\nXPU got the similar issue as cpu backend, both cpu and xpu float16 range has gap with scipy.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\n", "root_cause": "The issue is related to the float16 range gap with scipy.", "state": "closed"}


### Merged Result:618{"issue_number": 618, "issue_description": "1. module 'torch._C' has no attribute '_scatter' cases: TestAutograd.test_checkpointing_without_reentrant_dataparallel, TestMultithreadAutograd.test_dataparallel_saved_tensors_hooks\n2. AttributeError: module 'torch.xpu' has no attribute------passed on latest version cases: TestAutograd.test_graph_save_on_cpu_cuda, TestAutograd.test_checkpointing_without_reentrant_memory_savings\n3. NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend. cases: test_sparse_mask_autograd_xpu, test_sparse_ctor_getter_backward_xpu_float64, test_sparse_ctor_getter_backward_xpu_complex128, test_sparse_backward_xpu_float64, test_sparse_backward_xpu_complex128\n4. c10::NotImplementedError cases: TestAutogradMultipleDispatchXPU::test_autograd_multiple_dispatch_registrations_xpu\n5. RuntimeError: Double and complex datatype matmul is not supported in oneDNN cases: test_autograd_xpu.py::TestAutogradDeviceTypeXPU::test_mv_grad_stride_0_xpu\nLow priority features, No.1 is not an XPU specific issue. and not ATen operator (torch-xpu-ops) related. Scatter in c10d. No.2 is not operator (torch-xpu-ops) related. No.3 is a Sparse related feature. The direction of Sparse support is on-demand. Our priority follows requirements what we meet in HF/TIMM/TB models. No.4 is not an operator implementation issue. No.5 oneDNN issues are tracked in a separate issue.", "reporter": "PenghuiCheng", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:614{"issue_number": 614, "issue_description": "New failures occur when PyTorch uplifts. Guilty commit should be between f053be2a97e1f6f9b2252cb800edd46f720af502 and d44c30e2f90d9ebe829875324f0ac662d04833a8. test_compare_cpu_nn_functional_batch_norm_xpu_float16, test_compare_cpu_std_mean_xpu_bfloat16, test_compare_cpu_var_mean_xpu_bfloat16 all within the threashold.\nThe two cases can pass with updated threshold.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\nUpdated threshold", "root_cause": "", "state": "closed"}


### Merged Result:613{"issue_number": 613, "issue_description": "UT test error: RuntimeError: 0 <= device && static_cast<size_t>(device) < device_allocators.size() INTERNAL ASSERT FAILED\n\nCases:\n- TestDataLoaderDeviceTypeXPU.test_nested_tensor_multiprocessing_context_forkserver_xpu\n- TestDataLoaderDeviceTypeXPU.test_nested_tensor_multiprocessing_context_spawn_xpu\n\nThis issue is reported by PenghuiCheng and assigned to guangyey. The issue is closed.\nThis should not exists now. Close it.", "reporter": "PenghuiCheng", "assignee": "guangyey", "resolution": "\nThis should not exists now. Close it.", "root_cause": "", "state": "closed"}






### Merged Result:602{"issue_number": 602, "issue_description": "Nightly https://github.com/intel/torch-xpu-ops/actions/runs/9957414400/job/27509359347 xpu train jx_nest_base E0716 18:31:15.181000 140283238451008 torch/_dynamo/utils.py:1541] RMSE (res-fp64): 0.00285, (ref-fp64): 0.00003 and shape=torch.Size([512]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.010000 E0716 18:31:15.181000 140283238451008 torch/_dynamo/utils.py:1433] Accuracy failed for key name norm.bias.grad fail_accuracy Nightly https://github.com/intel/torch-xpu-ops/actions/runs/9957414400/job/27509359347 xpu train convnext_base E0716 19:50:58.170000 140456569096000 torch/_dynamo/utils.py:1541] RMSE (res-fp64): 0.02741, (ref-fp64): 0.01008 and shape=torch.Size([128, 1, 7, 7]). res.dtype: torch.float16, multiplier: 2.000000, tol: 0.010000 E0716 19:50:58.170000 140456569096000 torch/_dynamo/utils.py:1433] Accuracy failed for key name stages.0.blocks.0.conv_dw.weight.grad fail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/602. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\ncould pass with `_adaptive_avg_pool2d_backward` fallback to cpu", "root_cause": "jx_nest_base pass locally, convnext_base also fails on A100", "state": "closed"}


### Merged Result:601{"issue_number": 601, "issue_description": "RuntimeError: output 1: meta disagrees with real impl: \nThe issue is about the performance of the torch_xpu_ops library, specifically the problem of the slow execution of certain operations. The reporter of the issue is yuchengliu1, and the assignee is fengyuan14. The issue is closed with the state of closed. The comments for this issue are not provided.", "reporter": "yuchengliu1", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:598{"issue_number": 598, "issue_description": "TestCommonXPU::test_out_bincount_xpu_int64\n2024-07-11T08:48:47.7389771Z FATAL: Unexpected page fault from GPU at 0x56443f92d000, ctx_id: 1 (CCS) type: 0 (NotPresent), level: 3 (PML4), access: 1 (Write), banned: 1, aborting.\n\nThe issue is fixed in the latest Pytorch and torch-xpu-ops bundle.", "reporter": "mengfei25", "assignee": "Stonepia", "resolution": "\nThe issue is fixed in the latest Pytorch and torch-xpu-ops bundle.", "root_cause": "", "state": "closed"}


### Merged Result:594{"issue_number": 594, "issue_description": "AssertionError: Tensor-likes are not close! in test_reference_numerics_extremal__refs_acos_xpu_complex64, test_reference_numerics_extremal__refs_acosh_xpu_complex64, test_reference_numerics_extremal__refs_asin_xpu_complex64, test_reference_numerics_extremal__refs_log_xpu_complex64, test_reference_numerics_extremal__refs_tanh_xpu_complex128, test_reference_numerics_extremal__refs_tanh_xpu_complex64, test_reference_numerics_extremal_acos_xpu_complex64, test_reference_numerics_extremal_acosh_xpu_complex64, test_reference_numerics_extremal_asin_xpu_complex64, test_reference_numerics_extremal_log_xpu_complex64, test_reference_numerics_extremal_tanh_xpu_complex128, test_reference_numerics_extremal_tanh_xpu_complex64, test_reference_numerics_large__refs_acosh_xpu_complex64, test_reference_numerics_large_acosh_xpu_complex64, test_reference_numerics_large_tanh_xpu_complex32\nFor extreme value processing, Numpy and XPU results are inconsistent, std operations get different behavior on std::complex operarands for extremal cases.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nNot fixed", "root_cause": "Inconsistent results between Numpy and XPU for extreme value processing, different behavior for std::complex operands in std operations for extremal cases.", "state": "closed"}


### Merged Result:593{"issue_number": 593, "issue_description": "some case xfailed in cuda because of cuda bug. However XPU calculated correctly, and need not to xfail like cuda\n\nCase list as below:\n\"test_reference_numerics_large_rsqrt_xpu_complex32\"\n\"test_errors_histogramdd_xpu\"\n\"test_noncontiguous_samples__batch_norm_with_update_xpu_float32\"\n\"test_dispatch_symbolic_meta_outplace_all_strides__batch_norm_with_update_xpu_float32\"\n\"test_out_histc_xpu_float32\"\n\"test_out_warning_logcumsumexp_xpu\"\n\"test_python_ref__refs_mul_xpu_complex32\"\n\"test_python_ref_torch_fallback__refs_mul_xpu_complex32\"\n\"test_type_promotion_logaddexp_xpu\"\n\nUnexpected success on PVC and XFAIL on MTL device\n\"test_modules_xpu.py::TestModuleXPU::test_cpu_gpu_parity_nn_ConvTranspose1d_xpu_complex32\"\n\"test_modules_xpu.py::TestModuleXPU::test_cpu_gpu_parity_nn_ConvTranspose2d_xpu_complex32\"\n\"test_modules_xpu.py::TestModuleXPU::test_memory_format_nn_AvgPool2d_xpu_float32\"\n\"test_modules_xpu.py::TestModuleXPU::test_memory_format_nn_AvgPool2d_xpu_float64\"\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/593. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nDone in PR #608", "root_cause": "", "state": "closed"}


### Merged Result:592{"issue_number": 592, "issue_description": "AssertionError: True is not false\n\n- [ ] \"test_norm_fused_type_promotion_xpu_bfloat16\",\n- [ ] \"test_norm_fused_type_promotion_xpu_float16\",\n\nTo execute this test, run the following from the base repo dir:\n    PYTORCH_TEST_WITH_SLOW=1 python test/test_linalg.py -k TestLinalgXPU.test_norm_fused_type_promotion_xpu_bfloat16\n\n\nlow priority for 2.5", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:590{"issue_number": 590, "issue_description": "RuntimeError: \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/590. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nwill be fixed with #571", "root_cause": "", "state": "closed"}


### Merged Result:589{"issue_number": 589, "issue_description": "AssertionError: Tensor-likes are not equal! :\n    \"test_quick__batch_norm_with_update_xpu_bfloat16\",\n    \"test_quick__batch_norm_with_update_xpu_float16\",\n\nTo execute this test, run the following from the base repo dir:\n    PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=0 PYTORCH_TEST_WITH_SLOW=1 python test/test_decomp.py -k TestDecompXPU.test_quick__batch_norm_with_update_xpu_bfloat16\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/589. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nThe issue is closed.", "root_cause": "The issue is closed.", "state": "closed"}


### Merged Result:586{"issue_number": 586, "issue_description": "The existing solution: https://github.com/intel/torch-xpu-ops/pull/581\nTo split libtorch_xpu.so into multiple libraries,\n1. libtorch_xpu.so, including operator level program. It is host only.\n2. libtorch-xpu-ops-sycl-ker-partx.so, including kernel level program, both device code and host code.\ndevice code compression", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed\ndevice code compression mitigates panic", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/586. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed.", "state": "closed"}








### Merged Result:582{"issue_number": 582, "issue_description": "some cases in test_linalg.py use triton. Pre-ci has installed triton. But these case pass in local but fail in pre-ci. these case passed in a env with triton, but triton did not install in pre-ci.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/582. The reporter of the issue is yuchengliu1, and the assignee is mengfei25, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "mengfei25", "resolution": "\n", "root_cause": "pre-ci installed triton", "state": "closed"}


### Merged Result:578{"issue_number": 578, "issue_description": "RuntimeError: value cannot be converted to type float without overflow\n\nWhile considering the real part of complex inputs only, Jacobian computed with forward mode mismatch for output 0 with respect to input 0, test_fn_fwgrad_bwgrad_linalg_norm_xpu_complex128", "reporter": "yuchengliu1", "assignee": "fengyuan14", "resolution": "\nrrelu, sparsity, norm issues are fixed, addbmm issue depends on oneMKL.", "root_cause": "While considering the real part of complex inputs only, Jacobian computed with forward mode mismatch for output 0 with respect to input 0, test_fn_fwgrad_bwgrad_linalg_norm_xpu_complex128", "state": "closed"}


### Merged Result:577{"issue_number": 577, "issue_description": "Re-triage it by https://github.com/intel/torch-xpu-ops/commit/cbb4ab17a781c77108443f12f7ce254a345f1a14. Old issue is https://github.com/intel/torch-xpu-ops/issues/317\n\n# addmm.out, addmv.out, linalg_lstsq, norm.out, vdot&dot lack XPU support and fallback to CPU\n    \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/577. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\n", "root_cause": "", "state": "closed"}




### Merged Result:572{"issue_number": 572, "issue_description": "Implement aten::_unique for XPU\nThe operator is implemented, and the case is enabled.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed\nThe operator is implemented, and the case is enabled.", "root_cause": "", "state": "closed"}


### Merged Result:570{"issue_number": 570, "issue_description": "The operator 'aten::__lshift__.Scalar' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/570. The reporter of the issue is ZzEeKkAa, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "ZzEeKkAa", "assignee": "fengyuan14", "resolution": "closed\nClosing it, since it was implemented in #688", "root_cause": "The operators in the issue are not listed in our existing plan. However, the plan is changeable, we are listening to the community if there is any urgent requirement. And we will balance our limited efforts and pick them on at a proper moment.", "state": "closed"}


### Merged Result:551{"issue_number": 551, "issue_description": "E2E test got scratch page issue with rolling driver\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/551. The reporter of the issue is mengfei25, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "Stonepia", "resolution": "\n", "root_cause": "The issue is caused by the unexpected page fault from GPU, which is a known issue with the rolling driver and the xpu train AllenaiLongformerBase. The problem is that the GPU is trying to access a page that is not present in the memory, and the driver aborts the operation. This issue has been reported and closed in the github issue #551.", "state": "closed"}


### Merged Result:549{"issue_number": 549, "issue_description": "New failures occur when PyTorch uplifts. Guilty commit should be between f053be2a97e1f6f9b2252cb800edd46f720af502 and d44c30e2f90d9ebe829875324f0ac662d04833a8. The failures are: test_compare_cpu_nn_functional_batch_norm_xpu_float16, test_compare_cpu_std_mean_xpu_bfloat16, test_compare_cpu_sub_xpu_float16, test_compare_cpu_var_mean_xpu_bfloat16, test_symnode_hashing, test_index_ind_dtype_xpu. Also, the test_compare_cpu_sub_xpu_float16 failed on cuda.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/549. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\ntest_index_ind_dtype_xpu can pass with 10c7f037fe3271cb3865816c216007ba403f5347 and reverted PR https://github.com/intel/torch-xpu-ops/commit/4db0b0cd1ca51d9cfd890be2eb3527b165782220, because checkIndexTensorTypes interface changes is reverted by https://github.com/pytorch/pytorch/commit/fb696bf26457a60583f4c43f1a6547b16725c016. So we should also revert torch-xpu-ops 4db0b0cd1ca51d9cfd890be2eb3527b165782220, add by the 2nd parameter of checkIndexTensorTypes.", "root_cause": "The issue of test_symnode_hashing has disappeared in latest version of pytorch and torch-xpu-ops.", "state": "closed"}


### Merged Result:544{"issue_number": 544, "issue_description": "Got numerical difference compared with CPU results. It is hard to say who is better on accuracy.\nNot a critical error. Low priority.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:536{"issue_number": 536, "issue_description": "retrieve this case when this op has been implemented , test_backward_nn_functional_embedding_bag_xpu_float32\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/536. The reporter of the issue is chunhuanMeng, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "chunhuanMeng", "assignee": "fengyuan14", "resolution": "\nwe can close this issue", "root_cause": "The reporter of the issue is chunhuanMeng, and the assignee is fengyuan14, and the state of the issue is closed.", "state": "closed"}


### Merged Result:528{"issue_number": 528, "issue_description": "After enabling XPU adaptive pooling 2d, accuracy of Eager is better than Inductor (before that, CPU fallback == Inductor), and the gap could not be accepted by default Dynamo benchmark tolerance. Eager RMSE (res-fp64): 0.00068, (ref-fp64): 0.00004 and shape=torch.Size([1152]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.001000 Accuracy failed for key name blocks.6.0.bn1.bias.grad\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/528. The reporter of the issue is fengyuan14, and the assignee is riverliuintel, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "riverliuintel", "resolution": "Loose the tolerance for the model temporarily.\nverified pass with latest code", "root_cause": "The issue is caused by a precision error of Inductor on TorchBench/timm_efficientnet ampbf16 training. The reporter of the issue is fengyuan14, and the assignee is riverliuintel, and the state of the issue is closed.", "state": "closed"}


### Merged Result:523{"issue_number": 523, "issue_description": "When output_size == 1, CPU and CUDA are using reduce mean, but we are using adaptive_avg_pool. The story is we preferred oneDNN implementation before. The issue is recorded to evaluate whether the difference should be kept. The current implementation is to register wrapper variant aten::adaptive_avg_pool2d to retrieve the logic for XPU.\nWithout the logic (mean for output_size == 1), a model in TorchBench crashes due to lack of deterministic impl in adaptive avg pool2d.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nThe behavior of XPU `adaptive_avg_pool{2d/3d}` has been aligned to CUDA/CPU after [pytorch#132217](https://github.com/pytorch/pytorch/pull/132217) is merged. Our WA for `output_size == 1` will not be executed. The redundant implementation in torch-xpu-ops has been removed by #851.", "root_cause": "The logic for mean operation when output_size is 1 was missing in the XPU implementation of adaptive avg pool2d, leading to a crash in TorchBench due to lack of deterministic implementation.", "state": "closed"}


### Merged Result:510{"issue_number": 510, "issue_description": "torchbench_amp_bf16_training xpu train functorch_maml_omniglot fail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/510. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nclosed", "root_cause": "The issue was closed as the parent tracker passed.", "state": "closed"}


### Merged Result:509{"issue_number": 509, "issue_description": "torchbench_bfloat16_training\nxpu  train phlippe_resnet \nE0626 09:53:20.652000 139764145854272 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.00734, (ref-fp64): 0.00047 and shape=torch.Size([]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.001000\nfail_accuracy\n\nNot very large absolute error, and this model could pass if increasing tol to 5*1e-3", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nPublic PR to raise tolerance: https://github.com/pytorch/pytorch/pull/134192", "root_cause": "Not very large absolute error", "state": "closed"}


### Merged Result:508{"issue_number": 508, "issue_description": "torchbench_bfloat16_training xpu train functorch_dp_cifar10 got fail_accuracy, RMSE (res-fp64): 0.00109, (ref-fp64): 0.00027 and shape=torch.Size([64]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.001000, Accuracy failed for key name bn1.bias.grad\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/508. The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "The issue is caused by the convolution_backward but we're still looking for the fix.", "state": "closed"}




### Merged Result:506{"issue_number": 506, "issue_description": "torchbench_bfloat16_training xpu train demucs RuntimeError: Input type (float) and bias type (c10::BFloat16) should be the same\nBoth fp16 and bf16 have the problem", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nClose the issue, since A100 also encounters the issue", "root_cause": "A100 enabled this model recently", "state": "open"}


### Merged Result:505{"issue_number": 505, "issue_description": "torchbench_float32_training xpu train stable_diffusion_unet\nThis model is 'pass_due_to_skip' in cuda due to too large size. We need to make sure it has the same behaviour on xpu with fp32.", "reporter": "mengfei25", "assignee": "mengfei25", "resolution": "\nfixed `import name 'cached_download' from 'huggingface_hub'` in https://github.com/intel/torch-xpu-ops/pull/1218", "root_cause": "XPU out of memory, please use `empty_cache` to release all unoccupied cached memory.", "state": "closed"}


### Merged Result:504{"issue_number": 504, "issue_description": "torchbench_float32_training xpu train demucs got fail_accuracy, RMSE (res-fp64): 0.03316, (ref-fp64): 0.00065 and shape=torch.Size([]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000\nKnown issue. Use to be closed because it's a common issue for all backends: CPU, CUDA, XPU.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nwill close this issue, and only track issues their with adding fp32 label", "root_cause": "similar issue in https://github.com/intel/torch-xpu-ops/issues/488.", "state": "closed"}


### Merged Result:503{"issue_number": 503, "issue_description": "W0626 13:18:03.033000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:18:03.033000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:18:03.212000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:18:03.213000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:18:03.390000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:18:03.391000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:19:19.452000 139736640825152 torch/_dynamo/utils.py:1452] Found nan in reference. Consider running in higher precision.\nE0626 13:19:19.452000 139736640825152 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.02577, (ref-fp64): nan and shape=torch.Size([4, 3, 12, 16, 85]). res.dtype: torch.float16, multiplier: 2.000000, tol: 0.001000\nfail_accuracy\nNo accuracy issue in latest torch xpu ops", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nNo accuracy issue", "root_cause": "", "state": "closed"}


### Merged Result:502{"issue_number": 502, "issue_description": "torchbench_float16_training xpu train functorch_dp_cifar10 Accuracy failed for key name bn1.bias.grad\nSimilar to https://github.com/intel/torch-xpu-ops/issues/508", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\nclosed", "root_cause": "duplicate", "state": "closed"}


### Merged Result:501{"issue_number": 501, "issue_description": "torchbench_float16_training xpu train squeezenet1_1 got fail_accuracy\nSimilar issue in https://github.com/intel/torch-xpu-ops/issues/507, add fp16 datatype label in that issue", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:500{"issue_number": 500, "issue_description": "torchbench_float16_training xpu train Background_Matting RuntimeError: reflection_pad2d not implemented for 'Half'", "reporter": "mengfei25", "assignee": "", "resolution": "", "root_cause": "reflection_pad2d is not implemented for 'Half' in the current version of torch-xpu-ops.", "state": "closed"}






### Merged Result:495{"issue_number": 495, "issue_description": "torchbench_amp_fp16_training xpu train tts_angular Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2294, in validate_model self.model_iter_fn(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 456, in forward_and_backward_pass pred = mod(*cloned_inputs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1566, in _wrapped_call_impl return self._call_impl(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1575, in _call_impl return forward_call(*args, **kwargs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/tts_angular/model.py\", line 61, in forward d = torch.nn.functional.normalize(d[:, -1], p=2, dim=1)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/functional.py\", line 4816, in normalize denom = input.norm(p, dim, keepdim=True).clamp_min(eps).expand_as(input)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_tensor.py\", line 768, in norm return torch.norm(self, p, dim, keepdim, dtype=dtype)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/functional.py\", line 1858, in norm return _VF.norm(input, p, _dim, keepdim=keepdim)  # type: ignore[attr-defined] NotImplementedError: The operator 'aten::norm.dtype_out' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU. The above exception was the direct cause of the following exception: Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4177, in run ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 380, in load_model self.validate_model(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2296, in validate_model raise RuntimeError(\nThis operator is not implemented with xpu backend. @fengyuan14  please help to check this issue. Thanks.", "reporter": "mengfei25", "assignee": "fengyuan14", "resolution": "The issue has been resolved by the reporter and the assignee. The reporter opened a feature request on the github issue link provided and the assignee is working on it.\nThe issue is resolved by the pull request #556.", "root_cause": "The error is caused by the fact that the 'aten::norm.dtype_out' operator is not currently implemented for the XPU device. The reporter has provided a workaround by setting the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators.", "state": "closed"}


### Merged Result:494{"issue_number": 494, "issue_description": "Torch_multimodal_clip NotImplementedError: The operator 'aten::norm.dtype_out' is not currently implemented for the XPU device.\nThe reporter of the issue is mengfei25, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "fengyuan14", "resolution": "\npass in latest weekly", "root_cause": "The operator 'aten::norm.dtype_out' is not currently implemented for the XPU device.", "state": "closed"}


### Merged Result:493{"issue_number": 493, "issue_description": "torchbench_amp_fp16_training xpu train timm_regnet E0626 18:18:36.100000 139652021139264 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.00227, (ref-fp64): 0.00064 and shape=torch.Size([]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000 fail_accuracy float16 E0626 13:14:09.343000 139963949791040 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.00150, (ref-fp64): 0.00032 and shape=torch.Size([224]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000 E0626 13:14:09.343000 139963949791040 torch/_dynamo/utils.py:1392] Accuracy failed for key name s3.b4.se.fc1.bias.grad fail_accuracy\nNot very large absolute error, and this model could pass if increasing tol to 1e-2", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:492{"issue_number": 492, "issue_description": "Timm_efficientdet NotImplementedError: The original model code forces the use of CUDA.\nThis model requests us to add xpu support for both the benchmark repo and third-party repo efficientdet-pytorch as it writes hard code with cuda like: (in https://github.com/rwightman/efficientdet-pytorch/blob/master/effdet/data/loader.py).", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "The code in efficientdet-pytorch uses cuda and does not have xpu support.", "state": "open"}


### Merged Result:491{"issue_number": 491, "issue_description": "Pytorch_CycleGAN_and_pix2pix RuntimeError: \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/491. The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "The issue is on the cpu implementation. Our implementation in IPEX has enabled the fp16 support.", "state": "closed"}


### Merged Result:490{"issue_number": 490, "issue_description": "FastNLP_Bert Accuracy failed for key name bert.model.encoder.embeddings.LayerNorm.weight.grad, and issue body Content of #490 is : ### \ud83d\udc1b Describe the bug torchbench_amp_fp16_training xpu  train fastNLP_Bert W0626 17:49:36.629000 140234806880064 torch/_dynamo/variables/tensor.py:715] [3/0] Graph break from `Tensor.item()`, consider setting: W0626 17:49:36.629000 140234806880064 torch/_dynamo/variables/tensor.py:715] [3/0]     torch._dynamo.config.capture_scalar_outputs = True W0626 17:49:36.629000 140234806880064 torch/_dynamo/variables/tensor.py:715] [3/0] or: W0626 17:49:36.629000 140234806880064 torch/_dynamo/variables/tensor.py:715] [3/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1 W0626 17:49:36.629000 140234806880064 torch/_dynamo/variables/tensor.py:715] [3/0] to include these operations in the captured graph. W0626 17:49:36.629000 140234806880064 torch/_dynamo/variables/tensor.py:715] [3/0] or: W0626 17:49:36.629000 140234806880064 torch/_dynamo/variables/tensor.py:715] [3/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1 E0626 17:50:10.260000 140234806880064 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.00383, (ref-fp64): 0.00017 and shape=torch.Size([768]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000 E0626 17:50:10.261000 140234806880064 torch/_dynamo/utils.py:1392] Accuracy failed for key name bert.model.encoder.embeddings.LayerNorm.weight.grad fail_accuracy\nNo accuracy issue in latest torch xpu ops", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nNo accuracy issue", "root_cause": "", "state": "closed"}


### Merged Result:489{"issue_number": 489, "issue_description": "torchbench_amp_fp16_training xpu train moco Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4177, in run  ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 320, in load_model  benchmark = benchmark_cls(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__  obj = type.__call__(cls, *args, **kwargs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/moco/__init__.py\", line 80, in __init__  raise NotImplementedError(f\"{device} not supported\") NotImplementedError: xpu not supported model_fail_to_load\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/489. The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:488{"issue_number": 488, "issue_description": "torchbench_amp_fp16_training xpu train demucs E0626 17:48:23.409000 140063059666752 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.03316, (ref-fp64): 0.00065 and shape=torch.Size([]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000 fail_accuracy\nThis model failure is not the random seed setting issue which we fixed before. It should be a new issue caused by recent changes. Previously, the model could pass after fixing random seed issue.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nhttps://github.com/pytorch/pytorch/pull/134302 to disable reorder_for_locality.", "root_cause": "The random kernels output different value if their orders are changed on xpu.", "state": "closed"}






### Merged Result:475{"issue_number": 475, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/475. The reporter of the issue is dvrogozh, and the assignee is , and the state of the issue is closed.\ntypedef accscalar_t locally defined but not used\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/475. The reporter of the issue is dvrogozh, and the assignee is , and the state of the issue is closed.", "reporter": "dvrogozh", "assignee": "", "resolution": "\n\nthat's just a warning. I'll post a PR to fix shortly.", "root_cause": "", "state": "closed"}




### Merged Result:469{"issue_number": 469, "issue_description": "RuntimeError: ceil is not supported for complex inputs\nRuntimeError: floor is not supported for complex inputs\nRuntimeError: trunc is not supported for complex inputs\nRuntimeError: \nRuntimeError: ceil is not supported for complex inputs", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "closed\nclosed", "root_cause": "The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "state": "closed"}


### Merged Result:468{"issue_number": 468, "issue_description": "Implement interpolate_bilinear and interpolate_bicubic, cases which are skipped: 'test_dtypes_nn_functional_interpolate_bilinear_xpu', 'test_dtypes_nn_functional_interpolate_bicubic_xpu', root cause: fallback to cpu's implementation but use the dtypes claim by xpu\nall passed", "reporter": "chunhuanMeng", "assignee": "majing921201", "resolution": "closed\nall passed", "root_cause": "fallback to cpu's implementation but use the dtypes claim by xpu", "state": "closed"}


### Merged Result:464{"issue_number": 464, "issue_description": "test_fn_grad__unsafe_masked_index_xpu_complex128 test_fn_grad__unsafe_masked_index_xpu_float64 test_fn_gradgrad__unsafe_masked_index_put_accumulate_xpu_complex128 test_fn_gradgrad__unsafe_masked_index_put_accumulate_xpu_float64 New masked index put cases fail on complex128 and complex64\nThe case failed because with the same op output, the torch.autograd.grad() cannot return exactly the same result.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nThe PR #474 is created to align with cuda.", "root_cause": "The xpu implementation logic is different from cuda's, specifically in the logic of determining whether to use deterministic algorithms.", "state": "closed"}


### Merged Result:461{"issue_number": 461, "issue_description": "FP8 data types are turned on in index put cases. PT2.5 plan of XPU implementation does not include FP8 support. Skip them temporarily.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/461. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is open.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "FP8 is not a goal of PT2.5 or PT2.6", "state": "open"}


### Merged Result:455{"issue_number": 455, "issue_description": "We didn't add nn.functional.grid_sample to test list. due to grid_sampler_3d is not implemented. Will retrieve once 3d implemented\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/455. The reporter of the issue is majing921201, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "majing921201", "assignee": "majing921201", "resolution": "\nImplemented in https://github.com/intel/torch-xpu-ops/pull/898", "root_cause": "grid_sampler_3d is not implemented", "state": "closed"}


### Merged Result:436{"issue_number": 436, "issue_description": "The following dtypes did not work in backward but are listed by the OpInfo: {torch.bfloat16} cases: TestCommonXPU.test_dtypes_view_as_complex_xpu TestCommonXPU.test_dtypes_view_as_real_xpu\nThe reporter of the issue is PenghuiCheng, and the assignee is ZhiweiYan-96, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "ZhiweiYan-96", "resolution": "closed\nThe issue is closed.", "root_cause": "The issue is related to the MKLDNN implementation of addbmm, which does not support complex and real data types. The reporter PenghuiCheng reported the issue and the assignee ZhiweiYan-96 provided the resolution.", "state": "closed"}


### Merged Result:435{"issue_number": 435, "issue_description": "Sigmoid op didn't be supported with complex32 which didn't align with CUDA behavior.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/435. The reporter of the issue is PenghuiCheng, and the assignee is PenghuiCheng, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "\nfixed", "root_cause": "This issue is related to the implementation of the sigmoid operation in the XPU backend. The XPU backend does not currently support the sigmoid operation for complex32 data type, which is different from the behavior of CUDA. The root cause is that the XPU backend developers have not yet implemented the sigmoid operation for complex32 data type.", "state": "closed"}


### Merged Result:432{"issue_number": 432, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.", "reporter": "chuanqi129", "assignee": "mengfei25", "resolution": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "root_cause": "", "state": "open"}


### Merged Result:427{"issue_number": 427, "issue_description": "CUDA add one kernel for performance optimization to support partial channel last case.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/427. The reporter of the issue is majing921201, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "majing921201", "assignee": "majing921201", "resolution": "closed\n", "root_cause": "performance enhancement is not in 2.5 scope", "state": "closed"}


### Merged Result:426{"issue_number": 426, "issue_description": "This issue just for track on-demand test result and inform the result to scheduler.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.", "reporter": "chuanqi129", "assignee": "mengfei25", "resolution": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "root_cause": "The issue is related to the model dm_nfnet_f0, and the reporter mentioned that the issue is related to the on-demand test on 2024-07-08, where the test failed. The test failed on the device pytorch-06 with OS Ubuntu 22.04.2 LTS, GCC 11, Python 3.8, and Driver(DKMS) 1.23.10.49.231129.50, and Bundle(DPCPP) 2024.1.1.20240321. The test failed on the inputs huggingface/float32,bfloat16,float16,amp_bf16,amp_fp16/inference,training/accuracy. The test passed on the device pytorch-01 with OS Ubuntu 22.04.2 LTS, GCC 11, Python 3.10, and Driver(DKMS) 1.23.10.49.231129.50, and Bundle(DPCPP) 2024.1.1.20240321. The test passed on the inputs torchbench/float32,bfloat16,float16,amp_bf16,amp_fp16/inference,training/accuracy. The test passed on the device pytorch-02 with OS Ubuntu 22.04.2 LTS, GCC 11, Python 3.10, and Driver(DKMS) 1.23.10.49.231129.50, and Bundle(DPCPP) 2024.1.1.20240321. The test passed on the inputs timm_models/float32,bfloat16,float16,amp_bf16,amp_fp16/inference,training/accuracy.", "state": "open"}


### Merged Result:414{"issue_number": 414, "issue_description": "TorchBench Bf16 yolov3 fails\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/414. The reporter of the issue is fengyuan14, and the assignee is etaf, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "etaf", "resolution": "\nThe model can pass accuracy test on currently pinned triton now.", "root_cause": "The accuracy issue is introduced by https://github.com/pytorch/pytorch/pull/128269, which triggered a triton accuracy bug.", "state": "closed"}


### Merged Result:412{"issue_number": 412, "issue_description": "We have improved and aligned the condition of device choice in test infrastructure. After that, we cannot correctly skip fine-gran cases. New failure in fine-gran cases, test_compare_cpu_abs_xpu_bool, which was skipped before the commit above.\nThe reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nThe case is skipped in latest code", "root_cause": "", "state": "closed"}


### Merged Result:410{"issue_number": 410, "issue_description": "Inductor case expose a SegmentFault in XPU resize_as. I've created an isolate case:\n\n```\nimport torch\n# device=\"xpu\" #segmentfault\ndevice=\"cpu\" # ok\n# device=\"cuda\" ok\nx = torch.ones(1, 2, device=device)\ny = torch.ones(1, 1, 3, 2, 3, device=device)\nout = torch.ops.aten.resize_as(x, y)\nprint(\"x:\", x)\nprint(\"out size:\", out.size())\nprint(\"out\", out)\n\n# cpu result:\n# x:\n#  tensor([[1., 1.]])\n# out size:\n#  torch.Size([1, 1, 3, 2, 3])\n# out\n#  tensor([[[[ 1.0000e+00,  1.0000e+00,  0.0000e+00],\n#            [ 0.0000e+00, -2.8010e+32,  4.5898e-41]],\n# \n#           [[-5.5715e+20, -2.2881e+15, -4.8480e+32],\n#            [ 4.5898e-41, -7.0579e-22, -5.4955e-21]],\n# \n#           [[-2.8596e+32,  4.5898e-41,  5.2044e+27],\n#            [-2.9831e-31, -5.2708e+32,  4.5898e-41]]]])\n######\n# cuda result:\n# x:\n#  tensor([[1., 1.]], device='cuda:0')\n# out size:\n#  torch.Size([1, 1, 3, 2, 3])\n# out\n#  tensor([[[[1., 1., 0.],\n#            [0., 0., 0.]],\n# \n#           [[0., 0., 0.],\n#            [0., 0., 0.]],\n# \n#           [[0., 0., 0.],\n#            [0., 0., 0.]]]], device='cuda:0')\n\n```", "reporter": "etaf", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:408{"issue_number": 408, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/408. The reporter of the issue is fengyuan14, and the assignee is etaf, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/408. The reporter of the issue is fengyuan14, and the assignee is etaf, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "etaf", "resolution": "\nThis issue have been fixed in latest triton main branch.", "root_cause": "", "state": "closed"}


### Merged Result:397{"issue_number": 397, "issue_description": "UT got failed with python 3.10\nUfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_int32, UfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_int64, UfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_int8, UfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_uint8 got failed with python 3.10. PYTORCH_TEST_WITH_SLOW=1 python test/test_unary_ufuncs.py -k TestUnaryUfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_int64 PYTORCH_TEST_WITH_SLOW=1 python test/test_unary_ufuncs.py -k TestUnaryUfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_int8 PYTORCH_TEST_WITH_SLOW=1 python test/test_unary_ufuncs.py -k TestUnaryUfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_int8 PYTORCH_TEST_WITH_SLOW=1 python test/xpu/test_autocast_xpu.py -k TestAutocastGPU.test_cast_cache_is_global\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/397. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\n\n", "root_cause": "TestAutocastGPU.test_cast_cache_is_global is caused by pytorch, 0606 nightly is fine", "state": "closed"}


### Merged Result:386{"issue_number": 386, "issue_description": "The operator has been implemented in torch-xpu-ops. Need reevaluate the skipped cases (in run_test_with_skip.py). https://github.com/intel/torch-xpu-ops/pull/371\nRuntime error", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "closed\n", "root_cause": "", "state": "closed"}


### Merged Result:384{"issue_number": 384, "issue_description": "Failure in pre-ci, https://github.com/intel/torch-xpu-ops/actions/runs/9413187922/job/25929444890?pr=366 test_autocast_xpu.py::TestAutocastGPU::test_cast_cache_is_global FAILED [ 25%] Guilty commit, https://github.com/pytorch/pytorch/pull/127741\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/384. The reporter of the issue is fengyuan14, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "guangyey", "resolution": "\nfixed in https://github.com/pytorch/pytorch/pull/128383", "root_cause": "", "state": "closed"}


### Merged Result:380{"issue_number": 380, "issue_description": "To retrieve the fine gran case when embedding_renorm_ added.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/380. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nImplemented in https://github.com/intel/torch-xpu-ops/pull/885", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/380. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed.", "state": "closed"}


### Merged Result:379{"issue_number": 379, "issue_description": "Implement  op `aten::_upsample_nearest_exact3d.out` and `aten::upsample_nearest3d_backward.grad_input`, and cases: 'test_compare_cpu_nn_functional_interpolate_nearest-exact_xpu_bfloat16', 'test_compare_cpu_nn_functional_interpolate_nearest-exact_xpu_float16', 'test_compare_cpu_nn_functional_interpolate_nearest-exact_xpu_float32', 'test_compare_cpu_nn_functional_interpolate_nearest-exact_xpu_float64', 'test_compare_cpu_nn_functional_interpolate_nearest-exact_xpu_uint8', 'test_backward_nn_functional_interpolate_nearest-exact_xpu_float32', 'test_forward_ad_nn_functional_interpolate_nearest-exact_xpu_float32', 'test_operator_nn_functional_interpolate_nearest-exact_xpu_float32', 'test_view_replay_nn_functional_interpolate_nearest-exact_xpu_float32', 'test_backward_nn_functional_interpolate_nearest_xpu_float32', 'test_backward_nn_functional_upsample_nearest_xpu_float32'.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/379. The reporter of the issue is chunhuanMeng, and the assignee is chunhuanMeng, and the state of the issue is closed.", "reporter": "chunhuanMeng", "assignee": "chunhuanMeng", "resolution": "\nWith this pr, cases above all passed\uff0chttps://github.com/intel/torch-xpu-ops/pull/869", "root_cause": "", "state": "closed"}


### Merged Result:375{"issue_number": 375, "issue_description": "The following code some times hung and sometimes get Native API failed. Native API returns: -2 (PI_ERROR_DEVICE_NOT_AVAILABLE) -2 (PI_ERROR_DEVICE_NOT_AVAILABLE)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/375. The reporter of the issue is etaf, and the assignee is ZhiweiYan-96, and the state of the issue is closed.", "reporter": "etaf", "assignee": "ZhiweiYan-96", "resolution": "\nClose as this no longer exists", "root_cause": "should be issue in oneDNN, can reproduce error with benchdnn", "state": "closed"}


### Merged Result:372{"issue_number": 372, "issue_description": "These `nil_loss2d_*` ops are not currently implemented for XPU backend and are not marked for \"explicit\" CPU fallback. This means that running model with any of these ops will fail at runtime by default (with not implemented error) and will require to set `PYTORCH_DEBUG_XPU_FALLBACK=1`.\n\n- [x] `aten::nll_loss2d_backward`\n- [x] `aten::nll_loss2d_forward`\n\nThis issue affects XPU enabling for Huggingface - https://github.com/huggingface/transformers/issues/31237#issuecomment-2148067845. See table in this comment for the list of affected examples and models.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/372. The reporter of the issue is dvrogozh, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "dvrogozh", "assignee": "fengyuan14", "resolution": "\nThe issue is resolved and closed", "root_cause": "", "state": "closed"}


### Merged Result:367{"issue_number": 367, "issue_description": "11 tests failed in UT with latest driver 803.58. The tests are related to XPU ops and are listed in the issue body.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/367. The reporter of the issue is mengfei25, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "daisyden", "resolution": "\nclosed", "root_cause": "", "state": "closed"}


### Merged Result:365{"issue_number": 365, "issue_description": "reproduce this op error with a training script for model resnet50 with AMP FP16 and GradScaler", "reporter": "ZhaoqiongZ", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:363{"issue_number": 363, "issue_description": "Some XPU operators fail in test_meta. Will enable the case and have a fully functionality checking to see what we missed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/363. The reporter of the issue is fengyuan14, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "yuchengliu1", "resolution": "closed\nenable in https://github.com/intel/torch-xpu-ops/pull/571", "root_cause": "", "state": "closed"}


### Merged Result:358{"issue_number": 358, "issue_description": "The operator 'aten::_foreach_mul_.Scalar' is not currently implemented for the XPU device. Train with a simple resnet50 model on XPU and found _foreach_mul_.Scalar not implemented Fallback to CPU can help on running the training process, but will have a performance implications.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/358. The reporter of the issue is ZhaoqiongZ, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "ZhaoqiongZ", "assignee": "fengyuan14", "resolution": "The issue is closed.\nNot included", "root_cause": "The operator 'aten::_foreach_mul_.Scalar' is not currently implemented for the XPU device.", "state": "closed"}


### Merged Result:357{"issue_number": 357, "issue_description": "Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend. \n\nRuntimeError: device type of values (xpu) must be CPU or CUDA or Meta\n\nRuntimeError: device type of values (xpu) must be CPU or CUDA or Meta\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/357. The reporter of the issue is yuchengliu1, and the assignee is majing921201, and the state of the issue is closed.\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/357. The reporter of the issue is yuchengliu1, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "majing921201", "resolution": "\nclosed\n\nclosed", "root_cause": "The reporter reported that the operator 'aten::_sparse_coo_tensor_with_dims_and_tensors' has been supported, but the cases failed in other places. The root cause is that the operator is only available for certain backends, and the backend used in the test cases is not one of the supported backends.", "state": "closed"}


### Merged Result:348{"issue_number": 348, "issue_description": "1.  torch.backends.mkldnn.flags() got an unexpected keyword argument 'deterministic'---fixed\n2. Tensor-likes are not close! -----fixed\n3. check_random_bounds handles only integral, floating-point and boolean types---fixed\n4. largeTensorTest didn't support XPU device ---fixed\n5. NotImplementedError: The operators \nThe issue is about the convolution operation in torch-xpu-ops, which is not working as expected. The reporter PenghuiCheng has reported that the first four issues have been fixed, but the fifth issue is not in the 2.5 plan. The reporter also mentioned that there is a PR for convolution ut, but it is not merged yet.", "reporter": "PenghuiCheng", "assignee": "ZhiweiYan-96", "resolution": "fixed\n", "root_cause": "The fifth issue is not in the 2.5 plan and there is a PR for convolution ut, but it is not merged yet.", "state": "open"}


### Merged Result:342{"issue_number": 342, "issue_description": "Evaluated Issues in test_multihead_attention because _check_arg_device does not have xpu support\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/342. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "closed\nclosed", "root_cause": "_check_arg_device does not have xpu support", "state": "closed"}


### Merged Result:339{"issue_number": 339, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/339. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is closed.\nnn/utils/rnn.py PackedSequence() need to add an xpu() function.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "Device guard is correctly used. Will check backend.", "state": "closed"}


### Merged Result:327{"issue_number": 327, "issue_description": "Hard-coded CPU/CUDA bias in aten::mode_out. To upstream to make the operator device compatible.\nmode only supports CPU AND CUDA device type, got: xpu", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed\nThe PR is merged", "root_cause": "The issue is related to the PR https://github.com/pytorch/pytorch/pull/137575", "state": "closed"}


### Merged Result:325{"issue_number": 325, "issue_description": "Double and complex datatype matmul is not supported in oneDNN\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/325. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/325. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.\nRuntimeError: DispatchStub: unsupported device type xpu", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nclosed\nclosed\nclosed", "root_cause": "Double and complex datatype matmul is not supported in oneDNN", "state": "closed"}


### Merged Result:322{"issue_number": 322, "issue_description": "No failed, but some skips\n\n# FP8 is only supported on H100+ and sm_89 and MI300+ devices\n\nTestFP8MatmulCudaXPU::test_float32_output_errors_with_bias_xpu\nTestFP8MatmulCudaXPU::test_float8_basics_xpu\nTestFP8MatmulCudaXPU::test_float8_bias_relu_edgecase_xpu\nTestFP8MatmulCudaXPU::test_float8_bias_xpu\nTestFP8MatmulCudaXPU::test_float8_scale_fast_accum_xpu\nTestFP8MatmulCudaXPU::test_float8_scale_xpu\nTestFP8MatmulCudaXPU::test_non_divisible_leading_dim_bias_False_xpu\nTestFP8MatmulCudaXPU::test_non_divisible_leading_dim_bias_True_xpu\nTestFP8MatmulCudaXPU::test_scaled_mm_vs_emulated_bfloat16_xpu\nTestFP8MatmulCudaXPU::test_scaled_mm_vs_emulated_float16_xpu\nTestFP8MatmulCudaXPU::test_scaled_mm_vs_emulated_float32_xpu\n\n# mixed dtypes linear only supported on SM 8.x\n\nTestMixedDtypesLinearCudaXPU::test_mixed_dtypes_linear_xpu_bfloat16\nTestMixedDtypesLinearCudaXPU::test_mixed_dtypes_linear_xpu_float16\nThe test has been rework. The last failures are as below:\n# AssertionError: ", "reporter": "yuchengliu1", "assignee": "fengyuan14", "resolution": "\nLow priority", "root_cause": "FP8 support", "state": "open"}


### Merged Result:320{"issue_number": 320, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/320. The reporter of the issue is yuchengliu1, and the assignee is majing921201, and the state of the issue is closed. The issue title is [To Evaluate] Issues in masked_UT, and the issue body content is : ### \ud83d\udc1b Describe the bug\n#NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build).\n\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/320. The reporter of the issue is yuchengliu1, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "majing921201", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:317{"issue_number": 317, "issue_description": "addmm.out, addmv.out, addr, linalg_lstsq, linalg_vector_norm.out, norm.out, vdot&dot lack XPU support and fallback to CPU \n\n\nThe issue is that the addr function does not support xpu for complex128, complex64, float16, float32, float64, int16, int32, int64, int8, uint8 data types. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nThe issue is resolved and the new issue is https://github.com/intel/torch-xpu-ops/issues/577", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/317. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "state": "closed"}


### Merged Result:304{"issue_number": 304, "issue_description": "This issue will be auto-comment by actions when nightly failure detected for notify relevant owners awareness.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/304. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.\nXPU OPS Nightly Successful 2024-06-18, See: https://github.com/intel/torch-xpu-ops/actions/runs/9561980938", "reporter": "mengfei25", "assignee": "", "resolution": "\n\nclosed", "root_cause": "https://github.com/intel/torch-xpu-ops/issues/304", "state": "closed"}


### Merged Result:302{"issue_number": 302, "issue_description": "To enable memory check in test framework, we need to have the counterpart of the two cuda functions: torch.cuda.memory_allocated(), torch.cuda.mem_get_info(). To enable CudaSyncGuard in test framework, we depend on the counterpart of torch.cuda.set_sync_debug_mode. To enable largeTensorTest in test framework, we depend on the counterpart of torch.cuda.memory.mem_get_info. To run test_storage_meta_errors() need torch.TypedStorage.xpu support. To run test_dtypetensor_warnings need the counterpart of torch.cuda.FloatTensor and torch.cuda.DoubleTensor support in xpu backend. float() need to have an is_xpu() interface, test_broadcast(). xpu is missed in torch.backends, see https://pytorch.org/docs/stable/backends.html. So that we cannot write a counterpart of this: @unittest.skipIf(torch.backends.cuda.is_built() or IS_SANDCASTLE, \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/302. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "The TensorInfo will cause the error: https://github.com/intel/torch-xpu-ops/blob/main/src/comm/TensorInfo.h#L193. The issue is caused by the assumption of int64 in the template of getTensorInfo when index dtype is int32.", "state": "closed"}


### Merged Result:296{"issue_number": 296, "issue_description": "Tensor isn't pinned with DataLoader(..., pin_memory=True,..)\ncommand:\n```python\nPYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test/xpu/test_dataloader_xpu.py\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/296. The reporter of the issue is PenghuiCheng, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "guangyey", "resolution": "\nClosed as it has been improved.", "root_cause": "", "state": "closed"}


### Merged Result:294{"issue_number": 294, "issue_description": "1. RuntimeError: Unsupported memory formatPreserve\n2. NotImplementedError: Could not run 'aten::_empty_affine_quantized' with arguments from the 'QuantizedXPU' backend.\ntest_memory_format_resize_as_xpu is fixed by remove the xpu dispatch of resize_as_. Preci is WIP.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\nremove the xpu dispatch of resize_as_", "root_cause": "Not specified", "state": "closed"}


### Merged Result:281{"issue_number": 281, "issue_description": "To support or make XPU Tensor compatible with copy-on-write feature.\n\nCase: TestCompositeCompliance::test_cow_input\n\nAssertionError: False is not true : Keyword argument 'output grad 0' during backward call unexpectedly materializes. Either set `supports_cow_input_no_materialize_backward=False` in this operation's OpInfo, add the arg to the OpInfo's `allow_cow_input_materialize_backward` list, or change the implementation to avoid materialization.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/281. The reporter of the issue is fengyuan14, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "guangyey", "resolution": "The issue is closed.\n", "root_cause": "The reporter of the issue is fengyuan14, and the assignee is guangyey, and the state of the issue is closed.", "state": "closed"}




### Merged Result:275{"issue_number": 275, "issue_description": "Evaluated Support of Quantization // quantized op issues in test_shape_ops_xpu.py\nquantized op, low priority", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "Could not run 'aten::empty_quantized' with arguments from the 'QuantizedXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty_quantized' is only available for these backends: [XPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].", "state": "open"}


### Merged Result:271{"issue_number": 271, "issue_description": "gradchecker failed: first compare with cpu, if align with cpu it is low priority.\nEvaluated Issues in test_ops_gradients.py\nEvaluated Issues in test_ops_gradients.py\nFixed some issues in index_put, https://github.com/intel/torch-xpu-ops/pull/514", "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": "\nThe issue has been closed\nThe issue is closed.\nFixed", "root_cause": "Double and complex datatype matmul is not supported in oneDNN", "state": "closed"}


### Merged Result:267{"issue_number": 267, "issue_description": "Log:\n```python\nFile \"/home/gta/penghuic/pytorch_stock/third_party/torch-xpu-ops/test/xpu/../../../../test/test_content_store.py\", line 34, in test_basic\n    writer.write_tensor(\"x\", x)\nFile \"/home/gta/penghuic/pytorch_stock/torch/utils/_content_store.py\", line 183, in write_tensor\n    h = self.write_storage(storage)\nFile \"/home/gta/penghuic/pytorch_stock/torch/utils/_content_store.py\", line 165, in write_storage\n    torch.save(storage, target)\nFile \"/home/gta/penghuic/pytorch_stock/torch/serialization.py\", line 670, in save\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\nFile \"/home/gta/penghuic/pytorch_stock/torch/serialization.py\", line 882, in _save\n    pickler.dump(obj)\nFile \"/home/gta/penghuic/pytorch_stock/torch/serialization.py\", line 867, in persistent_id\n    location = location_tag(storage)\nFile \"/home/gta/penghuic/pytorch_stock/torch/serialization.py\", line 426, in location_tag\n    raise RuntimeError(\"don't know how to determine data location of \")\nRuntimeError: don't know how to determine data location of torch.storage.UntypedStorage\nCases:\n```python\ntest_content_store_xpu.py::TestContentStoreXPU::test_basic_xpu\ntest_content_store_xpu.py::TestContentStoreXPU::test_load_tensor_xpu\nCommand:\n```bash\nPYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test/xpu/test_content_store_xpu.py\n```\nThere is no failure involving existing XPU ops.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "\nMove to 2.5", "root_cause": "The issue is caused by the fact that the torch.storage.UntypedStorage is not supported by the current implementation of torch.save function. The reporter of the issue is PenghuiCheng, and the assignee is PenghuiCheng, and the state of the issue is closed.", "state": "closed"}


### Merged Result:264{"issue_number": 264, "issue_description": "torch.random.fork_rng(devices=rng_device) does not support XPU backend. TestRandomTensorCreationXPU.test_randperm_xpu PASSED. torch.xpu.FloatTensor is not supported. so tests like test_tensor_factory_gpu_type_inference and test_constructor_dtype had to be disabled. - PASSED. multiple device seems not supported, when I used ZE_AFFINITY_MASK to use tile 0 and tile 1, seems still only xpu:0 is detected. - PASSED. RuntimeError: \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/264. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/264. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n\n", "root_cause": "test_kaiser_window_xpu: DispatchStub support. It is a composite operator. But it is implemented by DispatchStub. XPU doesn't support DispatchStub.", "state": "open"}


### Merged Result:262{"issue_number": 262, "issue_description": "Provide a way to debug explicit CPU fallback. The commit 5bf9e0cc768f7a3b13d829118683275f324399f1 muted debug logs of 'explicit' CPU fallbacks. This complicated debug for 3d party contributors trying to evaluate XPU backend capabilities - now I am forced to revert noted commit to understand which operations are not currently implemented by XPU. Please:\n1. Explain what 'explicit CPU fallback' means - this seems to be internal to xpu team classification which is unclear and confusing\n2. Extend PYTORCH_DEBUG_XPU_FALLBACK=1 to track any CPU fallback happening in XPU backend. Note: I am fine if 'explicit' fallback will be muted by default, but I really need a way to be able to track it.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/262. The reporter of the issue is dvrogozh, and the assignee is , and the state of the issue is closed.", "reporter": "dvrogozh", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:261{"issue_number": 261, "issue_description": "Implement aten::_pin_memory for XPU device\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/261. The reporter of the issue is dvrogozh, and the assignee is , and the state of the issue is closed.", "reporter": "dvrogozh", "assignee": "", "resolution": "\nDone after merging https://github.com/pytorch/pytorch/pull/129353", "root_cause": "", "state": "closed"}


### Merged Result:259{"issue_number": 259, "issue_description": "Accuracy issue in TestDropoutNNDeviceTypeXPU.test_Dropout1d_xpu and TestDropoutNNDeviceTypeXPU.test_Dropout3d_xpu\nThe freeze_rng_state function does not work on xpu.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "closed\nThe freeze_rng_state function does not work on xpu.", "root_cause": "freeze_rng_state does not work on xpu.", "state": "closed"}


### Merged Result:258{"issue_number": 258, "issue_description": "oneDNN issues // Please check `run_test_with_skip.py` CPU fallback failures // Need to evaluate when we have XPU implementation aten::_thnn_fused_gru_cell // CPU fallback could not cover it\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/258. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:256{"issue_number": 256, "issue_description": "test_cpu_gpu_parity_nn_CrossEntropyLoss_xpu_float16 // Should be open when nll_loss2d is enabled.\noneDNN failures // Please check `run_test_with_skip`\nLack of aten::_thnn_fused_gru_cell // Need evaluate whether operator is needed.\nCPU fallback failures // To evaluate when there is an XPU implementation.\ntest_cpu_gpu_parity_nn_CrossEntropyLoss_xpu_float16 // Should be open when nll_loss2d is enabled.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:254{"issue_number": 254, "issue_description": "TestMathBitsXPU , totally 200 cases got RuntimeError: Double and complex datatype matmul is not supported in oneDNN\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/254. The reporter of the issue is daisyden, and the assignee is riverliuintel, and the state of the issue is closed.\nTracked in #253", "reporter": "daisyden", "assignee": "riverliuintel", "resolution": "\nclosed\ntesting", "root_cause": "OneDNN does not support double and complex datatype matmul operations.", "state": "closed"}




### Merged Result:249{"issue_number": 249, "issue_description": "TestMathBitsXPU issues:\n1. TestMathBitsXPU , RuntimeError: DispatchStub: unsupported device type expu - Fixed\n2. TestMathBitsXPU, RuntimeError: input tensor must have at least one element, but got input_sizes = [1, 0, 1] - 2.6\n3. accuracy issue\n4. RuntimeError: value cannot be converted to type float without overflow\nThe issue is related to the conversion of values to float without overflow, the unsupported matmul operation for double and complex data types in oneDNN, and the failure to create a primitive descriptor for a deconvolution forward propagation primitive.", "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": "Fixed\nThe issue has been closed by the assignee.", "root_cause": "In aten/src/ATen/native/DispatchStub.cpp function DispatchStubImpl::get_call_ptr() need to add a XPU path.", "state": "closed"}


### Merged Result:248{"issue_number": 248, "issue_description": "TestMathBitsXPU has 2 cases with RuntimeError: value cannot be converted to type float without overflow\n\n\"test_conj_view_addbmm_xpu_complex64\",\n\"test_neg_conj_view_addbmm_xpu_complex128\",\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/248. The reporter of the issue is daisyden, and the assignee is , and the state of the issue is closed.", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:246{"issue_number": 246, "issue_description": "1. RuntimeError: PyTorch was compiled without CUDA support----fixed cases: TestAutograd.test_checkpointing_non_reentrant_autocast_gpu\n2. module 'torch._C' has no attribute '_scatter' cases: TestAutograd.test_checkpointing_without_reentrant_dataparallel, TestMultithreadAutograd.test_dataparallel_saved_tensors_hooks\n3. AttributeError: module 'torch.xpu' has no attribute cases: TestAutograd.test_graph_save_on_cpu_cuda, TestAutograd.test_checkpointing_without_reentrant_memory_savings, TestAutogradDeviceTypeXPU.test_pin_memory_xpu\n4. NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend. cases: test_sparse_mask_autograd_xpu, test_sparse_ctor_getter_backward_xpu_float64, test_sparse_ctor_getter_backward_xpu_complex128, test_sparse_backward_xpu_float64, test_sparse_backward_xpu_complex128\n5. c10::NotImplementedError cases: TestAutogradMultipleDispatchXPU::test_autograd_composite_implicit_and_dispatch_registration_xpu, TestAutogradMultipleDispatchXPU::test_autograd_multiple_dispatch_registrations_xpu\n6. segment fault cases: TestAutograd::test_custom_function_cycle, TestAutograd::test_custom_function_forward_mode_wrong_formula, TestAutograd::test_custom_function_non_tensor_inputs_outputs, TestAutograd::test_custom_function_exception, TestAutograd::test_custom_function_save_for_forward, TestAutograd::test_custom_function_saved_tensors, TestAutograd::test_custom_function_setup_context_multi_input, TestAutograd::test_custom_function_setup_context_multi_output, TestAutogradDeviceTypeXPU::test_resize_version_bump_xpu, TestAutogradDeviceTypeXPU::test_resize_version_bump\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/246. The reporter of the issue is PenghuiCheng, and the assignee is PenghuiCheng, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "\nEvaluated. There is no failure exposing issues of existing XPU ops. Move to 2.5.", "root_cause": "", "state": "closed"}


### Merged Result:245{"issue_number": 245, "issue_description": "module 'torch._C' has no attribute '_scatter'", "reporter": "PenghuiCheng", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:244{"issue_number": 244, "issue_description": "Log:\nAttributeError: module 'torch.xpu' has no attribute\nCases:\nTestAutograd.test_graph_save_on_cpu_cuda,\nTestAutogradDeviceTypeXPU.test_pin_memory_xpu\nCommand:\nPYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test/xpu/test_autograd_xpu.py\n", "reporter": "PenghuiCheng", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:242{"issue_number": 242, "issue_description": "TestCompositeComplianceXPU.test_view_replay_to_sparse_xpu_float32 is failing on XPU backend. The test is expected to pass, but it is failing with the error message: 'RuntimeError: XPU backend does not support sparse op'.\n3 cases in TestMathBitsXPU have sparse op issue", "reporter": "daisyden", "assignee": "", "resolution": "\nThe issue is tracked in #240", "root_cause": "The operator 'aten::_sparse_coo_tensor_with_dims_and_tensors' is not available for the 'SparseXPU' backend", "state": "closed"}


### Merged Result:241{"issue_number": 241, "issue_description": "TestCompositeComplianceXPU.test_view_replay_nn_functional_conv_transpose2d_xpu_float32 and TestCompositeComplianceXPU.test_view_replay_nn_functional_group_norm_xpu_float32 are failing with RuntimeError in nn_functional* ops op creation. The reporter of the issue is daisyden, and the assignee is . The state of the issue is closed.\nThe first issue is tracked in #253 \nThe 2nd issue is tracked in #249", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:240{"issue_number": 240, "issue_description": "1. AssertionError: Jiterator is only supported on CUDA and ROCm GPUs, none are available. TestCompositeComplianceXPU.test_view_replay_jiterator_unary_xpu_float32\n2. More than 400 cases in TestCompositeComplianceXPU.test_cow_input got these errors:\n3. sparse is not supported\nTestCompositeComplianceXPU.test_view_replay_to_sparse_xpu_float32\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/240. The reporter of the issue is daisyden, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nEnable the suite in fine gran cases. Evaluated.", "root_cause": "", "state": "closed"}


### Merged Result:239{"issue_number": 239, "issue_description": "TestCompositeComplianceXPU.test_view_replay_addbmm_xpu_float32, TestCompositeComplianceXPU.test_view_replay_addmm_xpu_float32, TestCompositeComplianceXPU.test_view_replay_addmv_xpu_float32 cannot create primitive\ntracked in #253", "reporter": "daisyden", "assignee": "", "resolution": "\ntracked in #253", "root_cause": "", "state": "closed"}


### Merged Result:238{"issue_number": 238, "issue_description": "More than 400 cases in TestCompositeComplianceXPU.test_cow_input got these errors:\n\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/3e3a7496-cb09-436d-85d5-67e4e5efbd83)\n\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/59a30297-919d-4198-81b4-c6cdf6ec2402)\n\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/1216855f-a2c7-429d-bebf-51213bea8ee7)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/238. The reporter of the issue is daisyden, and the assignee is , and the state of the issue is closed.", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:237{"issue_number": 237, "issue_description": "RuntimeError: could not create a primitive descriptor for a deconvolution forward propagation primitive, timeout 10000 python run_test_with_skip.py 2>&1|tee TestCompositeCompliance_test_forward_ad.log\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/237. The reporter of the issue is daisyden, and the assignee is , and the state of the issue is closed.", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:236{"issue_number": 236, "issue_description": "TestCompositeComplianceXPU.test_forward_ad_nn_functional_rrelu_xpu_float32 and TestCompositeComplianceXPU.test_forward_ad_nn_functional_max_unpool1d_xpu_float32 have accuracy issues.\nTracked in #233", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:235{"issue_number": 235, "issue_description": "timeout 10000 python run_test_with_skip.py 2>&1|tee TestCompositeCompliance_test_forward_ad.log\n\nTestCompositeComplianceXPU.test_forward_ad_addmm_xpu_float32\nTestCompositeComplianceXPU.test_forward_ad_addbmm_xpu_float32\nTestCompositeComplianceXPU.test_forward_ad_addmv_xpu_float32\n\nRuntimeError: could not create a primitive in test_forward_ad\nTracked in #253", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:234{"issue_number": 234, "issue_description": "Support SparseXPU backend for 'aten::_sparse_coo_tensor_with_dims_and_tensors' cases: test_sparse_mask_autograd_xpu, test_sparse_ctor_getter_backward_xpu_float64, test_sparse_ctor_getter_backward_xpu_complex128, test_sparse_backward_xpu_float64, test_sparse_backward_xpu_complex128. The reporter of the issue is PenghuiCheng, and the assignee is , and the state of the issue is closed.\nThe reporter of the issue is PenghuiCheng, and the assignee is , and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "", "resolution": "\n", "root_cause": "According to our priority, before PyTorch 2.5, we will support Sparse operators on-demand. If the operator is required by our prioritized operator list (3 benchmarks + MPS), we will implement it, or will deprioritized it. We can skip it in unit test first.", "state": "closed"}




### Merged Result:232{"issue_number": 232, "issue_description": "PYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test_ops_xpu.py -k 'test_backward_diagonal_xpu_float32'\nThe reporter of the issue is daisyden, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nEnable cases in fine gran cases. There is no segfault there with bugfixing. Close the issue.", "root_cause": "In current XPU implementation, CPU fallback will copy the XPU tensor with an invalid address to CPU, which causes the segfault. The bugfixing is to support these view operators on XPU.", "state": "closed"}


### Merged Result:231{"issue_number": 231, "issue_description": "c10::NotImplementedError in test case: TestAutogradMultipleDispatchXPU::test_autograd_composite_implicit_and_dispatch_registration_xpu, TestAutogradMultipleDispatchXPU::test_autograd_multiple_dispatch_registrations_xpu. The error occurs when running the test case on xpu device. The error message is: 'c10::NotImplementedError: Not implemented for the current device type: xpu'. The root cause is that the current implementation of the test case does not support xpu device. The resolution is to add support for xpu device in the test case.", "reporter": "PenghuiCheng", "assignee": "fengyuan14", "resolution": "Add support for xpu device in the test case.", "root_cause": "The current implementation of the test case does not support xpu device.", "state": "closed"}


### Merged Result:230{"issue_number": 230, "issue_description": "segment fault for UT case TestAutogradDeviceTypeXPU::test_resize_version_bump_xpu", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:229{"issue_number": 229, "issue_description": "RuntimeError: NULL pointer argument in memory copy operation. -30 (PI_ERROR_INVALID_VALUE)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/229. The reporter of the issue is daisyden, and the assignee is , and the state of the issue is closed.", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:228{"issue_number": 228, "issue_description": "NotImplementedError: elapsed_time is not supported by XPUEvent\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/228. The reporter of the issue is etaf, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "etaf", "assignee": "guangyey", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:227{"issue_number": 227, "issue_description": "UT case <test_comprehensive_nn_functional_nll_loss_xpu_float16> fail because of cpu's nll_loss2d backward. We should try this ut when we implement xpu nll_loss2d op.\nThe issue is accuracy issue. The case test_comprehensive_nn_functional_nll_loss_xpu_float16 failed in calling nll_loss2d_backward. xpu result is: 0.04122925, cuda and cpu is: 0.04119873, atol is 1e-7. We have two findings: 1) the atol of nll_loss2d_forward is just 1e-2 as below: (torch.float16, torch.ops.aten.nll_loss2d_forward.default): 1e-2, so we think the atol of backward is too high, maybe we can enlarge the atol value of nll_loss2d_backward like nll_loss2d_forward. 2) If we use compile option O0 instead of O3, xpu and cuda can the same precision, that is 0.04119873. We know that there are some optimization bugs when the xpu compiler processes +-*/ of float16 and bfloat16, I tried to use volatile in nll_loss2d_backward kernel, but it had no effect and I still could not get the same precision as cuda.", "reporter": "chunhuanMeng", "assignee": "huaiyuzh", "resolution": "closed\n", "root_cause": "The issue is related to the backward pass of nll_loss2d on CPU. The fix is to ensure that the backward pass on CPU is consistent with the backward pass on XPU.", "state": "closed"}


### Merged Result:223{"issue_number": 223, "issue_description": "test/xpu/test_autocast_xpu.py::TestAutocastGPU::test_cache_disabled FAILED [ 20%]\n  File \"/home/gta/penghuic/pytorch_stock/third_party/torch-xpu-ops/test/xpu/test_autocast_xpu.py\", line 90, in test_cache_disabled\n    torch._C._set_cached_tensors_enabled(False)\nAttributeError: module 'torch._C' has no attribute '_set_cached_tensors_enabled'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/223. The reporter of the issue is PenghuiCheng, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "\nThe issue is closed.", "root_cause": "The issue is closed.", "state": "closed"}


### Merged Result:222{"issue_number": 222, "issue_description": "There are three types of failures in the test_reductions_xpu.py test suite: 1. There is an error in the results: skipped on cuda device, so we also skipped it. 2. RuntimeError: mode only supports CPU AND CUDA device type, got: xpu. 3. largeTensorTest will raise an \nThe reporter of the issue is PenghuiCheng, and the assignee is PenghuiCheng, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "\n", "root_cause": "The error of test_ref_extremal_values_mean_xpu_complex64 is due to the different treatment of nan and inf by xpu and cpu. xpu treats nan and inf in the same way as cuda, which is why cuda skips this test case. The error of test_ref_small_input_masked_prod_xpu_float16 is due to the accumulation of errors caused by cumulative multiplication. The accuracy requirement for this sample in cuda's test is lower, so xpu also reduces the accuracy requirement.", "state": "closed"}


### Merged Result:221{"issue_number": 221, "issue_description": "enable mode supports XPU device", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:220{"issue_number": 220, "issue_description": "We added this unit test for XPU device as below code, but the test `TestIndexingXPU` will raise a core dump\n`torch-xpu-ops/src/aten/sycl/Indexing.h:615: operator(): global id: [19,0,0], local id: [19,0,0] Assertion \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/220. The reporter of the issue is yuchengliu1, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "daisyden", "resolution": "\nclosed", "root_cause": "There are three issues in test_index: 1. Process abort due to kernel assertion. Kernel assertion is expected. These failed cases are designed for CPU backend, where it is a host exception, if passing an invalid index. We are same as CUDA. CUDA skips these kernels. 2. CUDA bias case. Need CUDA built. 3. Lack of meta process in index_put deterministic implementation. The meta process is new added in stock PyTorch. We missed the rebase during porting. Fixed.", "state": "closed"}


### Merged Result:213{"issue_number": 213, "issue_description": "Some operators fail in test_ops::TestCommonXPU::test_dtypes, since there is no XPU claimed data type in test_ops infrastructure. We will enhance test infrastructure to add XPU specific claimed data type.\nextended UT is added", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "closed\nextended UT is added", "root_cause": "No XPU claimed data type in test_ops infrastructure.", "state": "closed"}


### Merged Result:211{"issue_number": 211, "issue_description": "We added the unit test for XPU device as below code, but the function of  largeTensorTest will raise an \"unknow device type\" error.", "reporter": "PenghuiCheng", "assignee": "huaiyuzh", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:210{"issue_number": 210, "issue_description": "IPEX supports ChannelsLast1D. It was a requirement of KPI models before. According to staging goal of upstreaming, give it low priority.\nWon't support it in PyTorch.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed\nWon't support it in PyTorch.", "root_cause": "", "state": "closed"}


### Merged Result:208{"issue_number": 208, "issue_description": "Some utility functions of ATen operator level are operator semantics related and could be shared among different backends. In existing implementation, some of them are implemented in cpp file, could not be reused. So we will review existing torch-xpu-ops implementation, find them out and submit a PR to stock PyTorch to make them shared among like CPU, CUDA and XPU.\nIt's a long term task. We have got some unification between CUDA and XPU in other components, like runtime and Inductor. We are thinking about how to unified with CUDA implementation from ATen operator perspective.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:207{"issue_number": 207, "issue_description": "To support operator specific operator torch.isin. CPU fallback cannot cover it. See, https://github.com/intel/torch-xpu-ops/issues/206", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed", "root_cause": "Fail test_sort_and_select::test_isin_different_devices_xpu_float32 due to backend specific operator torch.isin.", "state": "closed"}


### Merged Result:206{"issue_number": 206, "issue_description": "Record limitations of CPU fallback during development. These will be the reference/check-list when we get a bug of CPU fallback in future.\n1. View/Tensor meta modification operators cannot be fallback to CPU. Breaking semantics. Requiring change Tensor metas inplace.\n3. Tensor factory operators cannot be fallback to CPU. Backend specific implementation.\n4. RNG operators cannot be fallback to CPU. Backend specific implementation.\n5. Fallback of compound operators don't work. The priority of compound operator dispatch is higher than fallback.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/206. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nClose it due to we plan to implement full op coverage", "root_cause": "", "state": "closed"}


### Merged Result:198{"issue_number": 198, "issue_description": "1. Find the most suitable parameter CAT_ARRAY_BATCH_SIZE.\n2. Compare performance with IPEX implementation\nDuplicated. Closed", "reporter": "xytintel", "assignee": "", "resolution": "\nDuplicated", "root_cause": "", "state": "closed"}


### Merged Result:197{"issue_number": 197, "issue_description": "As to staging goal of PyTorch 2.5, we collect 484 operators which are required working with XPU backend. Part of them are required XPU specific implementation.\nWhen we give XPU implementation for an ATen operator, we need register all variants of the operator, like xxx.out, xxx.Tensor, xxx.Scalar, xxx_ and so on.\nFollowing the rule,\n1. We won't take additional efforts to be back for lack of registration in future and complement them. Adding variants at the moment is cheap.\n2. When we align with CUDA registration, in-tree would be seamless.\n\n- [x] random_.to\n- [x] clamp.Tensor_out\n- [x] clamp_min.Tensor_out\n- [x] clamp_max.Tensor_out\n- [x] fmod.Scalar // remove\n- [x] fmod_.Scalar // remove\n- [x] index_add_\n- [x] index_add\n- [x] remainder.Scalar_out // remove\n- [x] remainder.Scalar // remove\n- [x] remainder_.Scalar // remove\n- [x] rsub.Scalar // remove\n- [x] rsub.Scalar_out // remove\n- [x] rsub.Tensor_out // remove\n- [x] sub.Scalar // remove\n- [x] sub_.Scalar // remove\n- [x] sub.Scalar_out // remove\n- [x] sum.out // remove\n- [x] sum.dim_IntList // remove, \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/197. The reporter of the issue is fengyuan14, and the assignee is chunhuanMeng, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "chunhuanMeng", "resolution": "\nwe can close this issue", "root_cause": "", "state": "closed"}


### Merged Result:195{"issue_number": 195, "issue_description": "6 models got failed in huggingface float16 inference accuracy test. Error info: Run failed with return code: -11, Output: None, Error: None. Summary: 40 models tested, 39 passed, 1 failed. 40 models tested, 40 passed, 0 failed.\nThe issue is related to a crash on fp16 due to a segment fault in the triton backend. The crash happens in the triton backend and is related to the input tensor being `divisible_by_16`. The root cause is that the input tensor is not divisible by 16, which leads to a segment fault. The fix for this issue is to make the Inductor generator hint for triton that the input tensor is `divisible_by_16`, so that triton can avoid the segment fault path. The PR https://github.com/pytorch/pytorch/pull/126261 will make Inductor generator hint for triton that input tensor is `divisible_by_16`, so that triton can avoid the segmentfault path. The fix will be in the next rolling driver and next LTS driver. The PR has been landed and the fix is expected to be verified.", "reporter": "mengfei25", "assignee": "etaf", "resolution": "\nThe fix for the issue is to make the Inductor generator hint for triton that input tensor is `divisible_by_16`, so that triton can avoid the segment fault path. The PR https://github.com/pytorch/pytorch/pull/126261 will make Inductor generator hint for triton that input tensor is `divisible_by_16`, so that triton can avoid the segmentfault path. The fix will be in the next rolling driver and next LTS driver. The PR has been landed and the fix is expected to be verified.", "root_cause": "The input tensor is not divisible by 16, which leads to a segment fault in the triton backend.", "state": "closed"}


### Merged Result:184{"issue_number": 184, "issue_description": "To Evaluate accuracy issue in fine-grained test, the test_compare_cpu tanh complex support has accuracy gap, bfloat16 accuracy gap in rsqrt, sub, rounding, cumsum, add, rsub, float16 cumsum accuracy gap, pow, mul, log, complex64 got nan, index_put, index_add with bool, rounding float36 got inf, XPU reports 'not implemented' with dtype bool, int*, uint8, while CPU would not report such error message.\nThe reporter of the issue is daisyden, and the assignee is huaiyuzh, and the state of the issue is open.", "reporter": "daisyden", "assignee": "huaiyuzh", "resolution": "\n", "root_cause": "The issue is related to the behavior of the tanh function when the input is -inf+nanj. The output of the tanh function on XPU is not aligned with the output on CUDA. The root cause is that the Sycl compiler and the CUDA compiler have different behaviors for this specific input. The expected behavior is that the tanh function should return -1 for -inf+nanj input.", "state": "open"}


### Merged Result:171{"issue_number": 171, "issue_description": "The Inducor UT passed on CPU and CUDA, but fail on XPU with error: RuntimeError: \"div_true_xpu\" not implemented for 'Long'\nfixed", "reporter": "etaf", "assignee": "fengyuan14", "resolution": "\nfixed", "root_cause": "", "state": "closed"}


### Merged Result:166{"issue_number": 166, "issue_description": "Since we use different ATen dispatch stub code gen script, we have to add checks manually to align with stock CUDA behavior. So far, these checks are not critical,\n1. Common device check. Informative error at runtime.\n2. Device guard. We set device guard on demand at kernel level.\nAccording to the latest evaluation, we don't need it", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nAccording to the latest evaluation, we don't need it", "root_cause": "", "state": "closed"}


### Merged Result:165{"issue_number": 165, "issue_description": "Most of e2e tests got failed with stock pytorch + related PR, there are most tests got failed after change pytorch from private to stock + PR.============ Summary for huggingface float32 inference accuracy ============num_total: 46num_passed: 13num_failed: 33pass_rate: 28.26%\nfixed", "reporter": "mengfei25", "assignee": "", "resolution": "\nfixed", "root_cause": "https://github.com/intel/torch-xpu-ops/pull/164/files/50fa410781382d991614281771ebcfcba40b67a7..c755ce75cfc1f2fb53b2643ab3c13e4c064e0d70", "state": "closed"}


### Merged Result:163{"issue_number": 163, "issue_description": "The reporter of the issue is etaf, the assignee is guangyey, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/163. The reporter of the issue is etaf, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "etaf", "assignee": "guangyey", "resolution": "closed\nclosed", "root_cause": "depends on device allocator unification. WIP...", "state": "closed"}


### Merged Result:162{"issue_number": 162, "issue_description": "For some priority gaps, we implement some operators with explicit CPU fallback. We will add XPU implementation according to priority requirements.\n\n- [ ] nonzero  // update xpu pin 2.5\n- [x] tril  // SDP math\n- [x] softmax  // SDP math\n- [x] softmax_backward  // SDP math\n- [ ] fft  // MKL 2.5\nThe reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nHF .compile and HF eager requirements are done. Nonzero and fft will be tracked by following operators development task.", "root_cause": "", "state": "closed"}


### Merged Result:157{"issue_number": 157, "issue_description": "A case fail due to oneDNN matmul implementation, Skip the case temporarily.\ntest_dtypes_nn_functional_multi_head_attention_forward_xpu, test_dtypes_nn_functional_linear_xpu, test_dtypes_pca_lowrank_xpu, test_dtypes_svd_lowrank_xpu, test_noncontiguous_samples_nn_functional_linear_xpu_int64, test_dtypes__refs_nn_functional_pdist_xpu", "reporter": "fengyuan14", "assignee": "PenghuiCheng", "resolution": "\n", "root_cause": "Access violation on Windows LNL pytorch UT test_dtypes_nn_functional_linear_xpu", "state": "open"}


### Merged Result:156{"issue_number": 156, "issue_description": "test_ops.py::TestCommonXPU::test_dtypes_nn_functional_scaled_dot_product_attention_xpu FAILED\n![image](https://github.com/intel/torch-xpu-ops/assets/51150101/a7f7842a-79a0-4f1e-85a5-177d1c9d6ddd)\nSkip this case to WA this issue.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/156. The reporter of the issue is AlienLiang23, and the assignee is ZhiweiYan-96, and the state of the issue is closed.", "reporter": "AlienLiang23", "assignee": "ZhiweiYan-96", "resolution": "\nclosed", "root_cause": "", "state": "closed"}


### Merged Result:155{"issue_number": 155, "issue_description": "import torch\ndevice = \"xpu\"\nself_cpu = torch.ones(1, dtype=torch.bool)\nself_xpu = self_cpu.to(device)\n\nsrc_cpu = torch.zeros(1, dtype=torch.bool)\nsrc_xpu = src_cpu.to(device)\n\n\ndim = 0\n\nindex_cpu = torch.zeros(1, dtype=torch.int64)\nindex_xpu = index_cpu.to(device)\n\nout_xpu = torch.index_add(self_xpu, dim, index_xpu, src_xpu)\nout_cpu = torch.index_add(self_cpu, dim, index_cpu, src_cpu)\n\nprint(\"out_xpu:\", out_xpu)\nprint(\"out_cpu:\", out_cpu)\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/155. The reporter of the issue is etaf, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "etaf", "assignee": "fengyuan14", "resolution": "\nFixed.", "root_cause": "", "state": "closed"}


### Merged Result:151{"issue_number": 151, "issue_description": "The latest nightly test HF FP32 training accuracy test failed on eager_two_runs_differ, there are a lot of models accuracy failed with `eager_two_runs_differ`. The summary for huggingface float32 training accuracy is as follows: num_total: 46, num_passed: 5, num_failed: 41, pass_rate: 10.87%.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/151. The reporter of the issue is chuanqi129, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "chuanqi129", "assignee": "fengyuan14", "resolution": "\nFixing after rebasing RNG kernels.", "root_cause": "Suspected guilty commit: **e7141bd66e30ac9620924168149c2ffc11c0c6d9**", "state": "closed"}


### Merged Result:149{"issue_number": 149, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/149. The reporter of the issue is etaf, and the assignee is guangyey, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/149. The reporter of the issue is etaf, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "etaf", "assignee": "guangyey", "resolution": "closed\n", "root_cause": "https://jira.devtools.intel.com/browse/URLZA-203", "state": "closed"}


### Merged Result:148{"issue_number": 148, "issue_description": "The feature, motivation and pitch: The reporter @EikanWang  can you help to assign to correct developer? Alternatives: No response Additional context: No response", "reporter": "EikanWang", "assignee": "guangyey", "resolution": "closed", "root_cause": "", "state": "closed"}


### Merged Result:146{"issue_number": 146, "issue_description": "The issue is about performance evaluation. A common performance tuning is to trad-off register usage and concurrency. From SYCL compiler log, we find some warnings about register spill. Need evaluation\nIt is the last mile of performance. When we start to pursue peak performance, we should deep dive for it. So far, it is a low priority task.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "open"}


### Merged Result:144{"issue_number": 144, "issue_description": "The feature, motivation and pitch\n\nWe need to support enough ATen operators for XPU backend to meet the requirement of PyTorch test infrastructure. The issue records updating ATen operators list we need support.\ntest_xpu.py::TestXpuXPU::test_compare_cpu_add_xpu_float32,\n\n```NotImplementedError: The operator 'aten::_local_scalar_dense' is not currently implemented for the XPU device. If you want this\nop to be added in priority during the prototype phase of this feature, please open issue on https://github.com/intel/torch-xpu-ops/issues. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on XPU.\n```\nextended cases are added", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nextended cases are added", "root_cause": "", "state": "closed"}


### Merged Result:135{"issue_number": 135, "issue_description": "We followed stock CUDA about grid and block configurations. For these configurations, stock CUDA has some NV GPU arch assumption. Even we followed similar configurations, we are not clear about what we can get from Xe arch.\n\n1. syclMaxWorkItemsPerEU: What we can get from Xe arch, when using it.\n2. syclMaxWorkItemsPerTile: We are using max sub-group size to deduce max number of work items per Tile. It is not accurate. When runtime chooses non-max-sub-group-size kernel (IGC's optimization), we might get insufficient occupancy.\n\nhttps://github.com/intel/torch-xpu-ops/blob/e914ada988343c0515753360de68812ea42d0ec3/src/aten/sycl/Loops.h#L330\n```int wg_sz = syclMaxWorkItemsPerEU();\nint num_wg = ceil_div<int>(N, wg_sz);\nint hw_max_num_wg = syclMaxWorkItemsPerTile() / wg_sz;\nnum_wg = num_wg > hw_max_num_wg ? hw_max_num_wg : num_wg;\nsycl_kernel_submit(wg_sz * num_wg, wg_sz, getCurrentSYCLQueue(), ker);```\nIt assumes explicit scaling GPU resources when using `syclMaxWorkItemsPerTile`. Or we should consider all resources of a device.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "\n", "root_cause": "It assumes explicit scaling GPU resources when using `syclMaxWorkItemsPerTile`. Or we should consider all resources of a device.", "state": "open"}


### Merged Result:128{"issue_number": 128, "issue_description": "test_noncontiguous_samples_native_dropout_backward_xpu_int64: CUDA fails with same error, IPEX has no such case, will skip. (RuntimeError: \"masked_scale\" not implemented for 'Long')\ntest_non_standard_bool_values_native_dropout_backward_xpu_bool: CUDA fails with same error, IPEX has no such case, will skip. (RuntimeError: \"masked_scale\" not implemented for 'Bool')\ntest_compare_cpu_nn_functional_alpha_dropout_xpu_float32: CUDA xfail, IPEX skips it, will skip.\ntest_dtypes_native_dropout_backward_xpu: Porting difference between IPEX and torch-xpu-ops, will skip (\"masked_scale\" does not claim int support)\ntest_dtypes_nn_functional_linear_xpu: https://github.com/intel/torch-xpu-ops/issues/157\ntest_dtypes_nn_functional_multi_head_attention_forward_xpu: https://github.com/intel/torch-xpu-ops/issues/157\ntest_dtypes_pca_lowrank_xpu: https://github.com/intel/torch-xpu-ops/issues/157\ntest_dtypes_svd_lowrank_xpu: https://github.com/intel/torch-xpu-ops/issues/157\ntest_noncontiguous_samples_nn_functional_linear_xpu_int64: https://github.com/intel/torch-xpu-ops/issues/157\n285 failures due to https://github.com/intel/torch-xpu-ops/issues/157\nmove oneDNN issue to #253", "reporter": "fengyuan14", "assignee": "ZhiweiYan-96", "resolution": "\n", "root_cause": "", "state": "closed"}


### Merged Result:126{"issue_number": 126, "issue_description": "We have some kernels not aligned with stock CUDA implementation, since,\n1. Functionality extension was added in stock CUDA implementation. But we have no sustainable rebase.\n2. General memory layout support was added in stock CUDA implementation. But we have no sustainable rebase.\n3. We have specific implementation for performance in some cases. But stock CUDA don't care these cases.\n\n1 is functionality related and 2 and 3 are performance related.\n- For Type-1, we should fix and aligned with stock CUDA during porting from IPEX to torch-xpu-ops.\n- For Type-2, we should align with CUDA implementation with proper priority.\n- For Type-3, we need to trad-off performance and feasibility of in-tree.\n\nHere is the list. We will add items gradually when op is ported.\n- [x] aten::bernoulli_ // Type-2\n- [ ] aten::cumsum // Type-3\n- [ ] atem::cat @xytintel // Type-3\n- [ ] aten::tril/triu @AlienLiang23 // Type-2 CUDA optimization commit: 1462d72904cb81917b9355d6a58916f389e9084c\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/126. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is open.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "It is a long term task to remainder us evaluate impacts when we have diff implementation as CUDA.", "state": "open"}


### Merged Result:125{"issue_number": 125, "issue_description": "HostCachingAlloctor provides pin memory for H2D and D2H copy. Some kernels require the alloctor, E.g. copy, cat... Using system memory + synchronization to ensure functionality. But it will hurt performance.\nWA for Copy.", "reporter": "fengyuan14", "assignee": "", "resolution": "closed\nDone.", "root_cause": "XPU HostCachingAlloctor PR of PyTorch, https://github.com/pytorch/pytorch/pull/123080", "state": "closed"}


### Merged Result:122{"issue_number": 122, "issue_description": "All `hf_clip` accuracy tests crashed with `AttributeError: 'str' object has no attribute 'shape'`\nThese issues also happen on A100 platform, not related to xpu implementation", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClosed", "root_cause": "Align with CUDA", "state": "closed"}


### Merged Result:121{"issue_number": 121, "issue_description": "tacotron2 training accuracy crashed with RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [XPUFloatType [4, 80, 724]], which is output 0 of torch::autograd::CopyBackwards, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the baseline has been refreshed.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "", "state": "closed"}


### Merged Result:120{"issue_number": 120, "issue_description": "dlrm `fp32/bf16/fp16` training accuracy crashed with below error message.\n\n```\nNotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_sparse_coo_tensor_with_dims_and_tensors' is only available for these backends: [XPU, Meta, SparseCPU, SparseMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher]\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/120. The reporter of the issue is chuanqi129, and the assignee is , and the state of the issue is closed.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nduplicate with #484", "root_cause": "", "state": "closed"}


### Merged Result:119{"issue_number": 119, "issue_description": "Torchbench functorch_dp_cifar10 training accuracy crashed with RuntimeError: slow_conv2d: grad_weight must be contiguous\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the problem has been resolved by refreshing the baseline.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nRefreshed baseline", "root_cause": "", "state": "closed"}


### Merged Result:118{"issue_number": 118, "issue_description": "Those detectron2 series models accuracy crash with `AssertionError: get_event_storage() has to be called inside a 'with EventStorage(...)' context!` Precision: fp32/bf16/fp16 Mode: training\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the problem has been resolved by refreshing the baseline.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nRefreshed baseline", "root_cause": "", "state": "closed"}


### Merged Result:117{"issue_number": 117, "issue_description": "Those detectron2 series models accuracy crash with RuntimeError: dets should have the same type as scores\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the problem has been resolved by refreshing the baseline.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nRefreshed baseline", "root_cause": "", "state": "closed"}


### Merged Result:116{"issue_number": 116, "issue_description": "Torchbench has 2 models fp16 training crashed with RuntimeError: \"reflection_pad2d\" not implemented for 'Half'\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the problem has been resolved by refreshing the baseline.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nRefreshed baseline", "root_cause": "", "state": "closed"}


### Merged Result:115{"issue_number": 115, "issue_description": "There are some models crashed on RuntimeError: DispatchStub: unsupported device typexpu. Model: demucs, precision: fp32, mode: training. Model: pytorch_CycleGAN_and_pix2pix, precision: fp32 / bf16, mode: training. Model: pytorch_stargan, precision: fp32 / bf16 / fp16, mode: training.\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the problem has been resolved by refreshing the baseline.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nRefreshed baseline", "root_cause": "", "state": "closed"}


### Merged Result:114{"issue_number": 114, "issue_description": "Below models training crashed with `RuntimeError: element   0 of tensors does not require grad and does not have a grad_fn`\nModel | Precision\n-- | --\ncm3leon_generate | fp32/bf16/fp16\nDALLE2_pytorch | fp32/bf16/fp16\nhf_T5_generate | fp32/bf16/fp16\nmaml | fp32/bf16/fp16\npyhpc_equation_of_state | fp32/bf16/fp16\npyhpc_isoneutral_mixing | fp32/bf16/fp16\nsam | fp32/bf16/fp16\nsam_fast | fp32\nDALLE2_pytorch and sam float16 cuda has same failure message", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "Not specified", "state": "closed"}


### Merged Result:113{"issue_number": 113, "issue_description": "Below models eager_two_runs_differ\n\n| Model | Precision | Mode |\n| -- | -- | -- |\n| hf_BigBird | fp32 | inference |\n| sam | bf16 | inference |\n| sam | fp16 | inference |\n\n\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the baseline has been refreshed.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "", "state": "closed"}


### Merged Result:112{"issue_number": 112, "issue_description": "moco crashed with below message, cuda can pass\n\nValueError: Default process group has not been initialized, please make sure to call init_process_group.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/112. The reporter of the issue is chuanqi129, and the assignee is , and the state of the issue is closed.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "The reporter of the issue is chuanqi129, and the assignee is , and the state of the issue is closed.", "state": "closed"}


### Merged Result:111{"issue_number": 111, "issue_description": "There are some models training crashed on `NotImplementedError:(\nThese issues also happen on A100 platform, not related to xpu implementation", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "Not related to xpu implementation", "state": "closed"}


### Merged Result:110{"issue_number": 110, "issue_description": "Torchbench has some models failed on accuracy check, the detail model list can be found as below table.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/110. The reporter of the issue is chuanqi129, and the assignee is etaf, and the state of the issue is closed.", "reporter": "chuanqi129", "assignee": "etaf", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "", "state": "closed"}


### Merged Result:109{"issue_number": 109, "issue_description": "Timm_models has some models failed on accuracy check, the detail model list can be found as below table.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/109. The reporter of the issue is chuanqi129, and the assignee is etaf, and the state of the issue is closed.", "reporter": "chuanqi129", "assignee": "etaf", "resolution": "\nclosed", "root_cause": "", "state": "closed"}


### Merged Result:88{"issue_number": 88, "issue_description": "The following script will get error \"le_xpu not implemented for 'ComplexFloat'\". This is a fake error message.\n```import torch\na = torch.tensor([3.+3.j], device=\"xpu\")\nb = torch.tensor([3.+3.j], device=\"xpu\")\nassert torch.isclose(a,b)\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/88. The reporter of the issue is etaf, and the assignee is , and the state of the issue is closed.", "reporter": "etaf", "assignee": "", "resolution": "The issue is closed.\nverified", "root_cause": "The abs function for complex data type is not implemented on XPU device.", "state": "closed"}


### Merged Result:74{"issue_number": 74, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/74. The reporter of the issue is EikanWang, and the assignee is , and the state of the issue is closed.\nThe operator was implemented by CPU fallback. MKL implementation will be tracked by following operator development task. Close the issue.", "reporter": "EikanWang", "assignee": "", "resolution": "\nThe operator was implemented by CPU fallback.", "root_cause": "The operator was implemented by CPU fallback.", "state": "closed"}


### Merged Result:72{"issue_number": 72, "issue_description": "Currently, this repo does not provide any code format logic. We need to port PyTorch linter tools here and enable it in our CI.\n", "reporter": "EikanWang", "assignee": "chuanqi129", "resolution": "\ndone", "root_cause": "", "state": "closed"}



### Merged Result:67{"issue_number": 67, "issue_description": "Stock PyTorch places the device kernel implementations under `aten/native/${device_tag}` while the code namespace is `at::native`. We need to refactor the code structure to align with the stock pytorch.", "reporter": "EikanWang", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:66{"issue_number": 66, "issue_description": "Currently, we have developed the test cases to validate the tensor creation operations for XPU. Compared to the stock PyTorch, the coverage of the self-developed test cases is lower than the stock PyTorch. So we need to port the storck PyTorch cases.\nDuplicated. Close.", "reporter": "EikanWang", "assignee": "daisyden", "resolution": "\nDuplicated. Close.", "root_cause": "", "state": "closed"}


### Merged Result:58{"issue_number": 58, "issue_description": "If we fallback `aten::set_.source_Storage` and `aten::set_.source_Storage_storage_offset` to CPU, pytorch cause a runtime error when running huggingface model: RuntimeError: Attempted to set the storage of a tensor on device \"cpu\" to a storage on different device \"xpu:0\". This is no longer allowed; the devices must match.", "reporter": "etaf", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}


### Merged Result:33{"issue_number": 33, "issue_description": "Integer div result with wrong data type(shoule be float but got int).\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/33. The reporter of the issue is etaf, and the assignee is , and the state of the issue is closed.", "reporter": "etaf", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}
