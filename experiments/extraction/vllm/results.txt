

Result: 1545{"issue_number": 1545, "issue_descrption": "NMS conflict", "reporter": "_githubsgi", "assignee": "None", "resolution": "", "root_cause": "Appears to a conflict between torch-xpu-ops NMS and torchvision NMS kernels.", "state": "open"}

Result: 1543{"issue_number": 1543, "issue_descrption": "Different allocation in memory management between XPU and CUDA", "reporter": "Meng, Hengyu", "assignee": "Yu, Guangye", "resolution": "The issue is related to the memory allocation behavior of the XPU allocator, which reserves more memory than necessary during the fine-tuning process. The reporter has tried to debug the issue by adding hooks to monitor memory usage and modifying the XPU allocator to record memory allocations. The logs show that the XPU allocator reserves an extra 8GB of memory even before the first backward pass, which is not observed in the CUDA allocator. The root cause of the issue is not explicitly mentioned, but it is likely related to the way the XPU allocator handles memory reservations.", "root_cause": "The root cause of the issue is not explicitly mentioned, but it is likely related to the way the XPU allocator handles memory reservations. The extra 8GB of memory reserved by the XPU allocator is not observed in the CUDA allocator, suggesting that there may be a difference in how the two allocators manage memory reservations.", "state": "open"}

Result: 1537{"issue_number": 1537, "issue_descrption": "With 2025.0 got accuracy gap when check optimizer state dict, PVC 1100, XELINK, 2 ranks.", "reporter": "Daisy Deng", "assignee": "Daisy Deng", "resolution": "", "root_cause": "", "state": "open"}

Result: 1536{"issue_number": 1536, "issue_descrption": "On PVC 1100 ZE_AFFINITY_MASK=0,1 GPU 0/0 GPU 1/0 GPU 2/0 GPU 3/0 CPU Affinity GPU 0/0 S XL4 SYS SYS 0-55,112-167 GPU 1/0 XL4 S SYS SYS 0-55,168-223 GPU 2/0 SYS SYS S XL24 56-111,168-223 GPU 3/0 SYS SYS XL24 S 56-111,168-223 branch https://github.com/daisyden/pytorch/tree/distributed_2.8 + torch-xpu-ops 11320f39484d1887870f24172c4803392491a76c, build with 2025.0 test_distributed_checkpoint.py got random failures, sometimes it can pass. pytest -v test_distributed_checkpoint.py -k test_distributed_checkpoint_state_dict_type0_xpu 11 ../../../../test/distributed/fsdp/test_distributed_checkpoint.py::TestDistributedCheckpointXPU::test_distributed_checkpoint_state_dict_type0_xpu 2025:04:02-03:30:40:(4119976) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi 12 2025:04:02-03:30:40:(4119976) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL 13 2025:04:02-03:30:40:(4119977) |CCL_WARN| did not find MPI-launcher specific variables, switch to ATL/OFI, to force enable ATL/MPI set CCL_ATL_TRANSPORT=mpi 14 2025:04:02-03

Result: 1536{"issue_number": 1536, "issue_descrption": "test_distributed_checkpoint.py got atl_status: FAILURE with 2025.0 randomly.", "reporter": "Daisy Deng", "assignee": "Daisy Deng", "resolution": "The issue was confirmed to be resolved with the latest DL-essentials kit: 2025.1.0.539, the latest xpu-ops commit, and the pytorch commit: https://github.com/daisyden/pytorch/tree/distributed_2.8. Additionally, oneCCL version **release/ccl_2025.16-gold** was used on Borealis. Further verification is needed to confirm if the issue persists in the latest release (basekit 2025.1).", "root_cause": "The root cause of the issue is not explicitly identified, but it is suspected to be related to the version of the basekit (2025.0) used. The issue does not occur with the latest DL-essentials kit and oneCCL version.", "state": "Open"}

Result: 1535{"issue_number": 1535, "issue_descrption": "RuntimeError: Process 0 terminated or timed out after 300.09047198295593 seconds", "reporter": "Cheng, Penghui", "assignee": "Ratnam Parikh", "resolution": "The issue was resolved by using the latest PyTorch commit from the fork: https://github.com/daisyden/pytorch/tree/distributed_2.8, and oneCCL release branch: **release/ccl_2021.15-gold**. Additionally, setting the environment variables CCL_SEND=direct, CCL_RECV=direct, and CCL_ATL_TRANSPORT=ofi helped in resolving the issue.", "root_cause": "The root cause of the issue was related to the configuration and version of PyTorch and oneCCL being used. The specific environment and version settings were necessary to ensure proper functioning of the distributed tests.", "state": "closed"}

Result: 1533{"issue_number": 1533, "issue_descrption": "pytorch build got permission issue for windows", "reporter": "None", "assignee": "None", "resolution": "None", "root_cause": "None", "state": "open"}

Result: 1532{"issue_number": 1532, "issue_descrption": "NotImplementedError: The operator 'torchvision::deform_conv2d' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.", "reporter": "None", "assignee": "Yutao Xu", "resolution": "The operator 'torchvision::deform_conv2d' is implemented in version 2.7 of the intel-extension-for-pytorch package. The user should update to this version to resolve the issue.", "root_cause": "The operator 'torchvision::deform_conv2d' was not implemented for the XPU device in the version of the intel-extension-for-pytorch package that the user was using.", "state": "RESOLVED"}

Result: 1527{"issue_number": 1527, "issue_descrption": "error: torch._dynamo.exc.InternalTorchDynamoError: AttributeError: __enter__ cases: FAILED [18.2440s] test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_automatic_dynamic_scalar FAILED [13.4365s] test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_automatic_dynamic_speculation_divergence FAILED [27.3630s] test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_automatic_dynamic_tensor FAILED [21.1475s] test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_dim_mismatch FAILED [15.2452s] test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_graph_break_empty_graph_still_collective FAILED [20.1470s] test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_missing_source FAILED [18.7465s] test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_scalar_missing_source FAILED [18.7466s] test_dynamo_distributed.py::TestMultiProc::test_compiler_collectives_type_mismatch FAILED [26.9530s] test_dynamo_distributed.py::TestMultiProc::test_ddp_activation_checkpointing FAILED [25.4544s] test_dynamo_distributed.py::TestMultiProc::test_ddp_baseline_aot_eager_multiprocess FAILED [24.2519s] test_dynamo_distributed.py::TestMultiProc::test_fsdp_activation_checkpointing FAILED [18.5464s] test_dynamo_distributed.py::TestMultiProc::test_fsdp_aot_eager FAILED [18.9482s] test_dynamo_distributed.py::TestMultiProc::test_fsdp_inductor FAILED [18.0446s] test_dynamo_distributed.py::TestMultiProc::test_fsdp_setattr FAILED [13.6382s] test_dynamo_distributed.py::TestMultiProc::test_fsdp_unspecialized_forced_getattr_inline FAILED [18.9445s] test_dynamo_d

Result: 1527{"issue_number": 1527, "issue_descrption": "The issue is related to a runtime error encountered when using torch._dynamo, specifically an `InternalTorchDynamoError: AttributeError: enter` error. The error seems to be related to a hard-coded reference to CUDA in the PyTorch codebase, which needs to be addressed. The error might be due to a division by zero or an unsupported CUDA operation.", "reporter": "Cheng, Penghui", "assignee": "Cherry Zhang", "resolution": "The issue was resolved by submitting a PR to fix the hard-coded reference to CUDA in the PyTorch codebase. The specific code that needed to be fixed is located at https://github.com/pytorch/pytorch/blob/17005992668f3f6e25761930e6514de435922b13/torch/_dynamo/output_graph.py#L1347-L1350.", "root_cause": "The root cause of the issue is a hard-coded reference to CUDA in the PyTorch codebase, which is not compatible with the XPU backend being used. This incompatibility leads to runtime errors such as `InternalTorchDynamoError: AttributeError: enter` and potential issues like division by zero or unsupported CUDA operations.", "state": "closed"}

Result: 1526{"issue_number": 1526, "issue_descrption": "RuntimeError: UR backend failed. UR backend returns:40 (UR_RESULT_ERROR_OUT_OF_RESOURCES)", "reporter": "Cheng, Penghui", "assignee": "Cherry Zhang", "resolution": "The issue was found to occur in multiple test cases, not just the dynamo case. The root cause is related to resource limitations in the UR backend, specifically an out-of-resources error (UR_RESULT_ERROR_OUT_OF_RESOURCES).", "root_cause": "The root cause is related to resource limitations in the UR backend, specifically an out-of-resources error (UR_RESULT_ERROR_OUT_OF_RESOURCES).", "state": "open"}

Result: 1525{"issue_number": 1525, "issue_descrption": "ValueError: trying to initialize the default process group twice!", "reporter": "Cheng, Penghui", "assignee": "Chao Han", "resolution": "", "root_cause": "", "state": "open"}

Result: 1521{"issue_number": 1521, "issue_descrption": "Enabling PyTorch Flex Attention on XPU fails with errors.", "reporter": "_githubsgi", "assignee": "Zhang, Liangang", "resolution": "FlexAttention is not enabled on XPU yet. It is targeted to be enabled on torch-2.8.", "root_cause": "PyTorch Flex Attention is not currently supported on XPU.", "state": "open"}

Result: 1519{"issue_number": 1519, "issue_descrption": "Ge, Qinling found 2 coredump issues in IPEX2.7, the two cases also failed in Pytroch2.7 without IPEX, please help to check: **nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_max_pool_nan_inf_xpu_float32 test_ops_xpu.py::TestCommonXPU::test_dtypes__refs_nn_functional_pdist_xpu** For nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_max_pool_nan_inf_xpu_float32, this case will fail on some tiles and pass on some tiles. You can reproduce it by running the command such as ", "reporter": "Huaiyu, Zheng", "assignee": "Yutao Xu", "resolution": "", "root_cause": "", "state": "open"}

Result: 1518{"issue_number": 1518, "issue_descrption": "Using nightly build PT2.8, this sample code will return wrong output: import torch from datasets import load_dataset from transformers import pipeline, Wav2Vec2Processor model_id = 'facebook/hubert-large-ls960-ft' device = 'xpu' torch_dtype = torch.float16 generator = pipeline( 'automatic-speech-recognition', model=model_id, device=device, torch_dtype=torch_dtype, ) ds = load_dataset('patrickvonplaten/librispeech_asr_dummy', 'clean', split='validation', trust_remote_code=True) input_data = ds[0]['audio']['array'] with torch.inference_mode(): output = generator(input_data) print(f'output: {output}') while using stock stable pytorch 2.6 version, the output is correct. After investigation, find the output of scaled_dot_product_attention API is wrong.", "reporter": "kaixuanliu", "assignee": "LuFengqing", "resolution": "The issue was related to a difference in the math implementation between the Scaled Dot-Product Attention (SDPA) and the Math Kernel Library (MKL) path in PyTorch 2.8. The root cause was identified as a bug in the SDPA implementation that caused incorrect output. The fix involved ensuring that the SDPA implementation aligns with the MKL path to produce the correct output.", "root_cause": "The root cause of the issue was a bug in the Scaled Dot-Product Attention (SDPA) implementation in PyTorch 2.8, which led to incorrect output. This discrepancy was observed when comparing the output with PyTorch 2.6, where the output was correct.", "state": "resolved"}

Result: 1518{"issue_number": 1518, "issue_descrption": "scaled_dot_product_attention returns wrong output", "reporter": "kaixuanliu", "assignee": "LuFengqing", "resolution": "The issue was caused by a difference between the SDPA math and MKL path. The output at line 766 of the Hubert model in the transformers library was incorrect when using PT2.8. The problem was resolved by ensuring that the correct path was used for the scaled_dot_product_attention function.", "root_cause": "The root cause of the issue was a discrepancy in the implementation of the scaled_dot_product_attention function between the SDPA math and MKL path. This discrepancy led to incorrect output in certain scenarios, particularly when using PT2.8.", "state": "closed"}

Result: 1513{"issue_number": 1513, "issue_descrption": "We plan to add Inductor UT test into nightly test, but met this issue when adding cases. Github action stage: https://github.com/intel/torch-xpu-ops/blob/refs/heads/ruijie/Inductor_UT/.github/workflows/_linux_ut.yml#L204 Test results: You can check the log: https://github.com/intel/torch-xpu-ops/actions/runs/14032267388", "reporter": "Zhong Ruijie", "assignee": "Zhong Ruijie", "resolution": "", "root_cause": "", "state": "open"}

Result: 1512{"issue_number": 1512, "issue_descrption": "first run take long time on windows bmg/arc (not test lnl etc), but proper time on Linux\n\nStep to repro\ncreate a new clean conda env \npip install pytorch wheels (try on both v2.6/v2.7.0_0312/v2.7_0326 )\n\nrun a simple script\n```\nimport torch\n\na = torch.randn(10,1).to('xpu')\nprint(a)\n```\n\nARC with torch_0312 take ~37s to finish the first run , ~5s for the second run\nARC with torch_0326 take ~37s to finish the first run, ~5s for the second run\nARC with v2.6 wheel take ~53s to finish the first run , ~4s for the second run\n\nBMG with release 2.6 take ~22s to finish the first run , ~3s for the second run\nBMG with torch_0312 take ~15s to finish the first run , ~3s for the second run\nBMG with torch_0326 take ~16s to finish the first run, ~3s for the second run\n\n\n### Versions\n\nrepro for both release 2.6/2.7\nrelease 2.7 a little better", "reporter": "None", "assignee": "LuFengqing", "resolution": "", "root_cause": "", "state": "open"}

Result: 1510{"issue_number": 1510, "issue_descrption": "Some test cases in test/xpu will be hang, such as test_tensor_creation_ops_xpu.py::TestTensorCreationXPU::test_linspace_xpu_complex128. Once a case got failed, all the next will be also failed, and rerun the failed individually will be passed.", "reporter": "None", "assignee": "Stonepia", "resolution": "Adding an `empty_cache` to the test cases to resolve the hanging issue.", "root_cause": "The hanging issue is caused by memory leaks or insufficient memory, leading to the test cases getting stuck and failing subsequently.", "state": "closed"}

Result: 1510{"issue_number": 1510, "issue_descrption": "Some test cases will be hang on BMG Ubuntu, including test_ops_xpu.py::TestCommonXPU::test_dtypes__refs_sum_xpu, test_ops_xpu.py::TestCommonXPU::test_dtypes__refs_tan_xpu, test_ops_xpu.py::TestCommonXPU::test_dtypes__refs_tanh_xpu, test_ops_xpu.py::TestCommonXPU::test_dtypes__refs_trace_xpu, test_ops_xpu.py::TestCommonXPU::test_dtypes__refs_tril_xpu. Additionally, some tests are running for a long time before failing, such as test_ops_xpu.py and test_meta_xpu.py.", "reporter": "None", "assignee": "Stonepia", "resolution": "Adding an `empty_cache` to the tests to clear the GPU memory might resolve the hanging and long-running issues.", "root_cause": "The hanging and long-running issues are likely due to memory leaks or insufficient GPU memory, leading to the tests getting stuck or taking an unusually long time to complete.", "state": "open"}

Result: 1509{"issue_number": 1509, "issue_descrption": "backward failed. log: File \"/home/penghuic/pytorch/test/distributed/test_multi_threaded_pg.py\", line 336, in test_bwd_sees_fwd_pg x.sum().backward() RuntimeError: Data corruption detected reproduce command: pytest -v test/distributed/test_multi_threaded_pg.py", "reporter": "Cheng, Penghui", "assignee": "Cherry Zhang", "resolution": "In max 1550 device, this case will result in segmentation fault error. Oneccl not perfect support multi-thread. So skip it first.", "root_cause": "The issue is related to the imperfect support of multi-threading in Oneccl, which leads to data corruption and segmentation fault errors when running the test case on a specific device (max 1550).", "state": "closed"}

Result: 1508{"issue_number": 1508, "issue_descrption": "torch.ops._c10d_functional.reduce_scatter_tensor_coalesced RuntimeError: result type Float can't be cast to the desired output type Long.", "reporter": "Cheng, Penghui", "assignee": "Ratnam Parikh", "resolution": "The issue is caused by the unsupported INT64 datatype in SYCL collectives. The latest release branch of oneCCL (release/ccl_2021.15-gold) has added a fallback mechanism for unsupported cases, but this fallback doesn't support the AVG operation. A ticket has been opened to address this issue.", "root_cause": "The root cause of the issue is the lack of support for INT64 datatype in SYCL collectives, which leads to a fallback to an older implementation that does not support the AVG operation.", "state": "open"}

Result: 1506{"issue_number": 1506, "issue_descrption": "E2E (hf & timm) models got fail_accuracy", "reporter": "Libo Hao", "assignee": "None", "resolution": "", "root_cause": "", "state": "open"}

Result: 1505{"issue_number": 1505, "issue_descrption": "14 Timm models got fail_accuracy on ARC-WSL.", "reporter": "Libo Hao", "assignee": "None", "resolution": "None", "root_cause": "None", "state": "open"}

Result: 1504{"issue_number": 1504, "issue_descrption": "On PVC 1550 with two tiles we found test/distributed/fsdp have 30% cases failed with accuracy issue. Verified on 1100 machine with ZE_AFFINITY_MASK=0,1 set, the cases also got accuracy failures. ", "reporter": "Daisy Deng", "assignee": "Cherry Zhang", "resolution": "We have identified this issue is a new regression issue from oneCCL and created a corresponding issue to oneCCL.", "root_cause": "This issue only occurs on PVC 1550 or PVC 1100 (multi-card) when Xelink is used.", "state": "open"}

Result: 1504{"issue_number": 1504, "issue_descrption": "tensor([-0.2677,  0.1447], device='xpu:0', requires_grad=True) To execute this test, run the following from the base repo dir: python test/distributed/fsdp/test_fsdp_misc.py TestFSDPMiscMultiProcess.test_fsdp_optimizer_overlap", "reporter": "Daisy Deng", "assignee": "Cherry Zhang", "resolution": "We have identified this issue is a new regression issue from oneCCL and created a corresponding issue to oneCCL.", "root_cause": "This issue only occurs on PVC 1550 or PVC 1100 (multi-card) when Xelink is used.", "state": "open"}

Result: 1503{"issue_number": 1503, "issue_descrption": "This is the github issue title redefinition error when build PyTorch release/2.7 from source with oneapi in conda env and then activate oneapi on Windows, and issue body Content of #1503: { ### 🐛 Describe the bug\n\n![Image](https://github.com/user-attachments/assets/56f500fb-059d-4392-b6b6-34295dbf4180)\nthis is the conda list of the conda env which contains intel-sycl-rt etc\nwith this env, if we build from source and activate oneapi with \n```\ncall \"C:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\env\\vars.bat\"\ncall \"C:\\Program Files (x86)\\Intel\\oneAPI\\ocloc\\latest\\env\\vars.bat\"\n```\nthe compilation will trigger redefinition error \n\n### Versions\n\npytorch release/2.7", "reporter": "None", "assignee": "None", "resolution": "None", "root_cause": "None", "state": "open"}

Result: 1502{"issue_number": 1502, "issue_descrption": "WSL will crash when running torchbench.", "reporter": "Libo Hao", "assignee": "None", "resolution": "", "root_cause": "", "state": "open"}

Result: 1500{"issue_number": 1500, "issue_descrption": "", "reporter": "", "assignee": "", "resolution": "", "root_cause": "", "state": ""}

Result: 1498{"issue_number": 1498, "issue_descrption": "5 extended uts failed with **RuntimeError: Native API failed. Native API returns: 29 (UR_RESULT_ERROR_INVALID_KERNEL_NAME)**.", "reporter": "Libo Hao", "assignee": "gaopengff", "resolution": "", "root_cause": "", "state": "open"}

Result: 1497{"issue_number": 1497, "issue_descrption": "RoIAlign autocast test got failed", "reporter": "None", "assignee": "None", "resolution": "None", "root_cause": "None", "state": "None"}

Result: 1496{"issue_number": 1496, "issue_descrption": "When running E2E inductor on LNL, the following error appears randomly: ![Image](https://github.com/user-attachments/assets/b5254124-e2a1-4d6a-80c7-40719aea7fb7) Versions: - stock pytorch : - pip install torch --index-url https://download.pytorch.org/whl/test/xpu - git clone https://github.com/pytorch/pytorch.git - git checkout b1940b5867e40e40ebdce4db76f76d3d0b71d3f4 - torch-xpu-ops: Commit (pin) - 026b2c8c7c92a7b2cec5d26334006e3423251cc6 - Driver: 32.0.101.6647", "reporter": "Libo Hao", "assignee": "None", "resolution": "This might be because of the driver that enables overcommit feature. Then the writing becomes invalid. We are still tracking this in GSD-10905. If there is a fix, we could switch back to re-test this.", "root_cause": "The issue might be due to the driver that enables overcommit feature, which causes invalid writing.", "state": "open"}

Result: 1483{"issue_number": 1483, "issue_descrption": "The issue is related to the segmentation fault encountered when running the SAM model on the XPU device with the Rolling driver version 1077.18. The model passes on the LTS driver version 803.61. The error occurs during the execution of the `torchbench.py` script with specific parameters, leading to a segmentation fault. The error message indicates issues with string operations and GPU access, suggesting potential problems with the driver or the model's interaction with the XPU device.", "reporter": "None", "assignee": "None", "resolution": "The issue was resolved by removing the function call `_sfdp_init()` as suggested by @jianyizh. This change allowed the model to pass the test on the XPU device.", "root_cause": "The root cause of the issue is likely related to the `sdpa` function, as indicated by the comments. The `_sfdp_init()` function call seems to be causing issues with the driver or the model's interaction with the XPU device, leading to segmentation faults.", "state": "closed"}

Result: 1480{"issue_number": 1480, "issue_descrption": "SDPBackend.FLASH_ATTENTION , SDPBackend.EFFICIENT_ATTENTION are missing and should be added a quickly as possible.", "reporter": "_githubsgi", "assignee": "LuFengqing", "resolution": "The backward pass is still missing.", "root_cause": "SDPBackend.FLASH_ATTENTION and SDPBackend.EFFICIENT_ATTENTION are not implemented yet.", "state": "open"}

Result: 1478{"issue_number": 1478, "issue_descrption": "When adding pytorch/test/test_xpu.py in torch-xpu-ops windows CI, we found these failure: FAILED [1.7263s] test_xpu.py::TestXpuXPU::test_lazy_init_xpu - subprocess.CalledProcessError: Command '['C:\\Users\\Devcloud\\.conda\\envs\\windows_ci\\python.exe' ...", "reporter": "Zhong Ruijie", "assignee": "LuFengqing", "resolution": "The issue was resolved by adding `if __name__ == '__main__':` for the `test_lazy_init_xpu` test case to address the process bootstrapping phase issue. The `test_wrong_xpu_fork_xpu` test case was skipped on Windows as it is a Linux-only test case.", "root_cause": "The root cause of the issue was that the test case `test_lazy_init_xpu` was not properly handling the process bootstrapping phase on Windows, leading to a `RuntimeError`. The `test_wrong_xpu_fork_xpu` test case was failing on Windows because it is designed for Linux and uses fork, which is not supported on Windows.", "state": "closed"}

Result: 1475{"issue_number": 1475, "issue_descrption": "When do the preci test for the branch daisyden/fsdp_test I found some cases of test_fsdp_core.py got random failures, such as: test_transformer_no_grad_mixed_precision_True_xpu test_transformer_no_grad_mixed_precision_False_xpu Traceback (most recent call last): File \"/home/sdp/miniforge3/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py\", line 605, in wrapper self._join_processes(fn) File \"/home/sdp/miniforge3/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py\", line 845, in _join_processes self._check_return_codes(elapsed_time) File \"/home/sdp/miniforge3/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py\", line 902, in _check_return_codes self.assertEqual( File \"/home/sdp/miniforge3/envs/xpu_op_/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py\", line 4094, in assertEqual raise error_metas.pop()[0].to_error( # type: ignore[index] AssertionError: Scalars are not equal! Expected 0 but got -11. Absolute difference: 11 Relative difference: inf Expect process 1 exit code to match Process 0 exit code of 0, but got -11", "reporter": "Daisy Deng", "assignee": "Cherry Zhang", "resolution": "The issue occurs on the CI machine and no specific resolution is provided in the comments.", "root_cause": "The root cause is not explicitly identified in the comments. However, it is suspected that the issue might be related to the driver or the environment setup on the CI machine.", "state": "open"}

Result: 1468{"issue_number": 1468, "issue_descrption": "With oneAPI 2025.1 int16/int32/int64 argmin result is incorrect", "reporter": "Daisy Deng", "assignee": "Stonepia", "resolution": "", "root_cause": "", "state": "open"}

Result: 1465{"issue_number": 1465, "issue_descrption": "RuntimeError: Non-uniform work-groups are not supported by the target device reported on BMG", "reporter": "Daisy Deng", "assignee": "None", "resolution": "", "root_cause": "", "state": "open"}

Result: 1453{"issue_number": 1453, "issue_descrption": "The BMG machine will crash when running hunggingface performance mode. The issue will go when adding parameter --batch-size=2.", "reporter": "Libo Hao", "assignee": "Stonepia", "resolution": "The issue is tracked in GSD-10905. The driver team is working on a switch to only use dedicated GPU memory and addressing the context break issue when the memory is almost full. This internal JIRA is marked for release 2.7.", "root_cause": "The system crash happens due to incorrect memory release after a few models run, leading to the crash. The issue occurs when the model is too large to fit into the dedicated GPU memory, and the shared GPU memory is taken into use. However, the memory is not correctly released after a few models run.", "state": "In Progress"}

Result: 1432{"issue_number": 1432, "issue_descrption": "torch-xpu-ops CI got the following failures, it is because pytorch commit https://github.com/pytorch/pytorch/commit/c21dc11a179b2714509cd901483138adacdce212. @DDEle  is investigating. test_transformers_xpu.py::TestTransformersXPU::test_multiheadattention_fastpath_attn_mask_attn_mask_dim_2_key_padding_mask_dim_2_bool_xpu test_transformers_xpu.py::TestTransformersXPU::test_multiheadattention_fastpath_attn_mask_attn_mask_dim_3_key_padding_mask_dim_2_bool_xpu test_transformers_xpu.py::TestTransformersXPU::test_transformerencoder_fastpath_use_torchscript_False_enable_nested_tensor_False_use_autocast_False_d_model_12_xpu test_transformers_xpu.py::TestTransformersXPU::test_transformerencoder_fastpath_use_torchscript_False_enable_nested_tensor_False_use_autocast_True_d_model_12_xpu test_transformers_xpu.py::TestTransformersXPU::test_transformerencoder_fastpath_use_torchscript_False_enable_nested_tensor_True_use_autocast_False_d_model_12_xpu test_transformers_xpu.py::TestTransformersXPU::test_transformerencoder_fastpath_use_torchscript_False_enable_nested_tensor_True_use_autocast_True_d_model_12_xpu see https://github.com/intel/torch-xpu-ops/actions/runs/13645060798/job/38146067831 _ TestMetaXPU.test_dispatch_symbolic_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_float32 _ Traceback (most recent call last): File \"/home/sdp/miniforge3/envs/xpu_op_1/lib/python3.10/site-packages/torch/testing/_internal/common_device_type.py\", line 1159, in test_wrapper return test(*args, **kwargs) File \"/home/sdp/miniforge3/envs/xpu_op_1/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py\", line 1534, in wrapper fn

Result: 1431{"issue_number": 1431, "issue_descrption": "", "reporter": "", "assignee": "", "resolution": "", "root_cause": "", "state": "open"}

Result: 1428{"issue_number": 1428, "issue_descrption": "test_quantize_per_channel gets core dump", "reporter": "Weishi.Deng", "assignee": "", "resolution": "This is because the XPU device is not found in checkZeroPoints(). The corresponding PR has been submitted to pytorch. However, after adding the device, the automatically called 'aten::dequantize.self' has not been registered on the QuantizedXPU backend, which needs further implementation.", "root_cause": "The XPU device is not found in checkZero_points().", "state": "open"}

Result: 1422 failed to extract

Result: 1401{"issue_number": 1401, "issue_descrption": "test_weight_norm.py::TestNNMethod::test_weight_norm_different_type failed with error AssertionError: Tensor-likes are not close!", "reporter": "Huaiyu, Zheng", "assignee": "Daisy Deng", "resolution": "", "root_cause": "", "state": "open"}

Result: 1400{"issue_number": 1400, "issue_descrption": "test_rms_norm.py::TestNNMethod::test_rms_norm_bw failed with error AssertionError: Tensor-likes are not close!", "reporter": "Huaiyu, Zheng", "assignee": "Yutao Xu", "resolution": "The issue is related to the IPEX (Intel PyTorch Extension) implementation of the RMSNorm operation. The error occurs when comparing gradients between CPU and XPU (Intel's AI processor) implementations, specifically with the bfloat16 data type. The discrepancy is significant, with the greatest absolute difference being 488.0 and the greatest relative difference being 0.265625, which exceeds the allowed tolerance of 0.1. The issue can only be reproduced on ARC (Intel's AI processor).", "root_cause": "The root cause of the issue is likely a precision or implementation difference between the CPU and XPU versions of the RMSNorm operation in IPEX. The significant differences in gradients suggest that there might be a numerical instability or precision loss in the XPU implementation, particularly with the bfloat16 data type.", "state": "open"}

Result: 1399{"issue_number": 1399, "issue_descrption": "According to 25ww07 weekly test, we check the torch2.6 found some ops have perf regression compared with 25ww06 weekly results [op list] loss.soft_margin_loss loss.soft_margin_loss_backward [Results] [microbench_report_zhong_ruijie_compare.xlsx](https://github.com/user-attachments/files/19046206/microbench_report_zhong_ruijie_compare.xlsx) This issue was found by Ruijie. Due to the size limit of the attachment, I only posted part of the data. If you need more data or information, please contact Ruijie or me.", "reporter": "Huaiyu, Zheng", "assignee": "Yutao Xu", "resolution": "", "root_cause": "", "state": "open"}

Result: 1381{"issue_number": 1381, "issue_descrption": "molan performance regression up to 76%, which is caused by pad_sequence and gru.input.", "reporter": "Huaiyu, Zheng", "assignee": "Yutao Xu", "resolution": "", "root_cause": "The performance regression up to 76% is caused by issues with pad_sequence and gru.input in molan.", "state": "open"}

Result: 1380{"issue_number": 1380, "issue_descrption": "Sort has performance regression in model pointnet-atlas(~5% on single tile and ~10% on scaling up)", "reporter": "Huaiyu, Zheng", "assignee": "Yutao Xu", "resolution": "", "root_cause": "", "state": "open"}

Result: 1352 failed to extract

Result: 1334{"issue_number": 1334, "issue_descrption": "timm_regnet BF16 gor fail accuracy recently, the last known good is https://github.com/intel/torch-xpu-ops/commit/b6786e31c36b31bb2cc18e2325451a3198832cb8 + torch a7c2d85. The command to reproduce the issue is `python benchmarks/dynamo/torchbench.py --accuracy --bfloat16 -d xpu -n10 --training --only timm_regnet --backend=inductor`. The error message is `RMSE (res-fp64): 0.01081, (ref-fp64): 0.00091 and shape=torch.Size([]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.001000, use_larger_multiplier_for_smaller_tensor: 0`. The issue is reproducible with torch-xpu-ops commit b6786e31c36b31bb2cc18e2325451a3198832cb8, python 3.10, TRITON_COMMIT_ID: e98b6fcb8df5b44eb0d0addb6767c573d37ba024, TORCH_COMMIT_ID: 106acf0eec837d93f7373d894c556bec6cb3c265, TORCHBENCH_COMMIT_ID: 373ffb19dc470f4423a3176a4133f8f4b3cdb5bd, TORCHVISION_COMMIT_ID: d23a6e1664d20707c11781299611436e1f0c104f, TORCHAUDIO_COMMIT_ID: 2709b65c9d3c55da40a5436ec4c45c427feb1d2a

Result: 1332{"issue_number": 1332, "issue_descrption": "compilation with XPU support fails with the issue below. Compiling CPU succeeds. error: unrecognizable insn: during RTL pass: vregs internal compiler error: in extract_insn, at recog.cc:2791", "reporter": "Jing Xu", "assignee": "Yutao Xu", "resolution": "The issue is resolved by using the `-fno-tree-loop-vectorize` flag with `-O2` optimization level. This avoids the error. The root cause is specific to GCC 12.3 and is related to the loop vectorization optimization.", "root_cause": "The root cause is specific to GCC 12.3 and is related to the loop vectorization optimization.", "state": "closed"}

Result: 1329{"issue_number": 1329, "issue_descrption": "NotImplementedError: The operator 'quantized::linear_dynamic' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.", "reporter": "Gurwinder Singh", "assignee": "Zhiwei", "resolution": "The operator 'quantized::linear_dynamic' is not currently implemented for the XPU device. The feature is targeted for PyTorch 2.8, and the user should try again when PyTorch 2.8 is released.", "root_cause": "The operator 'quantized::linear_dynamic' is not implemented for the XPU device in the current version of PyTorch.", "state": "open"}

Result: 1325{"issue_number": 1325, "issue_descrption": "So far, https://github.com/intel/torch-xpu-ops/pull/526 implemented the first MKL related operator, `aten.fft_c2c`. The PR introduced MKL building system in torch-xpu-ops. The MKL SDK introduced for building bases on oneAPI package. The potential issue is we would recommend using `pip install mkl-dpcpp` for runtime. There would be potential API breaking issue when MKL version in oneAPI package for building has gap with MKL in Pypi. We need to unify recommended MKL package for building and runtime.", "reporter": "Feng Yuan", "assignee": "Cui, Yifeng", "resolution": "", "root_cause": "Potential API breaking issue when MKL version in oneAPI package for building has gap with MKL in Pypi.", "state": "open"}

Result: 1324{"issue_number": 1324, "issue_descrption": "We found that when running models and the model is OOM, we get the UR Error, and this UR Error will break tensor context. \n\n```Python\n# On a 16.5 GB host memory of LNL\n\n# 1. First fill all the GPU memory\n(8GB)>>> x1 = torch.ones(1024*1024*1024, dtype=torch.float64, device='xpu')\n(8GB)>>> x2 = torch.ones(1024*1024*1024, dtype=torch.float64, device='xpu')\n(0.5GB)>>> x3 = torch.ones(128*1024*1024, dtype=torch.float32, device='xpu')\n\n# 1.1 You can see this access is ok\n>>> x1[100]\ntensor(1., device='xpu:0', dtype=torch.float64)\n\n# 1.2 Fill another tensor, this is OOM, expected. This x4 should not be created.\n>>> x4 = torch.ones(128*1024*1024, dtype=torch.float32, device='xpu')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\ntorch.OutOfMemoryError: XPU out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 16.50 GiB. Of the allocated memory 16.50 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. Please use `empty_cache` to release all unoccupied cached memory.\n \n# 2. Re-access the tensor, it gets UR Error with OUT_OF_RESOURCES. This is unexpected, the tensor context of x1 should be normal.\n \n>>> x1[100]\nFile \"C:\\Users\\sdp\\miniforge3\\envs\\tongsu_onednn_37_no_fix\\lib\\site-packages\\torch\\_tensor_str.py\", line

Result: 1315{"issue_number": 1315, "issue_descrption": "upstream test failures with test_ops.py", "reporter": "Daisy Deng", "assignee": "Zhiwei", "resolution": "", "root_cause": "The issue is related to test failures in test_ops.py. The failures are categorized into several types: oneDNN/oneMKL issue, unsupported op, unexpected success, dtype is not aligned and oneDNN complex issue, FP8, sparsity, fallback op accuracy issue, bug, and compiler issue. The detailed table is provided in the issue body.", "state": "open"}

Result: 1305{"issue_number": 1305, "issue_descrption": "The issue is about models failing accuracy tests on BMG but passing on PVC. The failing models include botnet26t_256, detectron2_maskrcnn_r_50_fpn, fbnetv3_b, gernet_l, and hf_Longformer. The tests were conducted using different configurations such as float16, bfloat16, and AMP. The issue was reported on BMG with specific software and hardware versions.", "reporter": "None", "assignee": "Stonepia", "resolution": "The issue was partially resolved. For the model `fbnetv3_b`, the problem was due to an unalignment with CUDA's test behavior. By applying the same behavior as CUDA's, which involves fallback to SGD optimizer for some models, the issue was resolved. For the model `gernet_l`, a PR was submitted to align with CUDA's optimizer. The root cause for `fbnetv3_b` was identified as the unalignment with CUDA's test behavior, and for `gernet_l`, further investigation is ongoing to identify the root cause.", "root_cause": "The root cause for `fbnetv3_b` was identified as the unalignment with CUDA's test behavior, specifically the fallback to SGD optimizer for some models. For `gernet_l`, the root cause is still under investigation.", "state": "partially resolved"}

Result: 1296{"issue_number": 1296, "issue_descrption": "Torchbench demucs training got fail accuracy", "reporter": "None", "assignee": "None", "resolution": "put fp64 ref on xpu can solve this issue", "root_cause": "currently cpu and xpu lstm have different implementation", "state": "closed"}

Result: 1280 failed to extract

Result: 1278{"issue_number": 1278, "issue_descrption": "Detectron2 inference accuracy got failed", "reporter": "None", "assignee": "None", "resolution": "For detectron2_fasterrcnn_r_50_c4 fp16 inference, the cosine similarity can increase from 0.955 to 0.986 (threshold is 0.99) if we keep the same calculation sequence in BatchNorm inference between eager and inductor. The root cause is that the calculation sequence in BatchNorm inference was different between eager and inductor, leading to a decrease in accuracy.", "root_cause": "The root cause is that the calculation sequence in BatchNorm inference was different between eager and inductor, leading to a decrease in accuracy.", "state": "open"}

Result: 1276{"issue_number": 1276, "issue_descrption": "Hf_T5_base inference got out of memory but training pass", "reporter": "None", "assignee": "LuFengqing", "resolution": "It failed in `nn.functional.softmax` in eager mode, which cannot be fused to SDPA operator. In addition, it seems that non-fused SDPA should only takes additional 768M (or 2x/3x of it), which should be fine to the platform with 48G memory. There might be other problems that results in the OOM.", "root_cause": "The failure is due to the `nn.functional.softmax` operation in eager mode, which cannot be fused to the SDPA operator. This operation requires additional memory, leading to an out-of-memory error. Additionally, there might be other issues causing the OOM error.", "state": "open"}

Result: 1275{"issue_number": 1275, "issue_descrption": "Eca_halonext26ts AMP_BF16 training accuracy got failed", "reporter": "None", "assignee": "Weishi.Deng", "resolution": "This issue exists on the inductor backend, disabling the decomposition of bn would help. This is a natural numeric issue caused by the bf16 type and we can mark it as a known issue.", "root_cause": "The issue has existed on all backends long before (xpu, cuda, according to weekly tests in 2024 Aug. and Sept.). This accuracy test still fails on CUDA backends in the local test with Pytorch 2.8. The test only fails on bf16 date type and the ut with one single model shape (128, 512, 32, 32) can cause about 0.00674 relative difference on bf16 dtype.", "state": "closed"}

Result: 1274{"issue_number": 1274, "issue_descrption": "Convnext_base BF16 training accuracy got failed", "reporter": "None", "assignee": "None", "resolution": "Can pass with onednn main after this fix https://github.com/uxlfoundation/oneDNN/pull/2935", "root_cause": "The root mean square error is very large, and I suspect onednn has some invalid memory access issues... Since we support fp64 now, we do not need patch to fallback fp64 reference, and the accuracy can pass with random nan reference result. The fp64 reference will have random nan issues on onednn v3.7 in a depthwise conv forward layer (accuracy test can pass when fp64 ref has nan...). Such nan only appears in model, it cannot be reproduced on a single ut with the same input and weight. Update onednn to main (7a741297e018707b21fb6a280b4399929503bbd7) will solve this issue, but there is small chance to meet gpu page fault and this large rmse again...", "state": "closed"}

Result: 1261{"issue_number": 1261, "issue_descrption": "Stable_diffusion_unet OutOfMemoryError: XPU out of memory, fp16 & bf16 inference are pass_due_to_skip but others throw out of memory error", "reporter": "None", "assignee": "Ye Ting", "resolution": "need to submit request to oneDNN for fp32.", "root_cause": "The issue is related to the memory allocation for fp32 precision in the Stable_diffusion_unet model on XPU. The model runs successfully with fp16 and bf16 precision but encounters an OutOfMemoryError when using fp32 precision.", "state": "open"}

Result: 1256{"issue_number": 1256, "issue_descrption": "The following models got 'eager_two_runs_differ':\n|     |            |                                                                                                           eager_two_runs_diff                                                                                                          |\n|-----|------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| LNL | HF         |                                                                                                                                                                                                                                        |\n|     | Timm       |                                                                                                                                        , ", "reporter": "Libo Hao", "assignee": "Stonepia", "resolution": "The issue was resolved by setting deterministic algorithms and avoiding atomic operations. Specifically, the models Super_SloMo and pytorch_CycleGAN_and_pix2pix were found to fail due to non-deterministic behavior caused by atomic operations. The resolution involved using `torch.use_deterministic_algorithms(True, warn_only=True)` and a pull request to avoid atomic operations. Additionally, a pull request was made to set deterministic algorithms for all models.", "root_cause": "The root cause of the issue was the non-deterministic behavior caused by atomic operations in certain models. The models were not set to use deterministic algorithms, leading to differences in results between runs.", "state": "closed"}

Result: 1256{"issue_number": 1256, "issue_descrption": "This is a github issue link https://github.com/intel/torch-xpu-ops/1256. The reporter of the issue is Libo Hao, and the assignee is Stonepia, This is the github issue title [E2E] HF/Timm/Torchbench models got ", "reporter": "Libo Hao", "assignee": "Stonepia", "resolution": "The issue was resolved by setting `torch.use_deterministic_algorithms(True, warn_only=True)` and avoiding atomic operations as mentioned in PR #1370. Additionally, a PR was created to set deterministic for all models, see https://github.com/pytorch/pytorch/pull/149028.", "root_cause": "The root cause of the issue is the non-deterministic behavior caused by atomic operations in the models. The models were not set to deterministic by default, leading to differences in runs.", "state": "closed"}

Result: 1256{"issue_number": 1256, "issue_descrption": "Timm, Torchbench, HF models got 'eager_two_runs_differ' error. Specifically, Super_SloMo (train_eager_fp32) and pytorch_CycleGAN_and_pix2pix (train_eager_fp32/bf16) from Torchbench, and DistilBertForMaskedLM (train_fp16_eager) from HF are failing. The issue is related to non-deterministic behavior due to atomic operations and the lack of deterministic settings in the models. The problem can be mitigated by setting `torch.use_deterministic_algorithms(True, warn_only=True)` and avoiding atomic operations.", "reporter": "Libo Hao", "assignee": "Stonepia", "resolution": "The issue can be resolved by setting `torch.use_deterministic_algorithms(True, warn_only=True)` and avoiding atomic operations. This approach has been tested and shown to pass for Super_SloMo. For pytorch_CycleGAN_and_pix2pix, the failure is expected due to atomic operations.", "root_cause": "The root cause of the issue is the non-deterministic behavior caused by atomic operations and the lack of deterministic settings in the models. The models do not set deterministic by default, leading to variations in results across runs.", "state": "closed"}

Result: 1256{"issue_number": 1256, "issue_descrption": "This is a github issue link https://github.com/intel/torch-xpu-ops/1256. The reporter of the issue is Libo Hao, and the assignee is Stonepia, This is the github issue title [E2E] HF/Timm/Torchbench models got ", "reporter": "Libo Hao", "assignee": "Stonepia", "resolution": "The issue was resolved by setting `torch.use_deterministic_algorithms(True, warn_only=True)` and avoiding atomic operations as per PR #1370. Additionally, some models passed on the latest PVC weekly, while others required further investigation on LNL and ARC. The root cause was identified as non-deterministic behavior due to atomic operations.", "root_cause": "Non-deterministic behavior due to atomic operations.", "state": "closed"}

Result: 1256{"issue_number": 1256, "issue_descrption": "HF/Timm/Torchbench models got 'eager_two_runs_differ'", "reporter": "Libo Hao", "assignee": "Stonepia", "resolution": "The issue is related to non-deterministic behavior caused by atomic operations in certain models. For Super_SloMo and pytorch_CycleGAN_and_pix2pix, the models did not set deterministic, leading to failures in eager-two-run-diff. Super_SloMo can pass if deterministic algorithms are used with `torch.use_deterministic_algorithms(True, warn_only=True)` and a fix for atomic operations. Other models like hf_Reformer, swin_base_patch4_window7_224, twins_pcpvt_base, coat_lite_mini, mobilevit_s, tnt_s_patch16_224 can pass in the latest PVC weekly. However, there are still issues with some models on BMG/LNL, indicating that atomic operations are not fully resolved.", "root_cause": "The root cause of the issue is the non-deterministic behavior caused by atomic operations in certain models. The models did not set deterministic, leading to failures in eager-two-run-diff. This is exacerbated by the fact that some models do not set deterministic by default, and the atomic operations cause non-deterministic behavior.", "state": "open"}

Result: 1221{"issue_number": 1221, "issue_descrption": "### 🐛 Describe the bug\n\npython benchmarks/dynamo/torchbench.py --accuracy --amp --amp-dtype float16 -d xpu -n10 --inference  --only torchrec_dlrm  --backend=inductor\n```bash\nxpu  eval  torchrec_dlrm                       \nERROR:common:\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 3095, in check_accuracy\n    new_result = optimized_model_iter_fn(model_copy, example_inputs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 576, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2786, in run_n_iterations\n    def run_n_iterations(self, mod, inputs):\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 751, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1186, in forward\n    return compiled_fn(full_args)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 322, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n  File \"/

Result: 1216{"issue_number": 1216, "issue_descrption": "Failed dtype: float32, float16 and bfloat16. AMP passed. The issue is encountered when running the command `python benchmarks/dynamo/huggingface.py --accuracy --float32 -d xpu -n10 --training--only DebertaV2ForQuestionAnswering --backend=inductor`. The error message indicates that the RMSE (res-fp64) is 0.53515, while (ref-fp64) is 0.01636, and the shape is torch.Size([]). The res.dtype is torch.float32, with a multiplier of 3.000000 and a tolerance of 0.010000. The issue is related to the DebertaV2ForQuestionAnswering model and the inductor backend.", "reporter": "None", "assignee": "None", "resolution": "The issue was resolved in a local test, but it regressed in a later nightly test. The resolution was achieved by upgrading the transformers library to the latest version. However, the issue reappeared in a subsequent nightly test.", "root_cause": "The root cause of the issue is related to a commit in the pytorch repository (https://github.com/pytorch/pytorch/commit/2980aed65b6c521e41ec8a995f4c94f184dd741b). Additionally, the issue persists even when using a specific version of transformers (243e186efbf7fb93328dd6b34927a4e8c8f24395) pinned by pytorch and with transformers version 4.44.2.", "state": "Closed"}

Result: 1214{"issue_number": 1214, "issue_descrption": "In preci test, there are random cases will fail, need root cause. The cases that have appeared are as follows: test/xpu/test_ops_xpu.py: test_python_ref__refs_exp_xpu_complex128 test_python_ref__refs_sigmoid_xpu_complex128 test_python_ref_executor__refs_log2_executor_aten_xpu_complex128 test_python_ref_executor__refs_exp_executor_aten_xpu_complex128 test_python_ref_torch_fallback__refs_log2_xpu_complex128 test_python_ref_torch_fallback__refs_log10_xpu_complex128 test_python_ref_torch_fallback__refs_sigmoid_xpu_complex128 More random failures to be added to skiplist: - [ ] TestCommonXPU.test_python_ref_executor__refs_sigmoid_executor_aten_xpu_complex128 - [ ] TestCommonXPU.test_compare_cpu_nn_functional_local_response_norm_xpu_bfloat16 - [ ] test_ops_xpu.py::TestCommonXPU::test_python_ref__refs_log10_xpu_complex128", "reporter": "Cheng, Penghui", "assignee": "Daisy Deng", "resolution": "Since it is a complex datatype, we lower its priority and target to 2.8.", "root_cause": "The issue involves random failures in precision tests for complex data types, specifically complex128. The failures are not consistent and occur sporadically, suggesting potential numerical instability or precision issues with the complex128 data type in the tests.", "state": "open"}

Result: 1210{"issue_number": 1210, "issue_descrption": "GoogleFnet in torch dynamo benchmarks uses fft, we need support it.", "reporter": "None", "assignee": "Cui, Yifeng", "resolution": "None", "root_cause": "None", "state": "open"}

Result: 1209{"issue_number": 1209, "issue_descrption": "we should add APIs like torch.backends.cuda.matmul.allow_tf32, torch.backends.cudnn.allow_tf32, torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction", "reporter": "None", "assignee": "Zhiwei", "resolution": "", "root_cause": "", "state": "open"}

Result: 1195{"issue_number": 1195, "issue_descrption": "We get nan when the dtype is complex. The tests are: PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal__refs_sin_xpu_complex128 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal__refs_sin_xpu_complex64 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_nn_functional_tanhshrink_xpu_complex128 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_nn_functional_tanhshrink_xpu_complex64 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sin_xpu_complex128 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sin_xpu_complex64 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sinh_xpu_complex128 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sinh_xpu_complex64 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large__refs_exp_xpu_complex128 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large__refs_exp_xpu_complex32 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large__refs_sin_xpu_complex32 PYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufunc

Result: 1173{"issue_number": 1173, "issue_descrption": "The error caused fatal error in extended UT when run `test_ops_xpu.py::TestCommonXPU::test_compare_cpu_grid_sampler_2d_xpu_float64` Current thread 0x00003b20 (most recent call first): File ", "reporter": "Daisy Deng", "assignee": "Xu Han", "resolution": "Suggest Meta roll back to VS2019, but no response so far: https://github.com/pytorch/pytorch/issues/145702#issuecomment-2671374271", "root_cause": "The error is reported when run torch cpu. It can be reproduced with the below small case. It only fails when run the code with pytest and with dtype float64. Run the code directly it can pass. I tried different pytest versions and it all have such issue. With XPU package 20241202 and nightly build both have this issue. However, with torch cpu package it can work.", "state": "open"}

Result: 1171{"issue_number": 1171, "issue_descrption": "On LNL Windows with 1202 nightly wheel we got this error. No such problem on linux.", "reporter": "Daisy Deng", "assignee": "gaopengff", "resolution": "This seems a compiler issue. We already tracked this on jira **PYTORCHDGQ-5888**", "root_cause": "The error is related to a compiler issue on Windows, which does not occur on Linux.", "state": "closed"}

Result: 1165{"issue_number": 1165, "issue_descrption": "Add ci github action test running Huggingface Transformers test suite against XPU backend. Test goals:\n1. Catch regressions coming from PyTorch XPU backend which affect Transformers\n2. Catch new features coming from Transformers which require implementation efforts in PyTorch XPU\nDesign approach is to be as close to Transformers ci environment as possible. See [Dockerfile](https://github.com/huggingface/transformers/blob/v4.47.0/docker/transformers-pytorch-gpu/Dockerfile), see [self-push.yml](https://github.com/huggingface/transformers/blob/v4.47.0/.github/workflows/self-push.yml) for the references.\n\nSetup the following test triggers:\n- [x] Per opened PR modifying github action workflow file with the test (or any file in the repo from which workflow file depends on)\n- [x] Per manual trigger event optionally specifying PyTorch XPU nightly build to test (default - latest nightly)\n\nSetup environment as follows (T - required by Transformers tests):\n- [x] Use `linux.idc.xpu` runners\n- [x] Use Ubuntu based hosts (22.04 or later)\n- [x] Install: `apt-get install git-lfs && git lfs install` (T)\n- [x] Install: `apt-get install espeak-ng` (T)\n  * `tests/models/wav2vec2_phoneme/test_tokenization_wav2vec2_phoneme.py::Wav2Vec2PhonemeCTCTokenizerTest::test_batch_encode_plus_padding` (v4.47.0)\n- [x] Install: `apt-get install pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libavfilter-dev libswscale-dev libswresample-dev` (T)\n- [x] Use Conda virtual environment with python 3.10: `conda create -y -n venv python=3.10` (3.12 steps into https://github.com/PyAV-Org/PyAV/issues/1140)\n- [x] Clone Transformers `v4.47.0` (https://github.com/huggingface/transformers

Result: 1163{"issue_number": 1163, "issue_descrption": "torch._standard_gamma() has accuracy gap compared to scipy and torch.cpu", "reporter": "Daisy Deng", "assignee": "Yutao Xu", "resolution": "Fixed by PR #1161", "root_cause": "The issue seems to be related to the driver version and the implementation of torch._standard_gamma() on XPU. Further investigation is needed to pinpoint the exact cause.", "state": "Investigation in progress"}

Result: 1160{"issue_number": 1160, "issue_descrption": "When the two tensors are the same, what is the expected result? It is 1.0 or a number close to 1.0? This will lead to different result when apply trunc, lead to the UT failures.", "reporter": "Daisy Deng", "assignee": "Yutao Xu", "resolution": "The issue arises due to the difference in the results of `torch.div` with and without the `rounding_mode` parameter. When `rounding_mode='trunc'` is used, the result is exactly 1.0, but without it, the result is a number very close to 1.0 but not exactly 1.0. This discrepancy causes issues when applying `torch.trunc` to the result, leading to test failures. The expected result should be 1.0 when the divisor and dividend are the same, and the resolution involves ensuring that the `torch.div` operation with `rounding_mode='trunc'` always returns 1.0 in such cases.", "root_cause": "The root cause of the issue is the difference in behavior of `torch.div` with and without the `rounding_mode` parameter when the divisor and dividend are the same. This leads to unexpected results when truncation is applied, causing test failures.", "state": "open"}

Result: 1159{"issue_number": 1159, "issue_descrption": "Huggingface model DebertaForQuestionAnswering && DebertaV2ForMaskedLM failed with **RuntimeError: value cannot be converted to type at::BFloat16 without overflow** . Script: `python benchmarks/dynamo/huggingface.py --accuracy -d xpu -n10 --inference --backend=eager --cold-start-latency --amp --amp-dtype float16 --only DebertaForQuestionAnswering` Error Log Info: xpu eval DebertaForMaskedLM Traceback (most recent call last): File ", "reporter": "Libo Hao", "assignee": "Stonepia", "resolution": "The issue is related to the transformers library and is being tracked in the following PR: https://github.com/huggingface/transformers/pull/35336. The resolution will be updated once the transformer library is fixed.", "root_cause": "The root cause of the issue is a problem within the transformers library that causes a RuntimeError when trying to convert a value to the BFloat16 data type without causing an overflow.", "state": "open"}

Result: 1158{"issue_number": 1158, "issue_descrption": "Script: `python benchmarks/dynamo/huggingface.py --accuracy -d xpu -n10 --inference --backend=eager --cold-start-latency --float32 --only BlenderbotSmallForCausalLM ` Log info: xpu eval BlenderbotForCausalLM Traceback (most recent call last): File ", "reporter": "Libo Hao", "assignee": "Stonepia", "resolution": "This issue should be tracked by https://github.com/intel/torch-xpu-ops/issues/1324, target agama 25.11. For internal user, please see JIRA GSD-10738.", "root_cause": "It seems a driver issue.", "state": "closed"}

Result: 1150{"issue_number": 1150, "issue_descrption": "Some operators UT fails on XPU with 'Kernel is incompatible with all devices' error. The error occurs when running certain operations on the XPU device, such as logcumsumexp and nn.functional.pdist. The issue is observed on Linux with PyTorch 2.6 nightly and an ARC A60 card. The error message is 'RuntimeError: Kernel is incompatible with all devices in devs'.", "reporter": "Cheng, Penghui", "assignee": "Feng Yuan", "resolution": "The issue is not critical and only impacts some unit tests on ARC. It will not be cherry-picked for PyTorch 2.6 as it is not the most critical issue. The error 'Kernel is incompatible with all devices in devs' is a runtime error of SYCL, which occurs when calling 'sycl::get_kernel_bundle' before 'sycl::queue::submit'. The root cause is that the data types of input tensors are not FP64, and the solution involves changing the scalar or immediate value from double to float to make the operation work on ARC.", "root_cause": "The root cause is that the data types of input tensors are not FP64, leading to a runtime error of SYCL when calling 'sycl::get_kernel_bundle' before 'sycl::queue::submit'.", "state": "closed"}

Result: 1141{"issue_number": 1141, "issue_descrption": "Support `NestedTensor` for `xpu` device\n\n- [x] https://github.com/pytorch/pytorch/pull/140461\n- [x] https://github.com/intel/torch-xpu-ops/pull/1140\n- [x] https://github.com/intel/torch-xpu-ops/pull/1142\n- [x] https://github.com/pytorch/pytorch/pull/145467\n- [x] Add NestedTensor XPU ops\n  - [x] `_nested_from_padded ` https://github.com/intel/torch-xpu-ops/pull/1045\n  - [x] `_nested_tensor_softmax_with_shape` https://github.com/intel/torch-xpu-ops/pull/1323 \n  - [x] device-agnostic NestedTensor ops https://github.com/intel/torch-xpu-ops/pull/1258\n\n- [ ] Add NestedTensor XPU Uts\n\n**Example**\n```\n>>> nt = torch.nested.nested_tensor([]).to(\"xpu\")\n>>> nt\nnested_tensor([\n\n], device='xpu:0')\n>>> nt.is_xpu\nTrue\n```\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_", "reporter": "None", "assignee": "Daisy Deng", "resolution": "", "root_cause": "", "state": "open"}

Result: 1129{"issue_number": 1129, "issue_descrption": "Investigate whether pad mm is useful on XPU", "reporter": "None", "assignee": "None", "resolution": "General guideline from onednn team: pad to a multiple of 64 bytes (1 cache line) in the unit-stride dimension, but try to avoid multiples of large powers of 2 (say 2048 bytes). So for bf16, 37 elements in contiguous dimension = 74 bytes -> pad to 128 bytes, but 1023 elements = 2046 bytes -> pad to 2048 + 64 bytes to avoid a multiple of 2048. It's enough to pad the tensor strides; you do not need to pad the tensor size itself if you don't want to.", "root_cause": "The issue is to investigate whether padding matrix multiplication (mm) is useful on XPU, and the comments provide guidelines on how to pad tensors to optimize performance on XPU.", "state": "open"}

Result: 1128{"issue_number": 1128, "issue_descrption": "add xpu support to align with cuda for inductor sdpa fusion", "reporter": "None", "assignee": "None", "resolution": "None", "root_cause": "None", "state": "open"}

Result: 1125{"issue_number": 1125, "issue_descrption": "We have a list of unit test on sparsity is to be enabled. test_dtypes_sparse_sampled_addmm_xpu, test_compare_cpu_sparse_sampled_addmm_xpu_float32, test_errors_sparse_mul_layout0_xpu, test_errors_sparse_mul_layout1_xpu, test_errors_sparse_mul_layout2_xpu, test_errors_sparse_mul_layout3_xpu, test_out_requires_grad_error_sparse_sampled_addmm_xpu_complex64, test_out_requires_grad_error_sparse_sampled_addmm_xpu_float32, test_mask_layout_sparse_coo_masked_amax_xpu_bfloat16, test_mask_layout_sparse_coo_masked_amax_xpu_float16, test_mask_layout_sparse_coo_masked_amax_xpu_float32, test_mask_layout_sparse_coo_masked_amax_xpu_float64, test_mask_layout_sparse_coo_masked_amin_xpu_bfloat16, test_mask_layout_sparse_coo_masked_amin_xpu_float16, test_mask_layout_sparse_coo_masked_amin_xpu_float32, test_mask_layout_sparse_coo_masked_amin_xpu_float64, test_mask_layout_sparse_coo_masked_prod_xpu_bfloat16, test_mask_layout_sparse_coo_masked_prod_xpu_bool, test_mask_layout_sparse_coo_masked_prod_xpu_complex128, test_mask_layout_sparse_coo_masked_prod_xpu_complex64, test_mask_layout_sparse_coo_masked_prod_xpu_float16, test_mask_layout_sparse_coo_masked_prod_xpu_float32, test_mask_layout_sparse_coo_masked_prod_xpu_float64, test_mask_layout_sparse_coo_masked_prod_xpu_int16, test_mask_layout_sparse_coo_masked_prod_xpu_int32, test_mask_layout_sparse_coo_masked_prod_xpu_int64, test_mask_layout_sparse_coo_masked_prod_xpu_int8, test_mask_layout_sparse_coo_masked_prod_xpu_uint8, test_mask_layout_sparse_coo_masked_sum_xpu_bfloat16, test_mask_layout_sparse_coo_masked_sum_xpu_bool, test_mask_layout_sparse_coo_masked_sum_xpu_complex128, test_mask_layout_sparse_coo_masked_sum_xpu_complex64, test_mask_layout_sparse_coo_mask

Result: 1124{"issue_number": 1124, "issue_descrption": "Precision issues depend on oneAPI", "reporter": "Daisy Deng", "assignee": "Daisy Deng", "resolution": "The precision issues are related to the handling of extreme values and complex numbers in certain operations. The inconsistencies between Numpy and XPU results are due to differences in how these operations are implemented and optimized in the respective libraries. Several tests have been fixed, but some remain unresolved.", "root_cause": "The root cause of the precision issues is the difference in handling extreme values and complex numbers between Numpy and XPU. This is due to the different implementations and optimizations in the respective libraries, leading to discrepancies in the results of certain operations.", "state": "open"}

Result: 1121{"issue_number": 1121, "issue_descrption": "The straight forward thought is kernel bundle is not device specific under a specific platform context (Like GPU platform). So we should not use `dev` (Existing WA for the https://github.com/intel/llvm/issues/15127) as a hint.", "reporter": "Feng Yuan", "assignee": "Feng Yuan", "resolution": "", "root_cause": "The existing workaround uses `dev` as a hint for `sycl::get_kernel_bundle`, which is not necessary under a specific platform context like GPU platform. The kernel bundle is not device specific in this context.", "state": "open"}

Result: 1109{"issue_number": 1109, "issue_descrption": "integrate oneDNN implementation of RNN", "reporter": "None", "assignee": "Yutao Xu", "resolution": "None", "root_cause": "None", "state": "open"}

Result: 1059{"issue_number": 1059, "issue_descrption": "Existing: https://github.com/intel/torch-xpu-ops/blob/b1582e14f9e27bcbc0666ff636389d2be61783e4/src/comm/DeviceProperties.h#L14-L28\nTODO: https://github.com/intel/llvm/pull/15650", "reporter": "Feng Yuan", "assignee": "majing", "resolution": "", "root_cause": "", "state": "open"}

Result: 1025{"issue_number": 1025, "issue_descrption": "Clarify branch policy of torch-xpu-ops repo - what's viable/strict branch?", "reporter": "Dmitry Rogozhkin", "assignee": "Eikan Wang", "resolution": "The branch policy for the torch-xpu-ops repository is as follows: the `torch-xpu-ops` commit pin in the PyTorch main branch is updated to the stock PyTorch every development cycle. As the release cycle approaches its freeze, a release branch, such as `release/2.5`, is created for tracking and debugging the remaining critical issues.", "root_cause": "The confusion arose due to the current setup where the PyTorch main branch points to the `viable/strict` branch of the `torch-xpu-ops` repository, which was not aligned with the expected behavior of pointing to the `main` branch unless code stabilization is required prior to a release.", "state": "closed"}

Result: 1016{"issue_number": 1016, "issue_descrption": "We are using kernel specific max work group size to avoid platform compatibility issue. The routine is, auto kid = ::sycl::get_kernel_id<KernelClass>(); auto kbundle = ::sycl::get_kernel_bundle<::sycl::bundle_state::executable>( ctx, {dev}, {kid}); sycl::kernel k = kbundle.get_kernel(kid); int max_work_group_size = k.get_info<::sycl::info::kernel_device_specific::work_group_size>(dev); sycl::get_kernel_bundles gets severe host overhead. The data is as below, ![image](https://github.com/user-attachments/assets/50788d43-8598-4167-b031-766de9487044) Impacts: All kernels in torch-xpu-ops launched with kernel specific max work group are impacted. 1. 40us overhead is not acceptable for some single batch inference cases, since latency of kernels might be less than 10us. 2. CUDA runtime usually spends ~6us for a kernel launch. https://github.com/intel/llvm/issues/15824", "reporter": "Feng Yuan", "assignee": "majing", "resolution": "", "root_cause": "", "state": "open"}

Result: 1009{"issue_number": 1009, "issue_descrption": "PyTorch XPU verbose log should be clear and comparable with PyTorch GPU practice. Need to investigate and give the clear implementation requirement on different scenarios. For example, 1) in initialize phase, Torch GPU should give clear software stack, running GPU information. 2) If run Torch XPU in a host with CPU only, it should give the CPU fallback information. 3) when run workload, it should give comparable verbose message as stock Pytorch. 4) when run in error, it should give helpful error message and reminder to report issue or get help message. 5) error message for some other exceptional cases", "reporter": "RiverLiu", "assignee": "Feng Yuan", "resolution": "", "root_cause": "", "state": "open"}

Result: 1005{"issue_number": 1005, "issue_descrption": "Integrate oneDNN GEMM INT4 kernels, and serves for Torchao LLM usage. It requires pass UT and example workloads usage.", "reporter": "RiverLiu", "assignee": "Zhiwei", "resolution": "", "root_cause": "", "state": "open"}

Result: 1004{"issue_number": 1004, "issue_descrption": "Analyze Triton kernels data and report to Triton XPU.\n\n1. Recollect reasonable competitive GPU performance data\n2. Use TorchInductor built-in benchmark tool to detect slower XPU triton kernels.", "reporter": "RiverLiu", "assignee": "Mao Yunfei", "resolution": "The issue is still open and no resolution has been provided yet.", "root_cause": "The root cause of the issue is not explicitly mentioned in the provided information. However, the comments suggest that there are performance issues related to layout optimization, atomic operations, and specific operations like scatter, RNN, SDPA, and pad mm.", "state": "open"}

Result: 986{"issue_number": 986, "issue_descrption": "### 🚀 The feature, motivation and pitch\n\n```[rank0]:   File \"/home/lxxx/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/parallel/distributed.py\", line 825, in __init__\n[rank0]:     _verify_param_shape_across_processes(self.process_group, parameters)\n[rank0]:   File \"/home/lxxx/miniconda3/envs/torch/lib/python3.11/site-packages/torch/distributed/utils.py\", line 288, in _verify_param_shape_across_processes\n[rank0]:     return dist._verify_params_across_processes(process_group, tensors, logger)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: NotImplementedError: The operator 'c10d::allgather_' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XP\n```\n\nDescription:\nWhen attempting to use Distributed Data Parallel (DDP) training on an Intel Arc A770 GPU, I encountered a NotImplementedError indicating that the `c10d::allgather_` operator is not currently implemented for the XPU device. This operator is crucial for multi-GPU training scenarios.\n\nSteps to reproduce:\n1. Set up a PyTorch environment with XPU support\n2. Attempt to initialize a DistributedDataParallel model\n3. Run the training script\n\nFeature request:\nPlease implement the `c10d::allgather_` operator for the XPU device to enable proper functionality of Distributed Data Parallel training on Intel GPUs.\n\nAlso c10d::allreduce_\n\nThank you for your attention to this matter.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_", "reporter": "Zhiyuan Li", "assignee": "Chao Han", "resolution": "The `c1

Result: 979{"issue_number": 979, "issue_descrption": "The issue is related to the random failure of accuracy in the timm jx_nest_base model when using amp_fp16 inference on the XPU device. The failure occurs intermittently, with most runs passing but one failing. The issue has been observed in multiple test runs, and it is suspected to be related to the non-deterministic behavior of Triton, which does not support deterministic operations. The root cause is identified as the non-deterministic behavior after the average pooling operation, which is optimized into a single aten.mean operation by the inductor. However, the issue cannot be reproduced using unit tests, and it is suggested to accept the variance.", "reporter": "None", "assignee": "None", "resolution": "The issue is suggested to be accepted due to the non-deterministic behavior of Triton and the inability to reproduce the issue using unit tests.", "root_cause": "The root cause is the non-deterministic behavior after the average pooling operation, which is optimized into a single aten.mean operation by the inductor. This non-determinism is suspected to be related to the limitations of Triton in supporting deterministic operations.", "state": "open"}

Result: 970{"issue_number": 970, "issue_descrption": "CPU time as below,\n```\noverride             aten::sum         2.52%      24.765ms         4.07%      39.978ms      60.849us \nnon-override         aten::sum         4.49%      46.832ms         5.74%      59.905ms      91.180us\n```,", "reporter": "Feng Yuan", "assignee": "majing", "resolution": "Same root cause, see https://github.com/intel/llvm/issues/15824", "root_cause": "The root cause is related to the issue mentioned in https://github.com/intel/llvm/issues/15824.", "state": "open"}

Result: 969{"issue_number": 969, "issue_descrption": "Content of #969: { ### 🐛 Describe the bug\n\n``` \n                              Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg      Self XPU    Self XPU %     XPU total  XPU time avg    # of Calls		 \nnon override         aten::nonzero         5.50%      63.456ms        54.60%     630.302ms     489.365us       5.160ms         4.35%      34.468ms      26.761us          1288  \noverride             aten::nonzero         5.40%      58.551ms        52.64%     570.870ms     443.222us       6.688ms         5.55%      34.737ms      26.970us          1288\n```\n\n### Versions\nLatest torch-xpu-ops vs IPEX 2.3 implementation. ", "reporter": "Feng Yuan", "assignee": "majing", "resolution": "The low performance is caused by SYCL API, which we used to query kernel specific max work group size. We filed an issue to the compiler to track this issue. [Link to compiler issue](https://github.com/intel/llvm/issues/15824)", "root_cause": "The root cause of the performance issue is the SYCL API used to query the maximum work group size for the kernel, which introduces overhead.", "state": "open"}

Result: 964{"issue_number": 964, "issue_descrption": "We need to expand our testing scope as much as possible due to the current requirement of 80% coverage for CUDA.", "reporter": "Yutao Xu", "assignee": "Daisy Deng", "resolution": "", "root_cause": "", "state": "open"}

Result: 954{"issue_number": 954, "issue_descrption": "### 🚀 The feature, motivation and pitch\n\n- https://github.com/intel/torch-xpu-ops/blob/main/cmake/BuildFlags.cmake#L25\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_", "reporter": "Allen Guo", "assignee": "Feng Yuan", "resolution": "The issue cannot be resolved by simply enabling Clang as the host compiler due to duplicate symbol issues with `torchgen.gen` headers in `torch-xpu-ops` and `pytorch`. Additionally, `torch-xpu-ops` is incompatible with the lld linker, which also reports duplicate symbols.", "root_cause": "The root cause of the issue is the presence of duplicate symbols in the `torchgen.gen` headers between `torch-xpu-ops` and `pytorch`, which causes Clang to fail. Furthermore, `torch-xpu-ops` is incompatible with the lld linker, leading to duplicate symbol errors.", "state": "closed"}

Result: 939{"issue_number": 939, "issue_descrption": "Comparing with oneDNN impl, torch-xpu-ops gets gap on the performance of UpsampleBilinear forward kernel and backward kernel.", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "We have implemented channels last kernel, which is on-par with cuda. But the performance still has gap with oneDNN. We will low prioritize performance optimize for oneDNN goal", "root_cause": "The performance gap between torch-xpu-ops and oneDNN for UpsampleBilinear forward and backward kernels is due to the current implementation not being optimized to match oneDNN's performance.", "state": "closed"}

Result: 937{"issue_number": 937, "issue_descrption": "### 🚀 The feature, motivation and pitch\n\nBasing on the consideration of accuracy, we followed the PyTorch CUDA implementation, using Welford algorithm and similar kernel template. Will improve the kernel template with vectorized load/store.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "Merged", "root_cause": "", "state": "closed"}

Result: 877{"issue_number": 877, "issue_descrption": "improve test coverage", "reporter": "daisyden", "assignee": "daisyden", "resolution": "The task has been completed as per the comments. The ops nn.functional.conv1d, nn.functional.conv2d, nn.functional.conv3d, nn.functional.conv_transpose1d, nn.functional.conv_transpose2d, nn.functional.conv_transpose3d, addmm, nn.functional.linear, and tensordot have been added to the extended UT. Some BLAS and LAPACK operations were not included in the oplist.", "root_cause": "The initial issue was to improve test coverage by adding more operations to the extended unit tests. The root cause was the lack of comprehensive test coverage for certain convolution and matrix operations.", "state": "closed"}

Result: 803{"issue_number": 803, "issue_descrption": "For more details, please refer to https://jira.devtools.intel.com/browse/PYTORCHDGQ-5072?filter=-2. For LayoutLMForSequenceClassification model on stock pytorch, div cost time on pvc-1100 worse than A100 * ratio.", "reporter": "Xiao, Wang", "assignee": "Feng Yuan", "resolution": "", "root_cause": "", "state": "open"}

Result: 795{"issue_number": 795, "issue_descrption": "For more details, please refer to https://jira.devtools.intel.com/browse/PYTORCHDGQ-5017?filter=-2. For T5Small model on stock pytorch, inplace add cost time on pvc-1100 worse than A100 * ratio.", "reporter": "Xiao, Wang", "assignee": "None", "resolution": "The perf gap can be reproduced by ut and it's pending for kernel optimization.", "root_cause": "xpu performance is not targeted to PT 2.6", "state": "open"}

Result: 789{"issue_number": 789, "issue_descrption": "For more details, please refer to https://jira.devtools.intel.com/browse/PYTORCHDGQ-5048?filter=-2. \n\n### 🐛 Describe the bug\nFor AllenaiLongformerBase model on stock pytorch, copy cost time on pvc-1100 worse than A100 * ratio", "reporter": "Xiao, Wang", "assignee": "Feng Yuan", "resolution": "", "root_cause": "", "state": "open"}

Result: 781{"issue_number": 781, "issue_descrption": "Seems both cpu and xpu result are questionable. the output's imag is expected to be \u201c-1.0020e+23\u201d but got 1.0020e+23 on cpu, while the xpu result imag is -inf.", "reporter": "Daisy Deng", "assignee": "Daisy Deng", "resolution": "This issue is related to the implementation of the compiler. It has been pushed to version 2.8.", "root_cause": "The discrepancy in results is due to the implementation of the compiler.", "state": "closed"}

Result: 774 failed to extract

Result: 772{"issue_number": 772, "issue_descrption": "Need quantization support, NotImplementedError: Could not run 'aten::_empty_affine_quantized' with arguments from the 'QuantizedXPU' backend. cases: test_view_ops_xpu.py::TestOldViewOpsXPU::test_flatten_xpu test_view_ops_xpu.py::TestOldViewOpsXPU::test_ravel_xpu", "reporter": "Cheng, Penghui", "assignee": "Feng Yuan", "resolution": "The issue has been lowered in priority as the usage of QuantizedXPU is for PyTorch legacy quantization solution, which is not being followed up. The Tensor with QuantizedXPU dispatch key implies quantization information, but in other quantization solutions, scale and shift are introduced in separate Tensors. Operator API or graph will introduce the scale and shift Tensors.", "root_cause": "NotImplementedError: Could not run 'aten::_empty_affine_quantized' with arguments from the 'QuantizedXPU' backend due to lack of support for QuantizedXPU in the current implementation.", "state": "closed"}

Result: 761{"issue_number": 761, "issue_descrption": "### 🚀 The feature, motivation and pitch 1. NotImplementedError: Could not run 'aten::_to_copy' with arguments from the 'NestedTensorXPU' backend cases: test_transformers.py::TestTransformersXPU::test_with_nested_tensor_input_xpu 2. We have no mechanism to handle SDPBackend::ERROR so far. Will give a fully support when we support all SDPBackends. cases: ", "reporter": "Cheng, Penghui", "assignee": "Feng Yuan", "resolution": "The issue depends on SDP implementation. We are evaluating a choice of XPU.", "root_cause": "The issue is related to the SDP implementation and the lack of a mechanism to handle SDPBackend::ERROR.", "state": "Evaluating"}

Result: 754 failed to extract

Result: 725{"issue_number": 725, "issue_descrption": "torchbench_amp_bf16_training xpu train detectron2_fcos_r_50_fpn Traceback (most recent call last): File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run ) = runner.load_model( File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 302, in load_model benchmark = benchmark_cls( File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__ obj = type.__call__(cls, *args, **kwargs) File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/detectron2_fcos_r_50_fpn/__init__.py\", line 15, in __init__ super().__init__(variant=\"COCO-Detection/fcos_R_50_FPN_1x.py\", test=test, device=device, File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/framework/detectron2/model_factory.py\", line 137, in __init__ raise NotImplementedError( NotImplementedError: FCOS train is not supported by upstream detectron2. See GH Issue: https://github.com/facebookresearch/detectron2/issues/4369. model_fail_to_load loading model: 0it [00:00, ?it/s] [W803 05:29:24.933151121 RegisterXPU.cpp:7580] Warning: Aten Op fallback from XPU to CPU happends. This may have performance implications. If need debug the fallback ops please set environment variable `PYTORCH_DEBUG_XPU_FALLBACK=1` (function operator()) loading model: 0it [00:13, ?it/s] Versions torch-xpu-ops: https://github.com/intel/torch-xpu-ops/commit/1d70431c072db889d9a47ea49560

Result: 719 failed to extract

Result: 711{"issue_number": 711, "issue_descrption": "### 🐛 Describe the bug\n\n- [ ] `resnet50_quantized_qat`\n- [ ] `mobilenet_v2_quantized_qat`\n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run\n    ) = runner.load_model(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 309, in load_model\n    benchmark = benchmark_cls(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__\n    obj = type.__call__(cls, *args, **kwargs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/resnet50_quantized_qat/__init__.py\", line 21, in __init__\n    raise NotImplementedError(\"The eval test only supports CPU.\")\nNotImplementedError: The eval test only supports CPU.\n\nmodel_fail_to_load\n\n### Versions\n\ntorch-xpu-ops: https://github.com/intel/torch-xpu-ops/commit/1d70431c072db889d9a47ea4956049fe340a426d\npytorch: d224857b3af5c9d5a3c7a48401475c09d90db296\ndevice: pvc 1100, bundle: 0.5.3, driver: 803.61", "reporter": "None", "assignee": "Mao Yunfei", "resolution": "The issue was resolved by addressing the error related to the unsupported operation on XPU. The error was due to the model's eval test only supporting CPU, which was causing a NotImplementedError. Additionally, a new error was encountered during the validation process, which was related to

Result: 685{"issue_number": 685, "issue_descrption": "It is a performance requirement. The existing CUDA implementation in PyTorch supports data type dynamic cast, so that there won't be an extra kernel to align data types of input and output.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "Not an urgent case, as the usage is rare. Lower the priority.", "root_cause": "The usage of the feature is rare, leading to a lower priority.", "state": "closed"}

Result: 632{"issue_number": 632, "issue_descrption": "The issue is related to a pattern of operations involving convolution, ReLU, and adaptive average pooling in the backward pass. The problem is difficult to reproduce in a unit test, but the root cause has been identified and a fix is being prepared.", "reporter": "Mao Yunfei", "assignee": "Mao Yunfei", "resolution": "The issue was resolved by reverting a specific PyTorch PR (https://github.com/pytorch/pytorch/pull/84541) after deciding to drop the block format solution.", "root_cause": "The root cause of the issue is related to a pattern of operations involving convolution, ReLU, and adaptive average pooling in the backward pass. The exact nature of the problem is not detailed, but it is noted that it is challenging to reproduce in a unit test.", "state": "closed"}

Result: 623{"issue_number": 623, "issue_descrption": "### 🐛 Describe the bug\n\nFailure case: test_nextafter_bfloat16_xpu_bfloat16.\nhttps://github.com/pytorch/pytorch/blob/c2425a3b572328c6c1fdadc080f8a83c6357f945/test/test_binary_ufuncs.py#L2949\n\nWe aligned CPU and CUDA implementation by using `std::nextafter`. But got failure,\n```\nAssertionError: Scalars are not equal!\n\nExpected 9.183549615799121e-41 but got 0.0.\nAbsolute difference: 9.183549615799121e-41\nRelative difference: 1.0\n```\n\n### Versions\n\nhttps://github.com/intel/torch-xpu-ops/pull/557", "reporter": "Feng Yuan", "assignee": "Daisy Deng", "resolution": "", "root_cause": "", "state": "open"}

Result: 618{"issue_number": 618, "issue_descrption": "### 🚀 The feature, motivation and pitch\n\n1. module 'torch._C' has no attribute '_scatter'\ncases:\nTestAutograd.test_checkpointing_without_reentrant_dataparallel,\nTestMultithreadAutograd.test_dataparallel_saved_tensors_hooks\n\n2. AttributeError: module 'torch.xpu' has no attribute------passed on latest version\ncases:\nTestAutograd.test_graph_save_on_cpu_cuda, -----passed\nTestAutograd.test_checkpointing_without_reentrant_memory_savings,-----passed\n\n3. NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend.\ncases:\ntest_sparse_mask_autograd_xpu\ntest_sparse_ctor_getter_backward_xpu_float64\ntest_sparse_ctor_getter_backward_xpu_complex128\ntest_sparse_backward_xpu_float64\ntest_sparse_backward_xpu_complex128\n\n4. c10::NotImplementedError\ncases:\nTestAutogradMultipleDispatchXPU::test_autograd_multiple_dispatch_registrations_xpu\n\n5. RuntimeError: Double and complex datatype matmul is not supported in oneDNN\ncases:\ntest_autograd_xpu.py::TestAutogradDeviceTypeXPU::test_mv_grad_stride_0_xpu\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_", "reporter": "Cheng, Penghui", "assignee": "Feng Yuan", "resolution": "The issues are categorized as follows:\n\n1. **Not an XPU specific issue**: The 'torch._C' module does not have the '_scatter' attribute, which is related to c10d and not specific to XPU.\n2. **Not operator related**: The AttributeError for 'torch.xpu' is not related to the operators in torch-xpu-ops.\n3. **Sparse related feature**: The NotImplementedError for sparse tensor operations is related to sparse tensor support, which is on-demand and follows requirements from models like HF/TIMM/TB.\n4. **Not an operator implementation issue**: The c10::NotImplementedError is not related to operator implementation.\n5. **OneDNN issues**: The RuntimeError for double

Result: 544{"issue_number": 544, "issue_descrption": "Got numerical difference compared with CPU results. It is hard to say who is better on accuracy.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "Not a critical error. Low priority.", "root_cause": "It should be the definition of the compiler behavior.", "state": "closed"}

Result: 506{"issue_number": 506, "issue_descrption": "torchbench_bfloat16_training xpu train demucs Traceback (most recent call last): File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2294, in validate_model self.model_iter_fn(model, example_inputs) File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 456, in forward_and_backward_pass pred = mod(*cloned_inputs) File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1566, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1575, in _call_impl return forward_call(*args, **kwargs) File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/demucs/__init__.py\", line 29, in forward return sources, self.model(mix) File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1566, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1575, in _call_impl return forward_call(*args, **kwargs) File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/demucs/demucs/model.py\", line 204, in forward x = encode(x) File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1566, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/home/sdp/miniforge3/envs/e2

Result: 493{"issue_number": 493, "issue_descrption": "torchbench_amp_fp16_training xpu train timm_regnet E0626 18:18:36.100000 139652021139264 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.00227, (ref-fp64): 0.00064 and shape=torch.Size([]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000 fail_accuracy float16 E0626 13:14:09.343000 139963949791040 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.00150, (ref-fp64): 0.00032 and shape=torch.Size([224]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000 E0626 13:14:09.343000 139963949791040 torch/_dynamo/utils.py:1392] Accuracy failed for key name s3.b4.se.fc1.bias.grad fail_accuracy", "reporter": "None", "assignee": "None", "resolution": "The issue was resolved by increasing the tolerance to 1e-2, as suggested by @retonym. Additionally, a public PR was created to raise the tolerance: https://github.com/pytorch/pytorch/pull/134192. However, the float16 training still fails, and it was found that the issue could be resolved by falling back BatchNorm to eager mode. Another PR was also created to address this: https://github.com/pytorch/pytorch/pull/144756.", "root_cause": "The root cause of the issue was the accuracy failure due to the RMSE exceeding the tolerance level for both float32

Result: 492{"issue_number": 492, "issue_descrption": "torchbench_amp_fp16_training xpu train timm_efficientdet Traceback (most recent call last): File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4177, in run ) = runner.load_model( File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 320, in load_model benchmark = benchmark_cls( File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__ obj = type.__call__(cls, *args, **kwargs) File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/timm_efficientdet/__init__.py\", line 55, in __init__ raise NotImplementedError(\"The original model code forces the use of CUDA.\") NotImplementedError: The original model code forces the use of CUDA. model_fail_to_load", "reporter": "None", "assignee": "Weishi.Deng", "resolution": "The issue was addressed with a PR to the benchmark repository (https://github.com/pytorch/benchmark/pull/2374), but the fix requires long-term effort and is not targeted for PyTorch 2.6.", "root_cause": "The original model code in the efficientdet-pytorch repository contains hard-coded CUDA dependencies, which prevent the model from running on XPU.", "state": "closed"}

Result: 489{"issue_number": 489, "issue_descrption": "torchbench_amp_fp16_training xpu train moco Traceback (most recent call last): File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4177, in run ) = runner.load_model( File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 320, in load_model benchmark = benchmark_cls( File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__ obj = type.__call__(cls, *args, **kwargs) File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/moco/__init__.py\", line 80, in __init__ raise NotImplementedError(f\"{device} not supported\") NotImplementedError: xpu not supported model_fail_to_load", "reporter": "None", "assignee": "Weishi.Deng", "resolution": "The fix requires long term effort, not target to PT 2.6", "root_cause": "The model requests the support for DDP models, which is still in progress.", "state": "open"}

Result: 461{"issue_number": 461, "issue_descrption": "FP8 data types are turned on in index put cases. PT2.5 plan of XPU implementation does not include FP8 support. Skip them temporarily.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "The FP8 data types support is not included in the PT2.5 plan for XPU implementation. The index put cases involving FP8 data types are being skipped temporarily.", "root_cause": "The PT2.5 plan for XPU implementation does not include support for FP8 data types.", "state": "open"}

Result: 432 failed to extract

Result: 426 failed to extract

Result: 348{"issue_number": 348, "issue_descrption": "### 🚀 The feature, motivation and pitch 1. torch.backends.mkldnn.flags() got an unexpected keyword argument 'deterministic'---fixed cases: ```python test/xpu/nn/test_convolution_xpu.py::TestConvolutionNNDeviceTypeXPU::test_conv_double_backward_xpu_float64 ``` 2. Tensor-likes are not close! -----fixed cases: ``` TestConvolutionNNDeviceTypeXPU.test_Conv2d_depthwise_naive_groups_xpu_float16 TestConvolutionNNDeviceTypeXPU.test_Conv3d_depthwise_naive_groups_xpu_float16 3. check_random_bounds handles only integral, floating-point and boolean types---fixed cases: ``` TestConvolutionNNDeviceTypeXPU.test_conv_cudnn_nhwc_xpu_complex64 ``` 4. largeTensorTest didn't support XPU device ---fixed cases: ``` TestConvolutionNNDeviceTypeXPU::test_conv_large_xpu TestConvolutionNNDeviceTypeXPU::test_conv_large_nosplit_xpu TestConvolutionNNDeviceTypeXPU::test_conv_large_batch_1_xpu TestConvolutionNNDeviceTypeXPU::test_conv3d_64bit_indexing_xpu TestConvolutionNNDeviceTypeXPU::test_conv3d_large_batch_1_xpu TestConvolutionNNDeviceTypeXPU::test_conv_transposed_large_xpu ``` 5. NotImplementedError: The operators ", "reporter": "Cheng, Penghui", "assignee": "Zhiwei", "resolution": "The issues 1, 2, 3, and 4 have been fixed. Issue 5 is not part of the 2.5 plan and is tracked in a separate PR (https://github.com/pytorch/pytorch/pull/118064).", "root_cause": "The root cause of the issues was due to the lack of support for certain operations and data types on the XPU device, as well as the unexpected keyword argument in the torch.backends.mkldnn.flags() function.", "state": "closed"}

Result: 322{"issue_number": 322, "issue_descrption": "No failed, but some skips\n\n# FP8 is only supported on H100+ and sm_89 and MI300+ devices\n\nTestFP8MatmulCudaXPU::test_float32_output_errors_with_bias_xpu\nTestFP8MatmulCudaXPU::test_float8_basics_xpu\nTestFP8MatmulCudaXPU::test_float8_bias_relu_edgecase_xpu\nTestFP8MatmulCudaXPU::test_float8_bias_xpu\nTestFP8MatmulCudaXPU::test_float8_scale_fast_accum_xpu\nTestFP8MatmulCudaXPU::test_float8_scale_xpu\nTestFP8MatmulCudaXPU::test_non_divisible_leading_dim_bias_False_xpu\nTestFP8MatmulCudaXPU::test_non_divisible_leading_dim_bias_True_xpu\nTestFP8MatmulCudaXPU::test_scaled_mm_vs_emulated_bfloat16_xpu\nTestFP8MatmulCudaXPU::test_scaled_mm_vs_emulated_float16_xpu\nTestFP8MatmulCudaXPU::test_scaled_mm_vs_emulated_float32_xpu\n\n# mixed dtypes linear only supported on SM 8.x\n\nTestMixedDtypesLinearCudaXPU::test_mixed_dtypes_linear_xpu_bfloat16\nTestMixedDtypesLinearCudaXPU::test_mixed_dtypes_linear_xpu_float16", "reporter": "None", "assignee": "Feng Yuan", "resolution": "FP8 support is being addressed with low priority. The remaining failures are related to FP8 matmul and are not supported on the current hardware.", "root_cause": "FP8 matmul operations are not supported on the current hardware (Intel Xeon Platinum 8480+), which does not meet the minimum requirements of H100+ and sm_89 and MI300+ devices. Additionally, mixed dtypes linear operations are only supported on SM 8.x devices.", "state": "Open"}

Result: 275{"issue_number": 275, "issue_descrption": "Support of Quantization // quantized op issues in test_shape_ops_xpu.py", "reporter": "daisyden", "assignee": "fengyuan", "resolution": "modified for PT2.6", "root_cause": "NotImplementedError: Could not run 'aten::empty_quantized' with arguments from the 'QuantizedXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build).", "state": "closed"}

Result: 275{"issue_number": 275, "issue_descrption": "Support of Quantization // quantized op issues in test_shape_ops_xpu.py", "reporter": "Daisy Deng", "assignee": "Feng Yuan", "resolution": "Not provided in the given information.", "root_cause": "Not provided in the given information.", "state": "open"}

Result: 264{"issue_number": 264, "issue_descrption": "### 🐛 Describe the bug\n\n1. torch.random.fork_rng(devices=rng_device) does not support XPU backend\nTestRandomTensorCreationXPU.test_randperm_xpu\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/3ffa121d-749c-4dc8-822d-da9fb088b546) - PASSED\n\n2. torch.xpu.FloatTensor is not supported. so tests like test_tensor_factory_gpu_type_inference and test_constructor_dtype had to be disabled. - PASSED\n\n3. multiple device seems not supported, when I used ZE_AFFINITY_MASK to use tile 0 and tile 1, seems still only xpu:0 is detected. - PASSED\n\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/ad1e0bd9-823f-45f1-90b1-ce833ae6443e)\n\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/3b3f7b8e-86d2-4ff8-8214-0b4e05bbf52b)\n\nThis makes test_copy_from_tensor_mult_devices and test_copy_from_dlpack_mult_devices cannot work.\n\n4.  RuntimeError: \"eq_xpu\" not implemented for 'UInt16' in case  TestTensorCreationXPU.test_cat_out_fast_path_dim0_dim1_xpu_uint16, the same apply to TestTensorCreationXPU.test_cat_out_fast_path_dim0_dim1_xpu_uint32 and TestTensorCreationXPU.test_cat_out_fast_path_dim0_dim1_xpu_uint64\n- PASSED\n", "reporter": "Daisy Deng", "assignee": "Daisy Deng", "resolution": "The issue was partially resolved. The `eq` and `ne` operations for unsigned int data types were fixed by @fengyuan14. However, the issue remains open for the following reasons:\n- `test_kaiser_window

Result: 264{"issue_number": 264, "issue_descrption": "To reproduce use branch daisyden/test_tensor_creation_ops, for example:\ncd test/xpu\nPYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test_tensor_creation_ops_xpu.py -k 'test_cat_out_fast_path_dim0_dim1_xpu_uint16'", "reporter": "Daisy Deng", "assignee": "Daisy Deng", "resolution": "Fixed an issue in `eq` and `ne`. Unsigned int data types are not supported in `eq` and `ne`. Keep the issue open.", "root_cause": "Unsigned int data types are not supported in `eq` and `ne`.", "state": "open"}

Result: 264{"issue_number": 264, "issue_descrption": "test_tensor_creation_ops.py has issues", "reporter": "Daisy Deng", "assignee": "Daisy Deng", "resolution": "Fixed an issue in `eq` and `ne`. Unsigned int data types are not supported in `eq` and `ne`. The modification of DispatchStub is filed at https://github.com/pytorch/pytorch/pull/126977/files#diff-54c373491da67eb31c3777457d7b043a49dd3966412edfd928ffd2013e4d6a54. test_kaiser_window_xpu: DispatchStub support. It is a composite operator. But it is implemented by DispatchStub. XPU doesn't support DispatchStub.", "root_cause": "Unsigned int data types are not supported in `eq` and `ne`. DispatchStub is not supported in XPU.", "state": "Open"}

Result: 253 failed to extract

Result: 208{"issue_number": 208, "issue_descrption": "Some utility functions of ATen operator level are operator semantics related and could be shared among different backends. In existing implementation, some of them are implemented in cpp file, could not be reused. So we will review existing torch-xpu-ops implementation, find them out and submit a PR to stock PyTorch to make them shared among like CPU, CUDA and XPU.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "open"}

Result: 184{"issue_number": 184, "issue_descrption": "I extended the fine-grained test to run all the xpu support ops and dtypes with test_compare_cpu() test. Please see branch daisyden/fin_grain. To run it with command:\n\n```bash\nexport PYTORCH_TEST_WITH_SLOW=1\nbash run_fin_grain.sh\n```\n\nI got the following failures in the end.\n\n- test_compare_cpu tanh complex support has accuracy gap.\n\n- bfloat16 accuracy gap in rsqrt, sub, rounding, cumsum, add, rsub, \n\n- float16 cumsum accuracy gap\n\n- pow, mul, log, complex64 got nan\n\n- index_put , index_add with bool\n\n- rounding float36 got inf\n\n- XPU reports \"not implemented\" with dtype bool, int*, uint8, while CPU would not report such error message.", "reporter": "Daisy Deng", "assignee": "Huaiyu, Zheng", "resolution": "The issues have been identified and some of them are being tracked in JIRA tickets. The test_compare_cpu tanh complex support has an accuracy gap, which is being tracked in JIRA tickets CMPLRLLVM-60397 and CMPLRLIBS-34974. The bfloat16 accuracy gaps in rsqrt, sub, rounding, cumsum, add, rsub, float16 cumsum accuracy gap, pow, mul, log, complex64 got nan, index_put, index_add with bool, rounding float36 got inf, and XPU reports \"not implemented\" with dtype bool, int*, uint8, while CPU would not report such error message are still under investigation.", "root_cause": "The root cause of the issues is a combination of accuracy gaps in certain operations and data types, and the XPU not implementing certain operations for specific data types. The accuracy gaps are likely due to differences in the underlying hardware and software implementations between XPU and CPU/CUDA, while the \"not implemented\" errors are due to the XPU not supporting certain operations for specific data types.", "state": "Open"}

Result: 157 failed to extract

Result: 146{"issue_number": 146, "issue_descrption": "Evaluate register spill in SYCL kernel", "reporter": "Feng Yuan", "assignee": "Feng Yuan", "resolution": "It is the last mile of performance. When we start to pursue peak performance, we should deep dive for it. So far, it is a low priority task.", "root_cause": "From SYCL compiler log, we find some warnings about register spill. Need evaluation.", "state": "open"}

Result: 135{"issue_number": 135, "issue_descrption": "We followed stock CUDA about grid and block configurations. For these configurations, stock CUDA has some NV GPU arch assumption. Even we followed similar configurations, we are not clear about what we can get from Xe arch.\n\n1. syclMaxWorkItemsPerEU: What we can get from Xe arch, when using it.\n2. syclMaxWorkItemsPerTile: We are using max sub-group size to deduce max number of work items per Tile. It is not accurate. When runtime chooses non-max-sub-group-size kernel (IGC's optimization), we might get insufficient occupancy.\n\nhttps://github.com/intel/torch-xpu-ops/blob/e914ada988343c0515753360de68812ea42d0ec3/src/aten/sycl/Loops.h#L330\n```int wg_sz = syclMaxWorkItemsPerEU();\nint num_wg = ceil_div<int>(N, wg_sz);\nint hw_max_num_wg = syclMaxWorkItemsPerTile() / wg_sz;\nnum_wg = num_wg > hw_max_num_wg ? hw_max_num_wg : num_wg;\nsycl_kernel_submit(wg_sz * num_wg, wg_sz, getCurrentSYCLQueue(), ker);\n```\n\n### Alternatives\nWe won't regard it as highest priority. We will discuss it when we need contribute SYCL kernels to in-tree, since,\n1. Limited GPU hardware.\n2. No performance exception on IPEX so far.\n\n### Additional context\n_No response_", "reporter": "fengyuan14", "assignee": "yutaoxu", "resolution": "", "root_cause": "The issue arises from the assumption of explicit scaling GPU resources when using `syclMaxWorkItemsPerTile`. The current approach of using max sub-group size to deduce the max number of work items per Tile is not accurate, leading to potential insufficient occupancy when the runtime chooses non-max-sub-group-size kernels (IGC's optimization).", "state": "open"}

Result: 126{"issue_number": 126, "issue_descrption": "Evaluate non-alignment of kernel implementation between IPEX and stock CUDA", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "", "root_cause": "The non-alignment of kernel implementation between IPEX and stock CUDA is due to several reasons: 1. Functionality extension was added in stock CUDA implementation, but there has been no sustainable rebase. 2. General memory layout support was added in stock CUDA implementation, but there has been no sustainable rebase. 3. Specific performance-oriented implementations exist in IPEX, but stock CUDA does not address these cases.", "state": "open"}
