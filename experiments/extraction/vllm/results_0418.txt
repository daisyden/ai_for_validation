


### Merged Result:1581{"issue_number": 1581, "issue_description": "Fatal Python error: Segmentation fault\n\ncases:\ntest/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_all_gather\ntest/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_all_gather_args_match\n\nlog:\n[test_inductor_collectives.log](https://github.com/user-attachments/files/19772103/test_inductor_collectives.log)", "reporter": "PenghuiCheng", "assignee": "zhangxiaoli73", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1577{"issue_number": 1577, "issue_description": "Here are the accuracy fail issues with known reasons that we won't fix recently, skip list of the models that will be skipped for accuracy check. ", "reporter": "jianyizh", "assignee": "jianyizh", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1576{"issue_number": 1576, "issue_description": "The operator 'aten::_conv_depthwise2d' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators.", "reporter": "mengfei25", "assignee": "ZhiweiYan-96", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1575{"issue_number": 1575, "issue_description": "An error occurred when executing the following command\uff1a\n\n```bash\nPYTORCH_OPINFO_SAMPLE_INPUT_INDEX=14 PYTORCH_TEST_WITH_SLOW=1 python ../../test/test_ops.py TestCommonXPU.test_noncontiguous_samples_nn_functional_conv2d_xpu_complex64\n```", "reporter": "xytintel", "assignee": "ZhiweiYan-96", "resolution": "", "root_cause": "NotImplementedError: Could not run 'aten::_conv_depthwise2d' with arguments from the 'CPU' backend.", "state": "open"}

### Merged Result:1574{"issue_number": 1574, "issue_description": "The operator 'aten::_grouped_mm' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues.\nFP8 `grouped_mm` is targeted at v2.10", "reporter": "githubsgi", "assignee": "ZhiweiYan-96", "resolution": "\n", "root_cause": "FP8 `grouped_mm` is targeted at v2.10", "state": "open"}

### Merged Result:1572{"issue_number": 1572, "issue_description": "Unit test test.distributed._composable.fsdp.test_fully_shard_state_dict.TestFullyShardStateDictMultiProcess | test_dp_state_dict_cpu_offload got assertion:\n-\nAssertionError: \"Found following parameters on non-CPU device: \\[\\('0.weight', device\\(type={device_type\" does not match \"FSDP parameters should be materialized on CPU when enabling CPU offloading. For example, load a CPU state dict or call module.to_empty(device=\"cpu\"). Found following parameters on non-CPU device: [('0.weight', device(type='xpu', index=7))]\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1572. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "It seems you have not enabled this test correctly. Please check with developer branch and fix.", "state": "open"}

### Merged Result:1571{"issue_number": 1571, "issue_description": "Cannot use ReduceOp.PREMUL_SUM with XCCL\nKnown feature gap not a bug. Pending on oneCCL support.", "reporter": "daisyden", "assignee": "zhangxiaoli73", "resolution": "\n", "root_cause": "oneCCL support", "state": "open"}

### Merged Result:1569{"issue_number": 1569, "issue_description": "RuntimeError: output 0: meta disagrees with real impl: aten.norm.ScalarOpt_dim(tensor(..., device='meta', size=(5, 5), dtype=torch.bfloat16) stride=(5, 1), 2.0, [],) = (tensor(..., device='meta', size=(5, 5), dtype=torch.bfloat16) stride=(5, 1)) for element 0, was torch.Size([5, 5]) but real shape was torch.Size([])", "reporter": "PenghuiCheng", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "open"}### Result:1565 failed to extract### Result:1561 failed to extract### Result:1559 failed to extract### Result:1556 failed to extract

### Merged Result:1581{"issue_number": 1581, "issue_description": "Fatal Python error: Segmentation fault\n\ncases:\ntest/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_all_gather\ntest/distributed/test_inductor_collectives.py::TestCollectivesInductor::test_dynamo_rewrite_dist_all_gather_args_match\n\nlog:\n[test_inductor_collectives.log](https://github.com/user-attachments/files/19772103/test_inductor_collectives.log)", "reporter": "PenghuiCheng", "assignee": "zhangxiaoli73", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1577{"issue_number": 1577, "issue_description": "Here are the accuracy fail issues with known reasons that we won't fix recently, skip list of the models that will be skipped for accuracy check. ", "reporter": "jianyizh", "assignee": "jianyizh", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1576{"issue_number": 1576, "issue_description": "The operator 'aten::_conv_depthwise2d' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators.", "reporter": "mengfei25", "assignee": "ZhiweiYan-96", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1575{"issue_number": 1575, "issue_description": "An error occurred when executing the following command\uff1a\n\n```bash\nPYTORCH_OPINFO_SAMPLE_INPUT_INDEX=14 PYTORCH_TEST_WITH_SLOW=1 python ../../test/test_ops.py TestCommonXPU.test_noncontiguous_samples_nn_functional_conv2d_xpu_complex64\n```", "reporter": "xytintel", "assignee": "ZhiweiYan-96", "resolution": "", "root_cause": "NotImplementedError: Could not run 'aten::_conv_depthwise2d' with arguments from the 'CPU' backend.", "state": "open"}

### Merged Result:1574{"issue_number": 1574, "issue_description": "The operator 'aten::_grouped_mm' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues.\nFP8 `grouped_mm` is targeted at v2.10", "reporter": "githubsgi", "assignee": "ZhiweiYan-96", "resolution": "\n", "root_cause": "FP8 `grouped_mm` is targeted at v2.10", "state": "open"}

### Merged Result:1572{"issue_number": 1572, "issue_description": "Unit test test.distributed._composable.fsdp.test_fully_shard_state_dict.TestFullyShardStateDictMultiProcess | test_dp_state_dict_cpu_offload got assertion:\n-\nAssertionError: \"Found following parameters on non-CPU device: \\[\\('0.weight', device\\(type={device_type\" does not match \"FSDP parameters should be materialized on CPU when enabling CPU offloading. For example, load a CPU state dict or call module.to_empty(device=\"cpu\"). Found following parameters on non-CPU device: [('0.weight', device(type='xpu', index=7))]\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1572. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "It seems you have not enabled this test correctly. Please check with developer branch and fix.", "state": "open"}

### Merged Result:1571{"issue_number": 1571, "issue_description": "Cannot use ReduceOp.PREMUL_SUM with XCCL\nKnown feature gap not a bug. Pending on oneCCL support.", "reporter": "daisyden", "assignee": "zhangxiaoli73", "resolution": "\n", "root_cause": "oneCCL support", "state": "open"}

### Merged Result:1569{"issue_number": 1569, "issue_description": "RuntimeError: output 0: meta disagrees with real impl: aten.norm.ScalarOpt_dim(tensor(..., device='meta', size=(5, 5), dtype=torch.bfloat16) stride=(5, 1), 2.0, [],) = (tensor(..., device='meta', size=(5, 5), dtype=torch.bfloat16) stride=(5, 1)) for element 0, was torch.Size([5, 5]) but real shape was torch.Size([])", "reporter": "PenghuiCheng", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "open"}### Result:1565 failed to extract### Result:1561 failed to extract

### Merged Result:1559{"issue_number": 1559, "issue_description": "RuntimeError: oneCCL: coll_param.cpp:455 validate: EXCEPTION: average operation is not supported for the scheduler path\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1559. The reporter of the issue is PenghuiCheng, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "\nclosed", "root_cause": "duplicated with https://github.com/intel/torch-xpu-ops/issues/1508", "state": "closed"}

### Merged Result:1556{"issue_number": 1556, "issue_description": "Error:\nNotImplementedError: Operator aten._scaled_dot_product_fused_attention_overrideable.default does not have a sharding strategy registered.\ncases:\ntest/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_norm__output\n[tp_examples.log](https://github.com/user-attachments/files/19641847/tp_examples.log)\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1556. The reporter of the issue is PenghuiCheng, and the assignee is ashokei, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "ashokei", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:1555{"issue_number": 1555, "issue_description": "Error: RuntimeError: aten.add.Tensor: got mixed torch.Tensor and DTensor, need to convert all torch.Tensor to DTensor before calling distributed operators!\ncases:\ntest/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_all\ntest/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_layers_0_attention_wv__layers_0_feed_forward_w1__layers_0_ffn_norm__output__tok_embeddings\ntest/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_layers_1_ffn_norm__norm__output__tok_embeddings\ntest/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_norm__output__tok_embeddings\ntest/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_req_grad_seq_parallel_float32_thaw_output__tok_embeddings\ntest/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_training_is_seq_parallel_False_float32\ntest/distributed/tensor/parallel/test_tp_examples.py::DistTensorParallelExampleTest::test_transformer_training_is_seq_parallel_True_float32\n[tp_examples.log](https://github.com/user-attachments/files/19641835/tp_examples.log)\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1555. The reporter of the issue is PenghuiCheng, and the assignee is githubsgi, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "githubsgi", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:1554{"issue_number": 1554, "issue_description": "Encountered `PermissionError: [Errno 13] Permission denied` during multi-threaded compilation. This error occurs when multiple threads attempt to open the same file simultaneously using `with open(filename, \"w\")`. The file should be opened and written to correctly without permission errors. Use a mutex lock to ensure that only one thread can access the file at a time.", "reporter": "chunhuanMeng", "assignee": "chunhuanMeng", "resolution": "Use a mutex lock to ensure that only one thread can access the file at a time", "root_cause": "When multiple threads attempt to open the same file simultaneously using `with open(filename, \"w\")`, it leads to `PermissionError: [Errno 13] Permission denied` error.", "state": "open"}

### Merged Result:1551{"issue_number": 1551, "issue_description": "The operator 'symm_mem::fused_scaled_matmul_reduce_scatter' is not currently implemented for the XPU device. This error occurs in the following test cases: test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_0, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_2_scatter_dim_1, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_0, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_1, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_A_dims_3_scatter_dim_2, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_0, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_1, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_scaled_matmul_reduce_scatter_rowwise_scales_reshape_mm_reshape_scatter_dim_2. The error message is: NotImplementedError: The operator 'symm_mem::fused_scaled_matmul_reduce_scatter' is not currently implemented for the XPU device.", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1550{"issue_number": 1550, "issue_description": "The operator 'aten::_scaled_mm.out' is not currently implemented for the XPU device. This error occurs in the following test cases: test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_1_return_A_False, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_1_return_A_True, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_2_return_A_False, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_all_gather_scaled_matmul_A_dims_3_gather_dim_2_return_A_True.\ntorch-xpu-ops not support this op. Reproduce", "reporter": "PenghuiCheng", "assignee": "xytintel", "resolution": "\n", "root_cause": "torch-xpu-ops not support this op", "state": "open"}

### Merged Result:1549{"issue_number": 1549, "issue_description": "AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\n......'", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1548{"issue_number": 1548, "issue_description": "AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [2_inference]\n......'", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1547{"issue_number": 1547, "issue_description": "The operator 'symm_mem::fused_matmul_reduce_scatter' is not currently implemented for the XPU device. This error occurs in the following test cases: test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_dtensor_seq_par_shard_dim_0, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_dtensor_seq_par_shard_dim_1, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_0, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_1, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_0, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_1, test/distributed/tensor/parallel/test_micro_pipeline_tp.py::MicroPipelineTPTest::test_fuse_matmul_reduce_scatter_A_dims_3_scatter_dim_2. The error message is: NotImplementedError: The operator 'symm_mem::fused_matmul_reduce_scatter' is not currently implemented for the XPU device. The issue is open and the reporter is PenghuiCheng, the assignee is Chao1Han.", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "default", "root_cause": "The operator 'symm_mem::fused_matmul_reduce_scatter' is not currently implemented for the XPU device.", "state": "open"}

### Merged Result:1545{"issue_number": 1545, "issue_description": "import torchvision\n\n*** RuntimeError: register_fake(...): the operator torchvision::nms already has an DispatchKey::Meta implementation via a pre-existing torch.library or TORCH_LIBRARY registration. Please either remove that registration or don't call register_fake.\nThis issue is found in PyTorch 2.8 master branch with Meta prebuild torchvision. Need to build Torchvision by yourself and then run this workloads. This is a not a torch-xpu-ops bug.", "reporter": "githubsgi", "assignee": "githubsgi", "resolution": "\n", "root_cause": "This issue is found in PyTorch 2.8 master branch with Meta prebuild torchvision. Need to build Torchvision by yourself and then run this workloads.", "state": "closed"}### Result:1543 failed to extract

### Merged Result:1537{"issue_number": 1537, "issue_description": "With 2025.0 got accuracy gap when check optimizer state dict, PVC 1100, XELINK, 2 ranks. Traceback (most recent call last): File \"/home/sdp/mambaforge/envs/dist_2.7/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py\", line 632, in wrapper self._join_processes(fn) File \"/home/sdp/mambaforge/envs/dist_2.7/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py\", line 872, in _join_processes self._check_return_codes(elapsed_time) File \"/home/sdp/mambaforge/envs/dist_2.7/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py\", line 921, in _check_return_codes raise RuntimeError(error) RuntimeError: Process 0 exited with error code 10 and exception: Traceback (most recent call last): File \"/home/sdp/mambaforge/envs/dist_2.7/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py\", line 761, in run_test getattr(self, test_name)() File \"/home/sdp/mambaforge/envs/dist_2.7/lib/python3.10/site-packages/torch/testing/_internal/common_distributed.py\", line 634, in wrapper fn() File \"/home/sdp/mambaforge/envs/dist_2.7/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py\", line 3154, in wrapper method(*args, **kwargs) File \"/home/sdp/daisyden/dist_2.8_2025.0/test/distributed/fsdp/test_fsdp_optim_state.py\", line 921, in test_use_orig_params self.run_subtests( File \"/home/sdp/mambaforge/envs/dist_2.7/lib/python3.10/site-packages/torch/testing/_internal/common_fsdp.py\", line 1188, in run_subtests return run_subtests(self, *args, **kwargs) File \"/home/sdp/daisyden/dist_2.8_2025.0/test/distributed/fsdp/test_fsdp_optim_state.py\", line 1841, in _test_load_optim_state_with_optim_state_dict self._check_same_state( File \"/home/sdp/daisyden/dist_2.8_2025.0/test/distributed/fsdp/test_fsdp_optim_state.py\", line 470, in _check_same_state self.assertEqual(value, ref_value) File \"/home/sdp/mambaforge/envs/dist_2.7/lib/python3.10/site-packages/torch/testing/_internal/common_utils.py\", line 4095, in assertEqual raise error_metas.pop()[0].to_error( # type: ignore[index] AssertionError: Tensor-likes are not close! Mismatched elements: 9 / 9 (100.0%) Greatest absolute difference: 0.9033937454223633 at index (2,) (up to 1e-05 allowed) Greatest relative difference: 0.40552470088005066 at index (0,) (up to 1.3e-06 allowed) To execute this test, run the following from the base repo dir: PYTORCH_TEST_WITH_SLOW=1 python test/distributed/fsdp/test_fsdp_optim_state.py TestFSDPOptimState.test_use_orig_params\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1537. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\nDuplicated with https://github.com/intel/torch-xpu-ops/issues/1504 on 2025.1, so close this issue.", "root_cause": "The issue is related to the difference in the optimizer state dict between the original and the loaded state dict. The optimizer state dict is not being loaded correctly, leading to the accuracy gap. The root cause is not yet identified, but it is related to the version 2025.0 and the specific hardware configuration (PVC 1100, XELINK, 2 ranks). The issue is closed, but the root cause needs further investigation.", "state": "closed"}### Result:1536 failed to extract

### Merged Result:1535{"issue_number": 1535, "issue_description": "Process 0 terminated or timed out after 300.09047198295593 seconds\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1535. The reporter of the issue is PenghuiCheng, and the assignee is ratnampa, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "ratnampa", "resolution": "\n", "root_cause": "The issue is a kind of random timeout. There are more cases with such issue. The test could pass sometimes, but it is not a stable solution.", "state": "open"}

### Merged Result:1533{"issue_number": 1533, "issue_description": "pytorch build got permission issue for windows\nFixed", "reporter": "mengfei25", "assignee": "chunhuanMeng", "resolution": "\nFixed", "root_cause": "The issue is caused by a permission denied error when trying to write to a file. The error message indicates that the user does not have permission to write to the specified file path. This is a known issue and the reporter has closed the issue as it is not a critical problem.", "state": "closed"}

### Merged Result:1532{"issue_number": 1532, "issue_description": "The operator 'torchvision::deform_conv2d' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1532. The reporter of the issue is jerryzhou0624, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "jerryzhou0624", "assignee": "xytintel", "resolution": "closed\nThis kernel is in 2.7 release scope, please use 2.7 wheel.", "root_cause": "The operator 'torchvision::deform_conv2d' is not currently implemented for the XPU device.", "state": "closed"}### Result:1527 failed to extract

### Merged Result:1526{"issue_number": 1526, "issue_description": "RuntimeError: UR backend failed. UR backend returns:40 (UR_RESULT_ERROR_OUT_OF_RESOURCES)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1526. The reporter of the issue is PenghuiCheng, and the assignee is PenghuiCheng, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "\n", "root_cause": "A known issue from Intel triton https://github.com/intel/intel-xpu-backend-for-triton/issues/3641, it will always run on same device to cause unexpected behaviour.", "state": "open"}

### Merged Result:1525{"issue_number": 1525, "issue_description": "ValueError: trying to initialize the default process group twice!\n\nThis error occurs when trying to initialize the default process group twice in the test_c10d_functional_native.py file. The error is raised in the setUp method of the test_c10d_functional_native.py file, which is part of the test suite for distributed functionalities in PyTorch. The error is triggered when the setUp method is called, which is responsible for initializing the process group. The error message indicates that the default process group is being initialized more than once, which is not allowed. The error is reproducible in the test suite and affects multiple test cases.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1525. The reporter of the issue is PenghuiCheng, and the assignee is Chao1Han, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": "\n", "root_cause": "The error is caused by the fact that the process group is being initialized more than once, which is not allowed. This can happen if the setUp method is called more than once, or if the process group is being initialized in multiple places in the code.", "state": "open"}

### Merged Result:1521{"issue_number": 1521, "issue_description": "PyTorch Flex Attention  fails on XPU. The error message is AssertionError: Torch not compiled with CUDA enabled. The reporter of the issue is githubsgi, and the assignee is liangan1, and the state of the issue is open.\nFlexAttention is not enabled on XPU yet. we target to enable it on torch-2.8. The draft pr can be found https://github.com/pytorch/pytorch/pull/143553", "reporter": "githubsgi", "assignee": "liangan1", "resolution": "\n", "root_cause": "FlexAttention is not enabled on XPU yet", "state": "open"}

### Merged Result:1520{"issue_number": 1520, "issue_description": "Expected zero exit code but got -11 for pid: 2718941. Scalars are not equal! Expected 0 but got -11. Absolute difference: 11. Relative difference: inf. Expected zero exit code but got -11 for pid: 2718941. error cases: FAILED [15.9430s] test_c10d_functional_native.py::TestWithNCCL::test_all_gather_into_tensor_coalesced FAILED [16.9512s] test_c10d_functional_native.py::TestWithNCCL::test_all_gather_into_tensor_single FAILED [15.6386s] test_c10d_functional_native.py::TestWithNCCL::test_all_reduce_coalesced FAILED [16.5396s] test_c10d_functional_native.py::TestWithNCCL::test_all_reduce_coalesced_ FAILED [16.0305s] test_functional_api.py::TestCollectivesWithDistributedBackendXPU::test_all_gather_into_tensor_coalesced_xpu FAILED [16.1296s] test_functional_api.py::TestDistributedBackendCollectivesWithWorldSize4XPU::test_all_gather_into_tensor_coalesced\nThe reporter of the issue is PenghuiCheng, and the assignee is ratnampa, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "ratnampa", "resolution": "\nThe cases passed on the main branch of torch-xpu-ops.", "root_cause": "The reporter PenghuiCheng used oneCCL bound MPI with Intel MPI, and the PyTorch commit is 44d55b9, the oneccl commit is 445b002423aea9be7496c7dfac41750cd562b529, and the Intel MPI version is 2021.15  Build 20250213 (id: d233448).", "state": "closed"}### Result:1519 failed to extract

### Merged Result:1518{"issue_number": 1518, "issue_description": "Using nightly build PT2.8, this sample code will return wrong output:\n```\nimport torch\nfrom datasets import load_dataset\nfrom transformers import pipeline, Wav2Vec2Processor\n\nmodel_id = \"facebook/hubert-large-ls960-ft\"\ndevice = \"xpu\"\ntorch_dtype = torch.float16\ngenerator = pipeline(\n    \"automatic-speech-recognition\",\n     model=model_id,\n    device=device,\n    torch_dtype=torch_dtype,\n)\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\", trust_remote_code=True)\ninput_data = ds[0]['audio']['array']\nwith torch.inference_mode():\n    output = generator(input_data)\nprint(f\"output: {output}\")\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1518. The reporter of the issue is kaixuanliu, and the assignee is LuFinch, and the state of the issue is open.", "reporter": "kaixuanliu", "assignee": "LuFinch", "resolution": "\n", "root_cause": "The output of scaled_dot_product_attention API is wrong.", "state": "open"}

### Merged Result:1513{"issue_number": 1513, "issue_description": "Linux][Inductor UT] Unable to exit github action stage normally after completing Inductor UT test, \n\n**Github action stage:** \nhttps://github.com/intel/torch-xpu-ops/blob/refs/heads/ruijie/Inductor_UT/.github/workflows/_linux_ut.yml#L204\n**Test results:** \nYou can check the log: https://github.com/intel/torch-xpu-ops/actions/runs/14032267388\n\nOnly the selected four UTs are tested, and the stage can be ended normally.", "reporter": "RUIJIEZHONG66166", "assignee": "RUIJIEZHONG66166", "resolution": "\nThe issue is resolved as only the selected four UTs are tested and the stage can be ended normally.", "root_cause": "The issue is caused by the fact that only the selected four UTs are tested and the stage can be ended normally.", "state": "open"}

### Merged Result:1512{"issue_number": 1512, "issue_description": "first run take long time on windows bmg/arc (not test lnl etc), but proper time on Linux\ncuda takes 6s for first run with 9th gen core i7 + Geforce RTX 2060", "reporter": "ZhaoqiongZ", "assignee": "LuFinch", "resolution": "\n", "root_cause": "cuda driver or runtime version issue", "state": "open"}

### Merged Result:1510{"issue_number": 1510, "issue_description": "Some test cases in test/xpu will be hang\nSuch as\n test_tensor_creation_ops_xpu.py::TestTensorCreationXPU::test_linspace_xpu_complex128\nOnce 1 case got failed, all the next will be also failed, and rerun the failed Individually will be passed\nThe issue is related to the test cases in the torch-xpu-ops repository, where some tests hang or fail due to memory issues. The root cause is that the tests are too large to fit into the memory, causing the tests to hang or fail. The resolution is to add a `empty_cache` function to the test cases to release the memory after each test run. This can be done by adding the following code to the `TestCommon` class in the `test_ops.py` file: ```Python import gc class TestCommon(TestCase):   @pytest.fixture(autouse=True)   def run_before_and_after_tests(tmpdir):     # Setup: fill with any logic you want, this is before every test run     yield # this is where the testing happens     # This is run after each test. We release the memory after every test run.     torch.xpu.empty_cache()     gc.collect()```. If the problem persists, the tests can be run with the `--co` flag and parsed to run them one-by-one.", "reporter": "mengfei25", "assignee": "Stonepia", "resolution": "\nAdd `empty_cache` function to the test cases to release the memory after each test run.", "root_cause": "The tests are too large to fit into the memory, causing the tests to hang or fail.", "state": "open"}

### Merged Result:1509{"issue_number": 1509, "issue_description": "backward failed.\n\nlog:\nFile \"/home/penghuic/pytorch/test/distributed/test_multi_threaded_pg.py\", line 336, in test_bwd_sees_fwd_pg\n    x.sum().backward()\nRuntimeError: Data corruption detected\n\nreproduce command:\npytest -v test/distributed/test_multi_threaded_pg.py\n\nIn max 1550 device, this case will result in segmentation fault error.", "reporter": "PenghuiCheng", "assignee": "ashokei", "resolution": "\n", "root_cause": "Oneccl not perfect support multi-thread. So skip it first.", "state": "open"}

### Merged Result:1508{"issue_number": 1508, "issue_description": "RuntimeError: oneCCL: coll_param.cpp:455 validate: EXCEPTION: average operation is not supported for the scheduler path\nRuntimeError: oneCCL: sycl_coll_base.hpp:517 invoke_scaleout: EXCEPTION: unsupported datatype INT64", "reporter": "PenghuiCheng", "assignee": "ratnampa", "resolution": "\nThe issue is resolved by changing the torch tensor dtype to torch.int32", "root_cause": "INT64 is not currently supported by SYCL collectives", "state": "open"}

### Merged Result:1507{"issue_number": 1507, "issue_description": "OffsetBasedRNGTracker didn't support XPU device. Log: torch/testing/_internal/common_distributed.py:748] RuntimeError: OffsetBasedRNGTracker instantiation requires the presence of CUDA/CUDA-like device. Got xpu instead. reproduce command: python test/distributed/tensor/parallel/test_tp_random_state.py\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1507. The reporter of the issue is PenghuiCheng, and the assignee is , and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "", "resolution": "\nhttps://github.com/pytorch/pytorch/pull/148360 was merged.", "root_cause": "The issue was fixed in https://github.com/pytorch/pytorch/pull/148360", "state": "closed"}

### Merged Result:1506{"issue_number": 1506, "issue_description": "E2E (hf & timm) models got fail_accuracy", "reporter": "libohao1201", "assignee": "", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1505{"issue_number": 1505, "issue_description": "14 Timm models got fail_accuracy on ARC-WSL.\n3 HF models also got fail_accuracy.", "reporter": "libohao1201", "assignee": "", "resolution": "\n", "root_cause": "3 HF models also got fail_accuracy.", "state": "open"}### Result:1504 failed to extract

### Merged Result:1503{"issue_number": 1503, "issue_description": "When building PyTorch release/2.7 from source with oneapi in conda env and then activate oneapi on Windows, the compilation will trigger redefinition error. The error message is shown in the attached image. The conda list of the conda env which contains intel-sycl-rt etc. The redefinition error occurs when building from source and activating oneapi with the following commands: call \"C:\\Program Files (x86)\\Intel\\oneAPI\\compiler\\latest\\env\\vars.bat\" call \"C:\\Program Files (x86)\\Intel\\oneAPI\\ocloc\\latest\\env\\vars.bat\". The root cause is not yet identified.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1503. The reporter of the issue is ZhaoqiongZ, and the assignee is , and the state of the issue is open.", "reporter": "ZhaoqiongZ", "assignee": "", "resolution": "\n", "root_cause": "This issue also happens when a user compiles a PyTorch extension.", "state": "open"}

### Merged Result:1502{"issue_number": 1502, "issue_description": "WSL will crash when running torchbench. cp torch-xpu-ops/.github/scripts/inductor_xpu_test.sh pytorch cd pytorch # change iterations (-n) to 20 in inductor_xpu_test.sh bash inductor_xpu_test.sh torchbench float32 inference accuracy xpu 0 static 1 0 basic_gnn_gin", "reporter": "libohao1201", "assignee": "", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1500{"issue_number": 1500, "issue_description": "The operator 'aten::_slow_conv2d_forward' is not currently implemented for the XPU device. The error occurs when running the following code:\n\n```python\nimport torch\nfrom transformers import AutoFeatureExtractor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\n\nDEVICE = \"xpu:0\"\n\ntransformers_model = AutoModelForImageClassification.from_pretrained(\n    \"hf-internal-testing/tiny-random-vit\", device_map=DEVICE\n)\npreprocessor = AutoFeatureExtractor.from_pretrained(\"hf-internal-testing/tiny-random-vit\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = preprocessor(images=image, return_tensors=\"pt\")\nwith torch.no_grad():\n    transformers_outputs = transformers_model(**inputs)\n```\nI run into this error when executing above sample code. Will you add support for this OP?", "reporter": "kaixuanliu", "assignee": "", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:1498{"issue_number": 1498, "issue_description": "5 extended uts failed with **RuntimeError: Native API failed. Native API returns: 29 (UR_RESULT_ERROR_INVALID_KERNEL_NAME) **.", "reporter": "libohao1201", "assignee": "gaopengff", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1497{"issue_number": 1497, "issue_description": "RoIAlign autocast test got failed. The test failed with the following error message: Tensor-likes are not close! Mismatched elements: 4663 / 5000 (93.3%). Greatest absolute difference: 0.0018805861473083496 at index (1, 38, 3, 3). Greatest relative difference: 0.008542869240045547 at index (1, 17, 4, 0).\nThe issue is about the performance of the torch_xpu_ops library, specifically the problem of the slow execution of certain operations. The reporter of the issue is mengfei25, and the assignee is chunhuanMeng. The issue has been closed, and the resolution is to use the latest version of the library. The root cause is that the operations were not optimized for the XPU hardware.", "reporter": "mengfei25", "assignee": "chunhuanMeng", "resolution": "\nUse the latest version of the library", "root_cause": "The operations were not optimized for the XPU hardware", "state": "closed"}

### Merged Result:1496{"issue_number": 1496, "issue_description": "When running E2E inductor on LNL, the following error appears randomly: The memory could not be read.\nThis might be because of the driver that enables overcommit feature. Then the writing becomes invalid. We are still tracking this in GSD-10905 . If there is a fix, we could switch back to re-test this.", "reporter": "libohao1201", "assignee": "", "resolution": "\n", "root_cause": "The driver that enables overcommit feature causes the writing to become invalid.", "state": "open"}

### Merged Result:1483{"issue_number": 1483, "issue_description": "python benchmarks/dynamo/torchbench.py --performance --float16 -d xpu -n10 --inference --only sam --backend=inductor --cold-start-latency\n\nTesting model sam\nloading model: 0it [00:06, ?it/s]\nxpu  eval  sam\nIn file included from /usr/include/string.h:535,\n                 from /usr/include/c++/11/cstring:42,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/detail/string.hpp:9,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/exception.hpp:16,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/detail/array.hpp:25,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/buffer.hpp:13,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/accessor.hpp:15,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/detail/core.hpp:21,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/include/sycl/sycl.hpp:25,\n                 from /home/sdp/yzt/miniforge3/envs/mengfeil/lib/python3.10/site-packages/triton/backends/intel/include/sycl_functions.h:16,\n                 from /tmp/tmpr0wjdmws/main.cpp:23:\nIn function \u2018char* strncat(char*, const char*, size_t)\u2019,\n    inlined from \u2018PyObject* load_binary(PyObject*)\u2019 at /tmp/tmpr0wjdmws/main.cpp:291:12:\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:138:34: warning: \u2018char* __builtin___strncat_chk(char*, const char*, long unsigned int, long unsigned int)\u2019 specified bound depends on the length of the source argument [-Wstringop-overflow=]\n  138 |   return __builtin___strncat_chk (__dest, __src, __len,\n      |          ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\n  139 |                                   __glibc_objsize (__dest));\n      |                                   ~~~~~~~~~~~~~~~~~~~~~~~~~\nIn file included from /usr/include/c++/11/ios:40,\n                 from /usr/include/c++/11/ostream:38,\n                 from /usr/include/c++/11/iostream:39,\n                 from /tmp/tmpr0wjdmws/main.cpp:10:\n/usr/include/c++/11/bits/char_traits.h: In function \u2018PyObject* load_binary(PyObject*)\u2019:\n/usr/include/c++/11/bits/char_traits.h:399:32: note: length computed here\n  399 |         return __builtin_strlen(__s);\n      |                ~~~~~~~~~~~~~~~~^~~~~\nSegmentation fault from GPU at 0xffc00000ffc0a000, ctx_id: 1 (CCS) type: 0 (NotPresent), level: 4 (PML5), access: 0 (Read), banned: 1, aborting.\nSegmentation fault from GPU at 0xffc00000ffc0a000, ctx_id: 1 (CCS) type: 0 (NotPresent), level: 4 (PML5), access: 0 (Read), banned: 1, aborting.\nAbort was called at 269 line in file:\n./shared/source/os_interface/linux/drm_neo.cpp\ninductor_xpu_test.sh: line 62: 1234146 Aborted                 (core dumped) \nsam_fast got same issue, and they are passed on CUDA", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\n", "root_cause": "sdpa", "state": "open"}

### Merged Result:1480{"issue_number": 1480, "issue_description": "SDPBackend.FLASH_ATTENTION , SDPBackend.EFFICIENT_ATTENTION are missing and should be added a quickly as possible.\nThe reporter of the issue is githubsgi, and the assignee is LuFinch, and the state of the issue is open.", "reporter": "githubsgi", "assignee": "LuFinch", "resolution": "\n", "root_cause": "The backward pass is still missing.", "state": "open"}

### Merged Result:1478{"issue_number": 1478, "issue_description": "When adding pytorch/test/test_xpu.py in torch-xpu-ops windows CI, we found these failure: FAILED [1.7263s] test_xpu.py::TestXpuXPU::test_lazy_init_xpu - subprocess.CalledProcessError: Command '['C:\\Users\\Devcloud\\.conda\\envs\\windows_ci\\python.exe', 'C:\\Users\\Devcloud\\.conda\\envs\\windows_ci\\lib\\site-packages\\torch_xpu\\test\\test_xpu.py']' FAILED [0.0043s] test_xpu.py::TestXpuXPU::test_mem_get_info_xpu - RuntimeError: The device (Intel(R) Arc(TM) Graphics) doesn't support querying the available free memory \u2014\u2014 **known issue: https://github.com/intel/torch-xpu-ops/issues/1384** FAILED [1.8017s] test_xpu.py::TestXpuXPU::test_wrong_xpu_fork_xpu - AssertionError: Regex didn't match: 'Cannot re-initialize XPU in forked subprocess.' not found in 'PYTORCH_API_USAGE'\nAn attempt has been made to start a new process before the current process has finished its bootstrapping phase.", "reporter": "RUIJIEZHONG66166", "assignee": "LuFinch", "resolution": "\nCreate PR to skip on Windows", "root_cause": "A subprocess has issues when creating another subprocess in Windows", "state": "open"}

### Merged Result:1475{"issue_number": 1475, "issue_description": "When do the preci test for the branch daisyden/fsdp_test I found some cases of test_fsdp_core.py got random failures, such as: test_transformer_no_grad_mixed_precision_True_xpu, test_transformer_no_grad_mixed_precision_False_xpu. The error message is: Scalars are not equal! Expected 0 but got -11. Absolute difference: 11 Relative difference: inf Expect process 1 exit code to match Process 0 exit code of 0, but got -11\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1475. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "The issue occurs on CI machine, I didn't see UR error. We may need to get a same machine as CI environment to check the issue.", "state": "open"}

### Merged Result:1472{"issue_number": 1472, "issue_description": "We have listed the following cases that need to be optimized at the operator level. At the practical level, We need to use a specific shape and stride to track their performance status.\n- [ ] BatchNorm\n- [ ] GroupNorm\n- [ ] LayerNorm", "reporter": "xytintel", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:1468{"issue_number": 1468, "issue_description": "With oneAPI 2025.1 int16/int32/int64 argmin result is incorrect, XPU result is incorrect, while int8 does not have the issue. The inputs are as follows: sample.input is a tensor with shape [3, 2, 2, 2] and dtype is int64. The op.name is 'argmin'. The sample.args and sample.kwargs are empty. The dim is -1 and keepdim is False. The expected result is a tensor with shape [3, 2] and dtype is int64, where the first dimension is the index of the minimum value along the specified dimension, and the second dimension is the value of the minimum element. The actual result from XPU is [[0, 1], [0, 0], [0, 0]], while the expected result is [[0, 1], [1, 0], [0, 0]]. The actual result from CPU is [[0, 1], [1, 0], [0, 0]].\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1468. The reporter of the issue is daisyden, and the assignee is Stonepia, and the state of the issue is open.", "reporter": "daisyden", "assignee": "Stonepia", "resolution": "\nShould be related to oneAPI and expected to be fixed in 2025.2", "root_cause": "internal JIRA for this", "state": "open"}

### Merged Result:1465{"issue_number": 1465, "issue_description": "RuntimeError: Non-uniform work-groups are not supported by the target device", "reporter": "daisyden", "assignee": "xytintel", "resolution": "", "root_cause": "This issue is related to the non-uniform work-groups not being supported by the target device, which is a limitation of the BMG (Bare Metal GPU) target device. The reporter daisyden has reported this issue and it is still open.", "state": "open"}

### Merged Result:1461{"issue_number": 1461, "issue_description": "The build failed when building the xpu ops in the isolated python virtual environment. The pytorch root cmake is using the `Python_EXECUTABLE` but the xpu cmake is using a different macro `PYTHON_EXECUTABLE`. After I quick change it to align it with the torch naming, the issue fixed.", "reporter": "chengjunlu", "assignee": "", "resolution": "Change the macro `PYTHON_EXECUTABLE` to align with the torch naming.", "root_cause": "The pytorch root cmake is using the `Python_EXECUTABLE` but the xpu cmake is using a different macro `PYTHON_EXECUTABLE`.", "state": "closed"}### Result:1459 failed to extract

### Merged Result:1453{"issue_number": 1453, "issue_description": "The BMG machine will crash when running hunggingface performance mode. The issue will go when adding parameter --batch-size=2. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is open.\nThe model crashes when the batch size is too large and the dedicated GPU memory is not enough.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\n", "root_cause": "The crash happens due to the memory pressure when the model is too large to fit into the dedicated GPU memory. The shared GPU memory is taken into use, and the model still could run. However, after a few models, the memory is not correctly released, leading to the crash.", "state": "open"}

### Merged Result:1444{"issue_number": 1444, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1444. The reporter of the issue is hoshibara, and the assignee is , and the state of the issue is closed. The issue title [FlexAttention] Accuracy issues during running FlexDecoding UT, and issue body Content of #1444 is : ### \ud83d\udc1b Describe the bug\nSorry for creating the incorrect issue., Extract the github issue description with error message information from issue tile and issue body, if possible also extract the resolution and root cause information.", "reporter": "hoshibara", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:1438{"issue_number": 1438, "issue_description": "On LNL and BMG, the xpu.memory_stats() have no output:\n\n```Python\n>>>  start_mem = torch.xpu.memory_stats()\n>>> for k,v in start_mem.items():\n...     print(k, v)\n...\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1438. The reporter of the issue is Stonepia, and the assignee is LuFinch, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "LuFinch", "resolution": "\nClose it as unnecessary.", "root_cause": "The reproducer is wrong. PVC print empty too. Even CUDA prints empty after change `xpu` to `cuda`.", "state": "closed"}

### Merged Result:1437{"issue_number": 1437, "issue_description": "These cases failed on PVC CI test_meta_xpu.py::TestMetaXPU::test_dispatch_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_bfloat16 test_meta_xpu.py::TestMetaXPU::test_dispatch_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_float16 test_meta_xpu.py::TestMetaXPU::test_dispatch_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_float32 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_all_strides_nn_functional_max_unpool3d_grad_xpu_float32 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_all_strides_nn_functional_max_unpool3d_xpu_float32 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_all_strides_nn_functional_scaled_dot_product_attention_xpu_float32 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_bfloat16 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_float16 test_meta_xpu.py::TestMetaXPU::test_dispatch_symbolic_meta_outplace_nn_functional_scaled_dot_product_attention_xpu_float32\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1437. The reporter of the issue is daisyden, and the assignee is LuFinch, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "LuFinch", "resolution": "\nfixed", "root_cause": "The reporter of the issue is daisyden, and the assignee is LuFinch, and the state of the issue is closed.", "state": "closed"}

### Merged Result:1432{"issue_number": 1432, "issue_description": "SDPA cases failed after XPU enabled in stock pytorch, the error message is RuntimeError: output 1: meta disagrees with real impl: aten._scaled_dot_product_fused_attention_overrideable.default(tensor(..., device='meta', size=(4, 4, 3, 8)) stride=(96, 24, 8, 1), tensor(..., device='meta', size=(4, 4, 3)) stride=(12, 3, 1), None, None, 3, 6, tensor(..., device='meta', size=(), dtype=torch.int64) stride=(), tensor(..., device='meta', size=(), dtype=torch.int64) stride=(), None) = (tensor(..., device='meta', size=(4, 4, 3, 8)) stride=(96, 24, 8, 1), tensor(..., device='meta', size=(4, 4, 3)) stride=(12, 3, 1), None, None, 3, 6, tensor(..., device='meta', size=(), dtype=torch.int64) stride=(), tensor(..., device='meta', size=(), dtype=torch.int64) stride=(), None) for element 1, was torch.Size([4, 4, 3]) but real shape was torch.Size([])\nSDPA outputs NaN for fully masked rows.", "reporter": "daisyden", "assignee": "LuFinch", "resolution": "\nOneDNN support", "root_cause": "NaN output for fully masked rows", "state": "open"}

### Merged Result:1431{"issue_number": 1431, "issue_description": "RuntimeError: to_padded_tensor: at least one constituent tensor should have non-zero numel", "reporter": "weishi-deng", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1429{"issue_number": 1429, "issue_description": "Reproducer: max_diff1: 4.466546854597908e-10 median: 4.466546854597908e-10 max_diff2: 1.0513642602200068e-17 median: 5.4526070938755164e-18\nNot a issue", "reporter": "xytintel", "assignee": "xytintel", "resolution": "\nNot a issue", "root_cause": "", "state": "closed"}

### Merged Result:1428{"issue_number": 1428, "issue_description": "test_quantize_per_channel gets core dump\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1428. The reporter of the issue is weishi-deng, and the assignee is yucai-intel, and the state of the issue is open.", "reporter": "weishi-deng", "assignee": "yucai-intel", "resolution": "\n", "root_cause": "The XPU device is not found in checkZeroPoints(). The corresponding PR has been submitted to pytorch. However, after adding the device, the automatically called 'aten::dequantize.self' has not been registered on the QuantizedXPU backend, which needs further implementation.", "state": "open"}

### Merged Result:1426{"issue_number": 1426, "issue_description": "AssertionError: The values for attribute 'shape' do not match: torch.Size([4, 2, 2, 12]) != torch.Size([4, 2, 8, 12]).\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1426. The reporter of the issue is weishi-deng, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "weishi-deng", "assignee": "xytintel", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/1487", "root_cause": "https://github.com/intel/torch-xpu-ops/blob/main/test/xpu/test_native_mha_xpu.py", "state": "closed"}

### Merged Result:1423{"issue_number": 1423, "issue_description": "binary add become slower than 2.6, current pytorch main gets 2342624ns on average, pytorch 2.6 gets 951728ns on max 1550. Register File Size Per Thread increased from 128 to 256\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1423. The reporter of the issue is jianyizh, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "jianyizh", "assignee": "xytintel", "resolution": "\n", "root_cause": "This pytorch commit add fp8, leads to igc auto grf choose 256 instead of 128 register files for cast, so it will be slower for large shapes", "state": "closed"}

### Merged Result:1422{"issue_number": 1422, "issue_description": "When building PyTorch without sourcing MKL, the PyTorch can't find the one in conda env, so that cause LAPACK support failed. The build cannot find the static linked MKL. The reporter of the issue is Stonepia, and the assignee is CuiYifeng, and the state of the issue is open. The error message is: -- LAPACK requires BLAS -- Cannot find a library with LAPACK API. Not using LAPACK. -- Found a library with LAPACK API (mkl). --   USE_BLAS              : 1 --     BLAS                : mkl --     BLAS_HAS_SBGEMM     : --   USE_LAPACK            : 1 --     LAPACK              : mkl\nIt's not our expected behavior, right? @CuiYifeng. Firstly, we don't enable the USE_ONEMKL=1, it shouldn't use oneMKL. Secondly, if we set USE_ONEMKL=1, and we source the compiler, we don't need source mkl any more.", "reporter": "Stonepia", "assignee": "CuiYifeng", "resolution": "A workaround is to source oneMKL before the build. This is what current CI is doing.\nThe issue was resolved by adding the following line to the build environment: `if defined CMAKE_PREFIX_PATH (set CMAKE_PREFIX_PATH=%CONDA_PREFIX%\nLibrary;%CMAKE_PREFIX_PATH%) else (set CMAKE_PREFIX_PATH=%CONDA_PREFIX%\nLibrary)`", "root_cause": "When building PyTorch without sourcing MKL, the PyTorch can't find the one in conda env, so that cause LAPACK support failed.", "state": "open"}

### Merged Result:1401{"issue_number": 1401, "issue_description": "test_weight_norm.py::TestNNMethod::test_weight_norm_different_type, FAILED, E       AssertionError: Tensor-likes are not close!, 1.980\n\nself = <test_weight_norm.TestNNMethod testMethod=test_weight_norm_different_type>\n\ndef test_weight_norm_different_type(self):\n    v = torch.randn(8193  8193).requires_grad_(True)\n    g = torch.randn(8193).to(torch.float).requires_grad_(True)\n    gw = torch.randn(8193  8193)\n    w  n = torch._weight_norm_interface(v  g  dim=0)\n    w.backward(gw)\n    v_xpu = v.detach().clone().to(\"xpu\").requires_grad_(True)\n    g_xpu = g.detach().clone().to(\"xpu\").requires_grad_(True)\n    w_xpu  n_xpu = torch._weight_norm_interface(v_xpu  g_xpu  dim=0)\n    w_xpu.backward(gw.to(\"xpu\"))\n    self.assertEqual(w  w_xpu.cpu()  atol=1e-3  rtol=1e-5)\n    self.assertEqual(n  n_xpu.cpu()  atol=1e-1  rtol=1e-5)\n    self.assertEqual(v.grad  v_xpu.grad.cpu()  atol=1e-3  rtol=1e-5)\n>       self.assertEqual(g.grad  g_xpu.grad.cpu()  atol=1e-3  rtol=1e-5)\nE       AssertionError: Tensor-likes are not close!\nE       \nE       Mismatched elements: 1 / 8193 (0.0%)\nE       Greatest absolute difference: 0.5634427070617676 at index (3813 ) (up to 0.001 allowed)\nE       Greatest relative difference: 2.646775960922241 at index (3813 ) (up to 1e-05 allowed)\nE       \nE       To execute this test  run the following from the base repo dir:\nE           python test_weight_norm.py TestNNMethod.test_weight_norm_different_type\nE       \nE       This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0", "reporter": "huaiyuzh", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1400{"issue_number": 1400, "issue_description": "test_rms_norm.py::TestNNMethod::test_rms_norm_bw, FAILED, E           AssertionError: Tensor-likes are not close!, 1.960\nself = <test_rms_norm.TestNNMethod testMethod=test_rms_norm_bw>\n\ndef test_rms_norm_bw(self):\n    def test_rms_norm_fwd_bwd(dtype):\n        print(\"test_rms_norm_fw_bw\"  dtype)\n        torch.manual_seed(13)\n        modelb = RMSNormRef(64)\n        model0 = RMSNormRef(768)\n        model1 = RMSNormRef(2048)\n        model2 = RMSNormRef(4096)\n        model3 = RMSNormRef(16384)\n        model4 = RMSNormRef(16384 * 4 + 123)\n        hszs = [64  768  2048  4096  16384  16384 * 4 + 123]\n        ls = [modelb  model0  model1  model2  model3  model4]\n        for i  model in enumerate(ls):\n            model = model.to(dtype)\n            hsz = hszs[i]\n            input_case = torch.rand(4  1024  hsz).to(dtype)\n            input_case.requires_grad_(True)\n            grad = torch.rand(4  1024  hsz).to(dtype)\n            output_ref = model(input_case)\n            output_ref.backward(grad)\n            grad_wei = model.weight.grad.clone()\n            input_grad_cpu = input_case.grad.clone()\n            w = model.weight.clone()\n\n            input_case_xpu = input_case.clone().xpu()\n            input_case_xpu.retain_grad()\n            input_case_xpu.requires_grad_(True)\n            grad_xpu = grad.xpu()\n            w = w.xpu()\n            w.retain_grad()\n            w.requires_grad_(True)\n            output1 = torch.xpu.IpexRmsNorm(input_case_xpu  [hsz]  w  1e-5)\n            output1.backward(grad_xpu)\n            grad_wei_xpu = w.grad\n\n>       self.assertEqual(grad_wei_xpu.cpu()  grad_wei  atol=10e-2  rtol=10e-2)\nE       AssertionError: Tensor-likes are not close!\nE       \nE       Mismatched elements: 128 / 16384 (0.8%)\nE       Greatest absolute difference: 488.0 at index (9973 ) (up to 0.1 allowed)\nE       Greatest relative difference: 0.265625 at index (9860 ) (up to 0.1 allowed)\nE       \nE       To execute this test  run the following from the base repo dir:\nE           python test_rms_norm.py TestNNMethod.test_rms_norm_bw\nE       \nE       This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\n\ntest_rms_norm.py:89: AssertionError\n---------------------------- Captured stdout call -----------------------------\ntest_rms_norm_fw_bw torch.bfloat16\n----------------------------------------------------\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1400. The reporter of the issue is huaiyuzh, and the assignee is PenghuiCheng, and the state of the issue is open.", "reporter": "huaiyuzh", "assignee": "PenghuiCheng", "resolution": "\n", "root_cause": "This is an IPEX issue. In the UT you can see it tests IPEX ops `torch.xpu.IpexRmsNorm`.", "state": "open"}

### Merged Result:1399{"issue_number": 1399, "issue_description": "Microbench] loss.soft_margin_loss forward/backward have performance regression, File an issue to track this perf regression on torch-xpu-ops.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1399. The reporter of the issue is huaiyuzh, and the assignee is xytintel, and the state of the issue is open.", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "\n", "root_cause": "", "state": "open"}### Result:1392 failed to extract

### Merged Result:1391{"issue_number": 1391, "issue_description": "python benchmarks/dynamo/torchbench.py --accuracy --float32 -d xpu -n10 --inference --only moondream --backend=inductor\n\nxpu  eval  moondream\n\neager_two_runs_differ\n\n\nThe reporter of the issue is kaileiyx, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "kaileiyx", "assignee": "jianyizh", "resolution": "\nThe issue was resolved as the latest test and local test both passed.", "root_cause": "", "state": "closed"}

### Merged Result:1390{"issue_number": 1390, "issue_description": "DebertaForQuestionAnswering amp_bf16/amp_fp16 training/inference accuracy/performance got failed, the error message is RuntimeError: value cannot be converted to type at::BFloat16 without overflow\nThe test PASSED on latest PyTorch.", "reporter": "kaileiyx", "assignee": "etaf", "resolution": "\nThe test PASSED on latest PyTorch.", "root_cause": "", "state": "closed"}

### Merged Result:1389{"issue_number": 1389, "issue_description": "DebertaForMaskedLM  amp_bf16/amp_fp16  training/inference accuracy/performance got failed, the error message is RuntimeError: value cannot be converted to type at::BFloat16 without overflow\nThe test PASSED on latest PyTorch.", "reporter": "kaileiyx", "assignee": "etaf", "resolution": "\nThe test PASSED on latest PyTorch.", "root_cause": "", "state": "closed"}

### Merged Result:1385{"issue_number": 1385, "issue_description": "Most of E2E models failed with  torch._inductor.exc.InductorError: RuntimeError: Triton Error [ZE]: 0x78000011 on BMG windows.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1385. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\n", "root_cause": "A bug in the driver", "state": "closed"}

### Merged Result:1384{"issue_number": 1384, "issue_description": "On integrated platforms (like LNL, MTL), the following test will failed:\n\n```Python\n>>> import torch\n>>> torch.xpu.mem_get_info()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Users\\sdp\\miniforge3\\envs\\tongsu_stock_pt\\lib\\site-packages\\torch\\xpu\\memory.py\", line 194, in mem_get_info\n    return torch._C._xpu_getMemoryInfo(device)\nRuntimeError: The device does not have the ext_intel_free_memory aspect\n>>> torch.version.xpu\n'20250000' \n```\nThe root cause of this issue is that the driver does not support it now. See the GSD-10758 for the internal track. The UR_DEVICE_INFO_GLOBAL_MEM_FREE needs device modules from Sysman being reported which are not, hence not supported return codes from UR. UR detects 0 modules being reported and returns this error.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nDuplicate with #1352", "root_cause": "The device does not have the ext_intel_free_memory aspect", "state": "closed"}

### Merged Result:1382{"issue_number": 1382, "issue_description": "test_transformer.py::TestTorchMethod::test_transformerencoderlayer\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1382. The reporter of the issue is huaiyuzh, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:1381{"issue_number": 1381, "issue_description": "molan performance regression up to 76%, which is caused by pad_sequence and gru.input. Extract the github issue description with error message information from issue tile and issue body, if possible also extract the resolution and root cause information.", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1380{"issue_number": 1380, "issue_description": "Sort has performance regression in model pointnet-atlas(~5% on single tile and ~10% on scaling up), call xpu-ops: aten::sort 0.16% 1.533ms 0.26% 2.497ms 624.143us 23.513ms 18.21% 24.079ms 6.020ms 4 call ipex: aten::sort 0.03% 280.956us 0.05% 439.005us 109.751us 13.320ms 12.00% 13.320ms 3.330ms 4", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1354{"issue_number": 1354, "issue_description": "Bfloat16 GroupNorm 4x slower than fp32\nGroupNorm with vectorization enabled", "reporter": "jianyizh", "assignee": "xytintel", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/1357", "root_cause": "bf16 enter the vectorize kernel, fp32 not", "state": "closed"}

### Merged Result:1352{"issue_number": 1352, "issue_description": "The device does not have the ext_intel_free_memory aspect. The error message is: The device does not have the ext_intel_free_memory aspect. The device does not have the ext_intel_free_memory aspect.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1352. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is open.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\n", "root_cause": "The root cause of this issue is that the driver does not support it now. See the **GSD-10758** for the internal track. UR_DEVICE_INFO_GLOBAL_MEM_FREE needs device modules from Sysman being reported which are not, hence not supported return codes from UR. UR detects 0 modules being reported and returns this error.", "state": "open"}

### Merged Result:1350{"issue_number": 1350, "issue_description": "Tested on Windows with nightly wheel UT will be hung\nPASSED with PyTorch: 2.7.0.dev20250310+xpu / cdb42bd8cc05bef0ec9b682b274c2acb273f2d62", "reporter": "mengfei25", "assignee": "chunhuanMeng", "resolution": "\nPASSED", "root_cause": "", "state": "closed"}

### Merged Result:1347{"issue_number": 1347, "issue_description": "The operator torch_ipex::prepare_4d_causal_attention_mask is currently not implemented for Intel GPUs (XPU) in the Intel Extension for PyTorch (IPEX). This limitation leads to fallbacks to the CPU, resulting in performance degradation during model inference. The operator is not implemented for XPU, and fallbacks to the CPU are used as a result. The environment variable PYTORCH_ENABLE_XPU_FALLBACK=1 can be set to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1347. The reporter of the issue is tcconnally, and the assignee is , and the state of the issue is closed.", "reporter": "tcconnally", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}### Result:1343 failed to extract

### Merged Result:1338{"issue_number": 1338, "issue_description": "erfcx_xpu and ndtri_xpu cause an IPEX UT fail. In IPEX2.6, we override this Ops with IPEX implementation to make this UT pass. \n\nRuntimeError: \"erfcx_xpu\" not implemented for 'BFloat16'\nRuntimeError: \"ndtri_xpu\" not implemented for 'BFloat16'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1338. The reporter of the issue is huaiyuzh, and the assignee is chunhuanMeng, and the state of the issue is closed.", "reporter": "huaiyuzh", "assignee": "chunhuanMeng", "resolution": "\nThis is because ipex adds support for BFloat16 data type for these two ops, but torch-xpu-ops does not have this support, and stock pytorch also does not have this support. Therefore, if you really need it, you can raise a PR in pytorch, and then torch-xpu-ops will make corresponding changes. We ensure that our code is consistent with the design of stock pytorch.", "root_cause": "ipex adds support for BFloat16 data type for these two ops, but torch-xpu-ops does not have this support, and stock pytorch also does not have this support.", "state": "closed"}

### Merged Result:1337{"issue_number": 1337, "issue_description": "fractional_max_pool2d and fractional_max_pool3d cause an IPEX UT fail. In IPEX2.6, we override this Ops with IPEX implementation to make this UT pass. The UT fails with the following error: AssertionError: Booleans mismatch: False is not True. The issue is related to the fractional_max_pool2d and fractional_max_pool3d operations in IPEX2.6, which are overridden with IPEX implementations to make the UT pass. However, the UT still fails with the same error.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1337. The reporter of the issue is huaiyuzh, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "\nI have fixed the issue by updating the case in IPEX2.7. We can close it.", "root_cause": "", "state": "closed"}

### Merged Result:1336{"issue_number": 1336, "issue_description": "index_copy_xpu cause an IPEX UT fail. In IPEX2.6, we override this Ops with IPEX implementation to make this UT pass.\n\nipex/tests/gpu/example/test_fp8_index_copy.py::TestTorchMethod::test_index_copy - RuntimeError: \"index_copy_xpu\" not implemented for 'Float8_e4m3fn'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1336. The reporter of the issue is huaiyuzh, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": "override with IPEX implementation\n", "root_cause": "not implemented for 'Float8_e4m3fn'", "state": "closed"}

### Merged Result:1335{"issue_number": 1335, "issue_description": "Character number of linkage command may exceed the length limit of Windows Command Line\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1335. The reporter of the issue is CuiYifeng, and the assignee is CuiYifeng, and the state of the issue is closed.", "reporter": "CuiYifeng", "assignee": "CuiYifeng", "resolution": "\nClose this issue since long command has been bypassed with intermediate libraries #1243.", "root_cause": "CMake will create a very long command for linkage (more than 32767 characters) if all XPU libraries are combined into one `libtorch_xpu.so`. The length of this command is related to the prefix of build path and the number of objects to be linked.", "state": "closed"}

### Merged Result:1334{"issue_number": 1334, "issue_description": "timm_regnet BF16 gor fail accuracy recently, the last known good is https://github.com/intel/torch-xpu-ops/commit/b6786e31c36b31bb2cc18e2325451a3198832cb8 + torch a7c2d85\n```\npython benchmarks/dynamo/torchbench.py --accuracy --bfloat16 -d xpu -n10 --training --only timm_regnet --backend=inductor\n```\nxpu  train timm_regnet \nE0204 15:16:48.599000 2195001 site-packages/torch/_dynamo/utils.py:2751] RMSE (res-fp64): 0.01081, (ref-fp64): 0.00091 and shape=torch.Size([]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.001000, use_larger_multiplier_for_smaller_tensor: 0\nfail_accuracy\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1334. The reporter of the issue is mengfei25, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nFall back fp64 to cpu", "root_cause": "The test can pass in the latest pytorch with PR ", "state": "closed"}

### Merged Result:1332{"issue_number": 1332, "issue_description": "internal compiler error: in extract_insn when compiling pytorch with xpu with gcc 12, the error message is unrecognizable insn: vregs, during RTL pass: vregs, please submit a full bug report, with preprocessed source (by using -freport-bug).\nThe error occurs with the default `-O2` optimization level but not with `-O1`. Further investigation revealed that the error occurs with GCC 12.3 installed via `sudo apt install gcc-12`, but not with GCC 12.4 built from source. Additionally, using `-O2` optimization with the `-fno-tree-loop-vectorize` flag avoids the error. This suggests that the issue is specific to GCC 12.3 and is related to the loop vectorization optimization.", "reporter": "jingxu10", "assignee": "xytintel", "resolution": "\nThe issue is resolved by using the `-fno-tree-loop-vectorize` flag.", "root_cause": "The loop vectorization optimization in GCC 12.3 is causing the error.", "state": "closed"}

### Merged Result:1331{"issue_number": 1331, "issue_description": "build the pytorch2.6.0 natively with B580. The log shows `ats-m150`\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1331. The reporter of the issue is alanzhai219, and the assignee is , and the state of the issue is closed.", "reporter": "alanzhai219", "assignee": "", "resolution": "\nThis is because AOT support multiple targets. Let's close the issue.", "root_cause": "can specify the target without building others?", "state": "closed"}

### Merged Result:1329{"issue_number": 1329, "issue_description": "The operator 'quantized::linear_dynamic' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1329. The reporter of the issue is gurwinderintel, and the assignee is ZhiweiYan-96, and the state of the issue is open.", "reporter": "gurwinderintel", "assignee": "ZhiweiYan-96", "resolution": "\n", "root_cause": "The quantization operation is still under development and targeted for PT 2.8, the reporter is advised to wait until PT 2.8 is out.", "state": "open"}

### Merged Result:1328{"issue_number": 1328, "issue_description": "The operator 'fsdp::all_gather_copy_in' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators.\nThe reporter of the issue is saforem2, and the assignee is Chao1Han, and the state of the issue is closed.\nNo backend type associated with device type xpu\nThe reporter of the issue is saforem2, and the assignee is Chao1Han, and the state of the issue is closed.", "reporter": "saforem2", "assignee": "Chao1Han", "resolution": "\nclosed\nEnvironment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` was set, but the issue still persists\nclosed", "root_cause": "The reporter of the issue is saforem2, and the assignee is Chao1Han, and the state of the issue is closed.", "state": "closed"}

### Merged Result:1325{"issue_number": 1325, "issue_description": "The reporter of the issue is fengyuan14, and the assignee is CuiYifeng, and the state of the issue is open. The issue title is Recommended building and runtime dependences for MKL., and issue body Content of #1325 is : ### \ud83d\ude80 The feature, motivation and pitch\n\nSo far, https://github.com/intel/torch-xpu-ops/pull/526 implemented the first MKL related operator, `aten.fft_c2c`. The PR introduced MKL building system in torch-xpu-ops. The MKL SDK introduced for building bases on oneAPI package.\n\nThe potential issue is we would recommend using `pip install mkl-dpcpp` for runtime. There would be potential API breaking issue when MKL version in oneAPI package for building has gap with MKL in Pypi.\n\nWe need to unify recommended MKL package for building and runtime.\n\n### Alternatives\n_No response_\n\n### Additional context\n_No response_,\nIs there a plan to add other oneMKL APIs such as those from Sparse BLAS in `torch-xpu-ops`? I'm seeing things like #1330 which appear to start adding Sparse CSR support but aren't (or maybe can't yet be?) using oneMKL.", "reporter": "fengyuan14", "assignee": "CuiYifeng", "resolution": "\n", "root_cause": "The reporter is asking if there is a plan to add other oneMKL APIs such as those from Sparse BLAS in `torch-xpu-ops`. The assignee is not providing a clear answer and is referring to another issue #1330 which is not using oneMKL.", "state": "open"}

### Merged Result:1324{"issue_number": 1324, "issue_description": "When running models and the model is OOM, we get the UR Error, and this UR Error will break tensor context. The error message is: UR backend failed. UR backend returns:40 (UR_RESULT_ERROR_OUT_OF_RESOURCES). The error also occurs when re-accessing the tensor after the error. The error message is: UR error. The error could be handled by CUDA, which has the following logic: 1. If the dedicated memory is not enough, it will try allocate from the host memory. This behavior could be disabled by https://forums.developer.nvidia.com/t/cuda-unified-memory-oversubscription-in-windows-systems/58391. 2. If disabled CUDA's unified memory oversubscription, it will throw OOM, and the re-access `x1` in the example could also work, its tensor context won't be affected.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1324. The reporter of the issue is Stonepia, and the assignee is guangyey, and the state of the issue is open.", "reporter": "Stonepia", "assignee": "guangyey", "resolution": "\n", "root_cause": "It seems like a driver bug.", "state": "open"}

### Merged Result:1315{"issue_number": 1315, "issue_description": "Without FALLBACK, and aligned the tests with CUDA, we got 693 failures on test_ops.py. I assigned owners for them, @xytintel @CuiYifeng  @ZhiweiYan-96  please have a check and feel free to reassign them. The detailed table is at [link](https://intel-my.sharepoint.com/:x:/p/daisy_deng/EWrUhd2WO2BCj0GOE_NccoQBTDG1OG_LF_dN57GlkWSwUQ?e=Z4L7eF)", "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1305{"issue_number": 1305, "issue_description": "Models got fail accuracy on BMG but passed on PVC\nThe accuracy gap for the models `fbnetv3_b` and `gernet_l` is not yet determined. The root cause of the accuracy gap for `fbnetv3_b` is an unalignment with CUDA's test behavior, where some models are fallback to SGD optimizer. To resolve this, the `fbnetv3_b` model is being adjusted to use the same optimizer behavior as CUDA. The root cause for `gernet_l` is yet to be determined and a PR is being submitted to align with CUDA's optimizer.", "reporter": "mengfei25", "assignee": "Stonepia", "resolution": "\nThe accuracy gap for the models `fbnetv3_b` and `gernet_l` is being addressed. The root cause for `fbnetv3_b` is an unalignment with CUDA's test behavior, and the root cause for `gernet_l` is yet to be determined.", "root_cause": "The root cause for `fbnetv3_b` is an unalignment with CUDA's test behavior, where some models are fallback to SGD optimizer. To resolve this, the `fbnetv3_b` model is being adjusted to use the same optimizer behavior as CUDA. The root cause for `gernet_l` is yet to be determined.", "state": "open"}

### Merged Result:1296{"issue_number": 1296, "issue_description": "Torchbench demucs training got fail accuracy, xpu  train demucs \nE0115 03:33:56.683000 2457704 torch/_dynamo/utils.py:2263] Accuracy failed: allclose not within tol=0\neager_two_runs_differ\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1296. The reporter of the issue is mengfei25, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nput fp64 ref on xpu can solve this issue", "root_cause": "currently cpu and xpu lstm have different implementation", "state": "closed"}

### Merged Result:1290{"issue_number": 1290, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1290. The reporter of the issue is frost-intel, and the assignee is , and the state of the issue is closed.", "reporter": "frost-intel", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}### Result:1280 failed to extract

### Merged Result:1279{"issue_number": 1279, "issue_description": "Just compile the pytorch XPU on latest viable/strict (https://github.com/pytorch/pytorch/commit/68dad26b950) will gives the following error. I tried on Windows but I think this bug should apply to all OS.\n\n```\n[136/3592] Generating ../../../xpu/ATen/XPUFunctions.h, ../../../xpu/ATen/RegisterXPU.cpp, ../../../xpu/ATen/RegisterSparseXPU.cpp, D:/pytorch/torch/csrc/inductor/aoti_torch/generated/extend/c_shim_xpu.h, D:/pytorch/torch/csrc/inductor/aoti_torch/generated/extend/c_shim_xpu.cpp\nFAILED: xpu/ATen/XPUFunctions.h xpu/ATen/RegisterXPU.cpp xpu/ATen/RegisterSparseXPU.cpp D:/pytorch/torch/csrc/inductor/aoti_torch/generated/extend/c_shim_xpu.h D:/pytorch/torch/csrc/inductor/aoti_torch/generated/extend/c_shim_xpu.cpp D:/pytorch/build/xpu/ATen/XPUFunctions.h D:/pytorch/build/xpu/ATen/RegisterXPU.cpp D:/pytorch/build/xpu/ATen/RegisterSparseXPU.cpp \nC:\\WINDOWS\\system32\\cmd.exe /C \"cd /D D:\\pytorch && C:\\Users\\Yi\\miniforge3\\envs\\yi\\python.exe -m torchgen.gen --source-path D:/pytorch/third_party/torch-xpu-ops/yaml/ --install-dir D:/pytorch/build/xpu/ATen/ --per-operator-headers --static-dispatch-backend --backend-whitelist XPU SparseXPU --xpu --update-aoti-c-shim --extend-aoti-c-shim --aoti-install-dir=D:/pytorch/torch/csrc/inductor/aoti_torch/generated/extend && type D:\\pytorch\\third_party\\torch-xpu-ops\\src\\ATen\\native\\xpu\\XPUFallback.template >> D:\\pytorch\\build\\xpu\\ATen\\//RegisterXPU.cpp && C:\\Users\\Yi\\miniforge3\\envs\\yi\\python.exe D:/pytorch/third_party/torch-xpu-ops/tools/codegen/remove_headers.py --register_xpu_path D:/pytorch/build/xpu/ATen//RegisterXPU.cpp && C:\\Users\\Yi\\miniforge3\\envs\\yi\\python.exe D:/pytorch/third_party/torch-xpu-ops/tools/codegen/remove_headers.py --register_xpu_path D:/pytorch/build/xpu/ATen//RegisterSparseXPU.cpp\"\nTraceback (most recent call last):\n  File \"D:\\pytorch\\third_party\\torch-xpu-ops\\tools\\codegen\\remove_headers.py\", line 31, in <module>\n    replace_op_headers()\n  File \"D:\\pytorch\\third_party\\torch-xpu-ops\\tools\\codegen\\remove_headers.py\", line 18, in replace_op_headers\n    with open(args.register_xpu_path, 'r') as fr:\nFileNotFoundError: [Errno 2] No such file or directory: 'D:/pytorch/build/xpu/ATen//RegisterSparseXPU.cpp'\n```", "reporter": "DDEle", "assignee": "", "resolution": "", "root_cause": "pytorch/pytorch#144364", "state": "closed"}

### Merged Result:1278{"issue_number": 1278, "issue_description": "Detectron2 inference accuracy got failed, the model detectron2_fasterrcnn_r_101_c4, detectron2_fasterrcnn_r_101_dc5, detectron2_fasterrcnn_r_101_fpn, detectron2_fasterrcnn_r_50_c4, detectron2_fasterrcnn_r_50_dc5, detectron2_fasterrcnn_r_50_fpn, detectron2_maskrcnn_r_101_c4, detectron2_maskrcnn_r_101_fpn, detectron2_maskrcnn_r_50_c4, detectron2_maskrcnn_r_50_fpn all got failed accuracy, the error message is eager_fail_to_run, the state of the issue is closed, the assignee is jianyizh, the reporter is mengfei25\nroi_align_forward_kernel_xpu not implemented for bf16", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nclosed", "root_cause": "roi_align_forward_kernel_xpu not implemented for bf16", "state": "closed"}

### Merged Result:1277{"issue_number": 1277, "issue_description": "Llava BF16 and FP16 inference accuracy got out of memory, the error message is XPU out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 48.00 GiB. Of the allocated memory 47.92 GiB is allocated by PyTorch, and 12.48 MiB is reserved by PyTorch but unallocated. Please use `empty_cache` to release all unoccupied cached memory.\nllava training is not enabled.... The fail is in load model stage.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "", "state": "closed"}### Result:1276 failed to extract

### Merged Result:1275{"issue_number": 1275, "issue_description": "Eca_halonext26ts AMP_BF16 training accuracy got failed, RMSE (res-fp64): 0.01146, (ref-fp64): 0.00222 and shape=torch.Size([512]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.010000, use_larger_multiplier_for_smaller_tensor: 0 Accuracy failed for key name stages.3.0.post_attn.running_var\nfbnetv3_b got same issue", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\ndisabling the decomposition of bn would help", "root_cause": "natural numeric issue caused by the bf16 type", "state": "closed"}

### Merged Result:1274{"issue_number": 1274, "issue_description": "xpu  train convnext_base                       E0104 06:32:47.808000 280838 site-packages/torch/_dynamo/utils.py:2353] RMSE (res-fp64): 1391.25547, (ref-fp64): 0.00008 and shape=torch.Size([128]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.040000, use_larger_multiplier_for_smaller_tensor: 0\nE0104 06:32:47.808000 280838 site-packages/torch/_dynamo/utils.py:2223] Accuracy failed for key name stem.0.bias.grad\nfail_accuracy\nThe root mean square error is very large, and I suspect onednn has some invalid memory access issues...", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nUpdate onednn to main (7a741297e018707b21fb6a280b4399929503bbd7) will solve this issue, but there is small chance to meet gpu page fault and this large rmse again...", "root_cause": "The root cause is suspected to be onednn invalid memory access issues.", "state": "closed"}

### Merged Result:1273{"issue_number": 1273, "issue_description": "Soft_actor_critic BF16 inference got fail accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1273. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nPassed with latest code base", "root_cause": "", "state": "closed"}

### Merged Result:1264{"issue_number": 1264, "issue_description": "Vision_maskrcnn RuntimeError: roi_align_backward_kernel_xpu does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.\nExisting `roi_align` implementation in torchvision disables the non-deterministic implementation and replaces the op with a pure python implementation which complies to a lower memory version. Supporting XPU with the same approach requires `is_compile_supported` in `torch/_dynamo/utils.py` to add XPU support.", "reporter": "mengfei25", "assignee": "frost-intel", "resolution": "You can turn off determinism just for this operation, or you can use the 'warn_only=True' option, if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.\nSupporting XPU with the same approach requires `is_compile_supported` in `torch/_dynamo/utils.py` to add XPU support.", "root_cause": "This issue is related to the deterministic implementation of the roi_align_backward_kernel_xpu function, which does not have a deterministic implementation. The reporter of the issue is mengfei25, and the assignee is frost-intel, and the state of the issue is closed.", "state": "closed"}

### Merged Result:1263{"issue_number": 1263, "issue_description": "xpu  train LearningToPaint                                                                                                 Traceback (most recent call last):                                                                                                                                                                                                                                 File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4886, in run                                                                                                                                                                                                                                 ) = runner.load_model(                                                                                                                                                                                                                                                                                File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 325, in load_model                                                                                                                                                                                                                                 model, example_inputs = benchmark.get_module(                                                                                                                                                                                                                                                                            File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/LearningToPaint/__init__.py\", line 81, in get_module                                                                                                                                                                                                                                 action = self.agent.select_action(                                                                                                                                                                                                                                                                                File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/LearningToPaint/baseline/DRL/ddpg.py\", line 211, in select_action                                                                                                                                                                                                                                 action = to_numpy(action)                                                                                                                                                                                                                                                                                File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/LearningToPaint/baseline/utils/util.py\", line 40, in to_numpy                                                                                                                                                                                                                                 return var.cpu().data.numpy() if USE_CUDA else var.data.numpy()                                                                                                                                                                                                                                                                            TypeError: can't convert xpu:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.                                                                                                                                                                                                                              eager_fail_to_run\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1263. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nPassed with latest code base", "root_cause": "", "state": "closed"}

### Merged Result:1262{"issue_number": 1262, "issue_description": "Hf_Reformer got different accuracy results 2 eager runs, Accuracy failed: allclose not within tol=0, Accuracy failed for key name logits, eager_two_runs_differ\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1262. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nPassed with latest code base", "root_cause": "", "state": "closed"}### Result:1261 failed to extract

### Merged Result:1260{"issue_number": 1260, "issue_description": "Nvidia_deeprecommender got failed on XPU device\n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4886, in run\n    ) = runner.load_model(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 325, in load_model\n    model, example_inputs = benchmark.get_module(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/nvidia_deeprecommender/__init__.py\", line 48, in get_module\n    return self.model.rencoder, (self.model.toyinputs,)\nAttributeError: 'DeepRecommenderTrainBenchmark' object has no attribute 'rencoder'\n\neager_fail_to_run\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1260. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nPassed with latest code base", "root_cause": "", "state": "closed"}

### Merged Result:1256{"issue_number": 1256, "issue_description": "The following models got 'eager_two_runs_differ':\n|     |            |                                                                                                           eager_two_runs_diff                                                                                                          |\n|-----|------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| LNL | HF         |                                                                                                                                                                                                                                        |\n|     | Timm       |                                                                                                                                                                                                                                        |\n|     | Torchbench | Super_SloMo   (train_eager_fp32/amp_bf16)      pytorch_CycleGAN_and_pix2pix (train_eager_fp32/bf16/amp_bf16)                                                                                                                           |\n| BMG | HF         |                                                                                                                                                                                                                                        |\n|     | Timm       |                                                                                                                                                                                                                                        |\n|     | Torchbench | Super_SloMo   (train_eager_fp32)      pytorch_CycleGAN_and_pix2pix (train_eager_fp32/bf16)                                                                                                                                             |\n| ARC | HF         | DistilBertForMaskedLM(train_fp16_eager)                                                                                                                                                                                                |\n|     | Timm       | convnext_base   (train_eager)      jx_nest_base (train_eager)      swin_base_patch4_window7_224 (train_eager)      twins_pcpvt_base (train_eager)      coat_lite_mini (train)      convit_base      mobilevit_s      tnt_s_patch16_224 |\n|     | Torchbench | hf_Reformer(train_eager)      timm_regnet (train_eager_fp32) \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1256. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is open.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\n", "root_cause": "The issue is related to atomic operations causing non-deterministic behavior. The reporter and assignee have discussed the need to set deterministic algorithms and avoid atomic operations to resolve the issue. However, the issue still persists on some models like Super_SloMo and pytorch_CycleGAN_and_pix2pix.", "state": "open"}

### Merged Result:1255{"issue_number": 1255, "issue_description": "The following models got fail_accuracy\n|     |            |                                                                                  fail_accuracy                                                                                 |\n|-----|------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| LNL | HF         |                                                                                                                                                                                |\n|     | Timm       |                                                                                                                                                                                |\n|     | Torchbench | hf_Roberta_base   (train_eager_fp16/bf16)      hf_BigBird (train_eager_amp_fp16/bf16)      hf_Reformer (train_eager_amp_fp16/bf16)      hf_Whisper (train_eager_amp_fp16/bf16) |\n| BMG | HF         |                                                                                                                                                                                |\n|     | Timm       |                                                                                                                                                                                |\n|     | Torchbench | hf_Reformer   (train_eager_amp_bf16/fp16)      hf_Whisper (train_eager_amp_fp16/bf16)                                                                                          |\n| ARC | HF         |                                                                                                                                                                                |\n|     | Timm       | coat_lite_mini(train_eager_fp32)                                                                                                                                               |\n|     | Torchbench | hf_Whisper   (train_eager_amp_bf16/fp16)                                                                                                                                       \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1255. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\nThose passed on latest client acceptance test. Thus closed.", "root_cause": "", "state": "closed"}

### Merged Result:1254{"issue_number": 1254, "issue_description": "During the test, we witnessed the following accuracy failures in `test_torchinductor_opinfo.py`:\n\n```\ntest_comprehensive_masked_mean_xpu_float16\ntest_comprehensive_masked_mean_xpu_float32\ntest_comprehensive_masked_mean_xpu_float64\ntest_comprehensive_nn_functional_pairwise_distance_xpu_float16\n```\nThe test PASSED on latest PyTorch.", "reporter": "Stonepia", "assignee": "etaf", "resolution": "\nThe test PASSED on latest PyTorch.", "root_cause": "", "state": "closed"}

### Merged Result:1253{"issue_number": 1253, "issue_description": "Just compile the Pytorch XPU with cmake 3.31. Examples can be found in CI logs (e.g. https://github.com/intel/torch-xpu-ops/actions/runs/12632141068/job/35195239288#step:5:1034). Spamming if `BUILD_SEPARATE_OPS=ON`", "reporter": "DDEle", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:1252{"issue_number": 1252, "issue_description": "Critical issue tracking\n\n### Functional issue:\n\n| Operator | Timeline  | Description | PRs | Flag |\n| ----------------  | ---------------- | ---------------- | ---------------- | ---------------- |\n| index_reduce | 1/8/2025~ | Meet page fault in test_torch_xpu | https://github.com/intel/torch-xpu-ops/pull/1156 | WIP |\n| batch_norm | 1/8/2025~ | Meet page fault in cosmic_tagging_train.h5 with channels_last | | WIP |\n\n### Performance issue:\n\n| Operator | Timeline | Description | PRs | Flag |\n| ----------------  | ---------------- | ---------------- | ---------------- | ---------------- |\n| BN/GN/LN | 1/8/2025~ | Using the IPEX norm backbone to apply the Welford algorithm | | WIP |\n| scatter_gather | ~1/7/2025 | Perf gap with IPEX | https://github.com/intel/torch-xpu-ops/pull/1112 | Merged |\n| batch_norm | ~1/7/2025 | Perf gap with IPEX | https://github.com/intel/torch-xpu-ops/pull/933 | Merged |\n| upsample_bilinear2d | ~1/7/2025 | Perf gap with IPEX | https://github.com/intel/torch-xpu-ops/pull/950 | Merged |\n| max_pool2d_with_indices | ~1/7/2025 | Perf gap with IPEX | https://github.com/intel/torch-xpu-ops/pull/1127 | Merged |\n| group_norm | ~1/7/2025 | Perf gap with IPEX | https://github.com/intel/torch-xpu-ops/pull/1116 | Merged |", "reporter": "xytintel", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:1251{"issue_number": 1251, "issue_description": "With rhel and suse container, the 2 models accuracy failed.\npython benchmarks/dynamo/huggingface.py --accuracy --amp_fp16 -d xpu -n10 --training --only AlbertForMaskedLM --backend=inductor\n\nCannot reproduce on ubuntu container, is it only in rhel and suse container?", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "", "state": "closed"}### Result:1246 failed to extract

### Merged Result:1245{"issue_number": 1245, "issue_description": "Since the build logs and test logs are way too long, how about we consider redirecting them to different files. For example, for the following build command: python setup.py bdist_wheel we could direct them in the following way: Linux: python setup.py bdist_wheel 1> logs\test.log 2> logs\test.err Windows: python setup.py bdist_wheel 1>.\test.log 2>.\test.err The above would redirect the output to different files. Normally, we just need to check about the output from 1 and go to the error.log for details. Please go to the Additional Context part for example output of build. For the pytest, we could also do the same. Below is an example of an unsuccessful build. The stdout.log is 3037 lines, and stderr.log is 39166 lines, but actually, we need only about the last 100 lines of the stderr.log. You could find that the error is because the device does not have enough space, that's all we need. The error message is: LLVM ERROR: IO failure on output stream: No space left on device icpx: error: sycl-link command failed with exit code 1 (use -v to see invocation) Traceback (most recent call last): File \"/home/pt-gpu/4T-4652/usr/client_test/pytorch/setup.py\", line 1219, in <module> setup( File \"/home/pt-gpu/4T-4652/envs/usr_/lib/python3.10/site-packages/setuptools/__init__.py\", line 117, in setup return distutils.core.setup(**attrs) File \"/home/pt-gpu/4T-4652/envs/usr_/lib/python3.10/subprocess.py\", line 369, in check_call raise CalledProcessError(retcode, cmd) subprocess.CalledProcessError: Command '['make', '-j', '224', 'install']' returned non-zero exit status 2.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1245. The reporter of the issue is Stonepia, and the assignee is RUIJIEZHONG66166, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "RUIJIEZHONG66166", "resolution": "The issue was closed.\nClose as already done.", "root_cause": "The reporter of the issue is Stonepia, and the assignee is RUIJIEZHONG66166, and the state of the issue is closed.", "state": "closed"}

### Merged Result:1237{"issue_number": 1237, "issue_description": "FAILED test_native_mha_xpu.py::TestMHADeviceTypeXPU::test_native_multihead_attention_xpu_float16\n================================== FAILURES ===================================\n______ TestMHADeviceTypeXPU.test_native_multihead_attention_xpu_float16 _______\nTraceback (most recent call last):\n  File \"C:\\Users\\gta\\penghuic\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\test_native_mha.py\", line 323, in test_native_multihead_attention\n    self._test_multihead_attention_impl(\n  File \"C:\\Users\\gta\\penghuic\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\test_native_mha.py\", line 256, in _test_multihead_attention_impl\n    torch.testing.assert_close(ypt, ynpt.to(torch.float32), atol=1e-3, rtol=1e-3)\n  File \"C:\\Users\\gta\\Miniforge3\\envs\\ut-py310\\lib\\site-packages\\torch\\testing\\_comparison.py\", line 1530, in assert_close\n    raise error_metas[0].to_error(msg)\nAssertionError: Tensor-likes are not close!\n\nMismatched elements: 416 / 8192 (5.1%)\nGreatest absolute difference: 0.0026940107345581055 at index (2, 7, 62) (up to 0.001 allowed)\nGreatest relative difference: 10.974992752075195 at index (10, 4, 51) (up to 0.001 allowed)\n\nTo execute this test, run the following from the base repo dir:\n    python test\\test_native_mha.py TestMHADeviceTypeXPU.test_native_multihead_attention_xpu_float16\n\nFAILED test_native_mha_xpu.py::TestMHADeviceTypeXPU::test_native_multihead_encoder_decoder_attention_xpu_float16\n\n================================== FAILURES ===================================\n_ TestMHADeviceTypeXPU.test_native_multihead_encoder_decoder_attention_xpu_float16 _\nTraceback (most recent call last):\n  File \"C:\\Users\\gta\\penghuic\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\test_native_mha.py\", line 309, in test_native_multihead_encoder_decoder_attention\n    self._test_multihead_attention_impl(\n  File \"C:\\Users\\gta\\penghuic\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\test_native_mha.py\", line 256, in _test_multihead_attention_impl\n    torch.testing.assert_close(ypt, ynpt.to(torch.float32), atol=1e-3, rtol=1e-3)\n  File \"C:\\Users\\gta\\Miniforge3\\envs\\ut-py310\\lib\\site-packages\\torch\\testing\\_comparison.py\", line 1530, in assert_close\n    raise error_metas[0].to_error(msg)\nAssertionError: Tensor-likes are not close!\n\nMismatched elements: 454 / 8192 (5.5%)\nGreatest absolute difference: 0.0034734010696411133 at index (1, 3, 48) (up to 0.001 allowed)\nGreatest relative difference: 1.2274675369262695 at index (1, 6, 26) (up to 0.001 allowed)\n\nTo execute this test, run the following from the base repo dir:\n    python test\\test_native_mha.py TestMHADeviceTypeXPU.test_native_multihead_encoder_decoder_attention_xpu_float16\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1237. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\nThis case passed with the latest driver 6647 + 0309 nightly torch whl.", "root_cause": "", "state": "closed"}

### Merged Result:1236{"issue_number": 1236, "issue_description": "nn\\test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_max_pool_nan_inf_xpu_float64\n\n________ TestPoolingNNDeviceTypeXPU.test_max_pool_nan_inf_xpu_float64 _________\nTraceback (most recent call last):\n  File \"C:\\Users\\gta\\penghuic\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\nn\\test_pooling_xpu.py\", line 362, in _test_max_pool_nan_inf\n    self.assertTrue(math.isinf(res2.item()))\nRuntimeError: Native API failed. Native API returns: 2147483646 (UR_RESULT_ERROR_UNKNOWN)\n\nTo execute this test, run the following from the base repo dir:\n    python test\\nn\\test_pooling.py TestPoolingNNDeviceTypeXPU.test_max_pool_nan_inf_xpu_float64\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1236. The reporter of the issue is daisyden, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "Stonepia", "resolution": "\nThe issue was resolved by testing on the following environment and it passed: Pytorch: '2.7.0a0+git924a247', Driver: 32.0.101.6647.", "root_cause": "", "state": "closed"}

### Merged Result:1235{"issue_number": 1235, "issue_description": "nn\\test_embedding_xpu.py::TestEmbeddingNNDeviceTypeXPU::test_embedding_max_norm_device_xpu_float32\nTraceback (most recent call last):\n  File \"C:\\Users\\gta\\penghuic\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\nn\\test_embedding.py\", line 764, in test_embedding_max_norm_device\n    self.assertTrue(output.data.norm(p=2, dim=1).le(1).all())\n  File \"C:\\Users\\gta\\Miniforge3\\envs\\ut-py310\\lib\\unittest\\case.py\", line 687, in assertTrue\n    raise self.failureException(msg)\nAssertionError: tensor(False, device='xpu:0') is not true\n\nTo execute this test, run the following from the base repo dir:\n    python test\\nn\\test_embedding.py TestEmbeddingNNDeviceTypeXPU.test_embedding_max_norm_device_xpu_float32\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1235. The reporter of the issue is daisyden, and the assignee is gaopengff, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "gaopengff", "resolution": "\nClose it due to MTL windows is low priority", "root_cause": "MTL windows is low priority", "state": "closed"}

### Merged Result:1234{"issue_number": 1234, "issue_description": "The operator 'customflash::custom_flash_aligned' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nSame as https://github.com/intel/torch-xpu-ops/issues/714", "reporter": "mengfei25", "assignee": "xytintel", "resolution": "\nduplicate", "root_cause": "https://github.com/intel/torch-xpu-ops/issues/714", "state": "closed"}

### Merged Result:1231{"issue_number": 1231, "issue_description": "The operator 'aten::_thnn_fused_lstm_cell' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\ndemucs and DLND have the same issue with the XPU device not supporting the LSTM operator.\nThis operator has already been cherry-picked to release/2.6: https://github.com/intel/torch-xpu-ops/pull/1233", "reporter": "mengfei25", "assignee": "", "resolution": "\nclosed\nThe issue has been resolved and the operator has been cherry-picked to the release version.", "root_cause": "The XPU device does not support the LSTM operator, leading to a NotImplementedError.", "state": "closed"}

### Merged Result:1229{"issue_number": 1229, "issue_description": "python benchmarks/dynamo/torchbench.py --accuracy --bfloat16 -d xpu -n10 --training --only yolov3 --backend=inductor\n\nTraceback (most recent call last):\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4886, in run\n) = runner.load_model(\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 312, in load_model\nbenchmark = benchmark_cls(\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 24, in call\nobj = type.call(cls, *args, **kwargs)\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/yolov3/init.py\", line 58, in init\nself.training_loop, self.model, self.example_inputs = prepare_training_loop(\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/yolov3/yolo_train.py\", line 630, in prepare_training_loop\ndevice = torch_utils.select_device(\nFile \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/yolov3/yolo_utils/torch_utils.py\", line 31, in select_device\nassert torch.cuda.is_available(), 'CUDA unavailable, invalid device %s requested' % device # check availablity\nAssertionError: CUDA unavailable, invalid device xpu requested\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1229. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nSkipped in CI & Nightly test https://github.com/intel/torch-xpu-ops/pull/1230", "root_cause": "The issue is caused by the fact that the XPU device is not available. The reporter of the issue is mengfei25, and the assignee is . The state of the issue is closed.", "state": "closed"}

### Merged Result:1222{"issue_number": 1222, "issue_description": "Torchbench models got fail_accuracy when using bfloat16 or amp_bf16. The models that failed are listed in the issue body. The issue is reproducible with the command `python benchmarks/dynamo/torchbench.py --accuracy --bfloat16 -d xpu -n10 --inference  --only shufflenet_v2_x1_0 --backend=inductor`.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1222. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nFixed", "root_cause": "https://github.com/intel/torch-xpu-ops/commit/e035f6b3fc8aea782d57bfe90e64fb43cf5ffe55", "state": "closed"}

### Merged Result:1221{"issue_number": 1221, "issue_description": "TorchDynamo optimized model failed to run because of following error\nfail_to_run\nself and mat2 must have the same dtype, but got Float and BFloat16\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1221. The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\nRegression", "root_cause": "2.5 is passed", "state": "closed"}### Result:1220 failed to extract

### Merged Result:1219{"issue_number": 1219, "issue_description": "cannot import name 'cached_download' from 'huggingface_hub' (/global/panfs01/users/mengfei25/miniforge3/envs/test/lib/python3.10/site-packages/huggingface_hub/__init__.py)\nDowngrading huggingface-hub to 0.25.0", "reporter": "mengfei25", "assignee": "", "resolution": "\nDowngrading huggingface-hub to 0.25.0", "root_cause": "Unknown", "state": "closed"}

### Merged Result:1217{"issue_number": 1217, "issue_description": "Failed dtype: bfloat16 and amp_bf16.  float32, float16 and amp_fp16 passed\nCaused by https://github.com/intel/torch-xpu-ops/commit/e035f6b3fc8aea782d57bfe90e64fb43cf5ffe55", "reporter": "mengfei25", "assignee": "", "resolution": "\n", "root_cause": "https://github.com/intel/torch-xpu-ops/commit/e035f6b3fc8aea782d57bfe90e64fb43cf5ffe55", "state": "closed"}

### Merged Result:1216{"issue_number": 1216, "issue_description": "Failed dtype: float32, float16 and bfloat16. AMP passed\npython benchmarks/dynamo/huggingface.py --accuracy --float32 -d xpu -n10 --training--only DebertaV2ForQuestionAnswering --backend=inductor\nxpu  train DebertaV2ForQuestionAnswering\nE1220 16:43:35.601000 756971 site-packages/torch/_dynamo/utils.py:2307] RMSE (res-fp64): 0.53515, (ref-fp64): 0.01636 and shape=torch.Size([]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.010000, use_larger_multiplier_for_smaller_tensor: 0\nfail_accuracy\nThe issue is caused by a commit in pytorch/pytorch, and upgrading transformers to the latest version will fix the issue.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nUpgrading transformers to the latest version will fix the issue.", "root_cause": "A commit in pytorch/pytorch caused the issue.", "state": "open"}

### Merged Result:1214{"issue_number": 1214, "issue_description": "In preci test, there are random cases will fail, need root cause. The cases that have appeared are as follows: test/xpu/test_ops_xpu.py: test_python_ref__refs_exp_xpu_complex128 test_python_ref__refs_sigmoid_xpu_complex128 test_python_ref_executor__refs_log2_executor_aten_xpu_complex128 test_python_ref_executor__refs_exp_executor_aten_xpu_complex128 test_python_ref_torch_fallback__refs_log2_xpu_complex128 test_python_ref_torch_fallback__refs_log10_xpu_complex128 test_python_ref_torch_fallback__refs_sigmoid_xpu_complex128 workaround PR: https://github.com/intel/torch-xpu-ops/pull/1211 More random failures to be added to skiplist: - [ ] TestCommonXPU.test_python_ref_executor__refs_sigmoid_executor_aten_xpu_complex128 - [ ] TestCommonXPU.test_compare_cpu_nn_functional_local_response_norm_xpu_bfloat16 - [ ] test_ops_xpu.py::TestCommonXPU::test_python_ref__refs_log10_xpu_complex128\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1214. The reporter of the issue is PenghuiCheng, and the assignee is daisyden, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "\n", "root_cause": "New random failure with release/2.7 RC2 pre release wheel test_foreach_xpu.py::TestForeachXPU::test_parity__foreach_div_fastpath_outplace_xpu_complex128", "state": "open"}

### Merged Result:1213{"issue_number": 1213, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1213. The reporter of the issue is zhangxiaoli73, and the assignee is fengyuan14, and the state of the issue is closed.\nThe issue is about the performance of the torch_xpu_ops library, specifically the problem with the `torch_xpu_ops.matmul` function. The reporter of the issue is zhangxiaoli73, and the assignee is fengyuan14. The issue is closed, and the resolution is that the problem is caused by a bug in the `torch_xpu_ops.matmul` function, which has been fixed in the latest version of the library. The root cause is that the function does not handle certain edge cases properly, leading to performance degradation.", "reporter": "zhangxiaoli73", "assignee": "fengyuan14", "resolution": "\nThe problem is caused by a bug in the `torch_xpu_ops.matmul` function, which has been fixed in the latest version of the library.", "root_cause": "The function does not handle certain edge cases properly, leading to performance degradation.", "state": "closed"}

### Merged Result:1210{"issue_number": 1210, "issue_description": "The feature, motivation and pitch\n\nGoogleFnet in torch dynamo benchmarks uses fft, we need support it.\n\n### Alternatives\n_No response_\n\n### Additional context\n_No response_,", "reporter": "jianyizh", "assignee": "CuiYifeng", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1209{"issue_number": 1209, "issue_description": "Need tf32 for matmul, the reporter of the issue is jianyizh, the assignee is ZhiweiYan-96, the state of the issue is open, Content of #1209 is : ### \ud83d\ude80 The feature, motivation and pitch\nwe should add APIs like torch.backends.cuda.matmul.allow_tf32, torch.backends.cudnn.allow_tf32, torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction\n### Alternatives\n_No response_\n### Additional context\n_No response_", "reporter": "jianyizh", "assignee": "ZhiweiYan-96", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1200{"issue_number": 1200, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1200. The reporter of the issue is daisyden, and the assignee is xytintel, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1200. The reporter of the issue is daisyden, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "\n[Rel](https://github.com/pytorch/pytorch/commit/a52d9f6f4c9c839b0312fbe02925b38cd1f40758c3b3e579c277cdfe26af18a2#diff-fef2ee1307d8d82cd8ccfa4188ab3808635370a8c3b3e579c277cdfe26af18a2)", "root_cause": "", "state": "closed"}

### Merged Result:1199{"issue_number": 1199, "issue_description": "Seems that there should be a dtype conversion but we didn't do it:\n\n```py\nPYTORCH_TEST_WITH_SLOW=1 python test\\xpu\\test_tensor_creation_ops_xpu.py TestTensorCreationXPU.test_block_diag_scipy_xpu\n``` \n\nWe get the following error:\n\n```py\nTraceback (most recent call last):\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 59, in testPartExecutor\n    yield\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 591, in run\n    self._callTestMethod(testMethod)\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 549, in _callTestMethod\n    method()\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_utils.py\", line 3099, in wrapper\n    method(*args, **kwargs)\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_device_type.py\", line 460, in instantiated_test\n    result = test(self, **param_kwargs)\n  File \"C:\\pt26_ww48\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\test_tensor_creation_ops_xpu.py\", line 408, in test_block_diag_scipy\n    self.assertEqual(scipy_result.dtype, scipy_type)\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_utils.py\", line 4007, in assertEqual\n    raise error_metas.pop()[0].to_error(\nAssertionError: Object comparison failed: torch.int64 != torch.int32\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1199. The reporter of the issue is Stonepia, and the assignee is LuFinch, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "LuFinch", "resolution": "\nThe issue should be related to the environment.", "root_cause": "There should be a dtype conversion but we didn't do it.", "state": "closed"}

### Merged Result:1198{"issue_number": 1198, "issue_description": "RuntimeError: result type Float can't be cast to the desired output type Long\n\nTraceback (most recent call last):\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 59, in testPartExecutor\n    yield\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 591, in run\n    self._callTestMethod(testMethod)\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 549, in _callTestMethod\n    method()\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_utils.py\", line 3099, in wrapper\n    method(*args, **kwargs)\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_device_type.py\", line 460, in instantiated_test\n    result = test(self, **param_kwargs)\n  File \"C:\\pt26_ww48\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\test_binary_ufuncs.py\", line 1638, in test_long_tensor_pow_floats\n    self._test_pow(tensor, pow)\n  File \"C:\\pt26_ww48\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\test_binary_ufuncs_xpu.py\", line 66, in _test_pow\n    self.assertRaisesRegex(\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_utils.py\", line 4065, in assertRaisesRegex\n    return super().assertRaisesRegex(expected_exception, expected_regex, *args, **kwargs)\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 1291, in assertRaisesRegex\n    return context.handle('assertRaisesRegex', args, kwargs)\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 200, in handle\n    with self:\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 239, in __exit__\n    self._raiseFailure('\nThe issue is related to the environment.", "reporter": "Stonepia", "assignee": "gaopengff", "resolution": "\nCorrectly setup the environment of pip packages", "root_cause": "The reporter of the issue is Stonepia, and the assignee is gaopengff, and the state of the issue is closed.", "state": "closed"}

### Merged Result:1197{"issue_number": 1197, "issue_description": "When running the following test:\n\n```PYTORCH_TEST_WITH_SLOW=1 python test\\quantization\\core\\test_workflow_ops.py TestFakeQuantizeOpsXPU.test_learnable_forward_per_channel_cpu_xpu```\n\n```Traceback (most recent call last):\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 59, in testPartExecutor\n    yield\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 591, in run\n    self._callTestMethod(testMethod)\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 549, in _callTestMethod\n    method()\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_utils.py\", line 3099, in wrapper\n    method(*args, **kwargs)\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\torch\\testing\\_internal\\common_device_type.py\", line 460, in instantiated_test\n    result = test(self, **param_kwargs)\n  File \"C:\\pt26_ww48\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\quantization\\core\\test_workflow_ops.py\", line 807, in test_learnable_forward_per_channel\n    qparams=hu.qparams(dtypes=torch.quint8)))\n  File \"C:\\Users\\sdp\\pt26_ww48_virtual_env\\lib\\site-packages\\hypothesis\\core.py\", line 1758, in wrapped_test\n    raise the_error_hypothesis_found\n  File \"C:\\pt26_ww48\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\quantization\\core\\test_workflow_ops.py\", line 815, in test_learnable_forward_per_channel\n    self._test_learnable_forward_per_channel(\n  File \"C:\\pt26_ww48\\pytorch\\third_party\\torch-xpu-ops\\test\\xpu\\../../../../test\\quantization\\core\\test_workflow_ops.py\", line 802, in _test_learnable_forward_per_channel\n    self.assertTrue(\n  File \"C:\\Users\\sdp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\unittest\\case.py\", line 687, in assertTrue\n    raise self.failureException(msg)\nAssertionError: False is not true : Expected kernel forward function to have results match the reference forward function```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1197. The reporter of the issue is Stonepia, and the assignee is LuFinch, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "LuFinch", "resolution": "\nThe issue should be related to the environment.", "root_cause": "The issue should be related to the environment.", "state": "closed"}

### Merged Result:1196{"issue_number": 1196, "issue_description": "Need to skip CUDA tests, A typical error msg would be like: AssertionError: Torch not compiled with CUDA enabled\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1196. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "closed\nPR https://github.com/intel/torch-xpu-ops/pull/1123", "root_cause": "XPU does not support FP8 tests for now. We need to skip them.", "state": "closed"}

### Merged Result:1195{"issue_number": 1195, "issue_description": "We get nan when the dtype is complex. The tests are:\n\n```py\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal__refs_sin_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal__refs_sin_xpu_complex64\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_nn_functional_tanhshrink_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_nn_functional_tanhshrink_xpu_complex64\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sin_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sin_xpu_complex64\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sinh_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_extremal_sinh_xpu_complex64\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large__refs_exp_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large__refs_exp_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large__refs_sin_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large_exp_xpu_complex128\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large_exp_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_large_sin_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_normal_sigmoid_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_acos_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_asin_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_asinh_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_atan_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_atanh_xpu_complex32\nPYTORCH_TEST_WITH_SLOW=1 python test\\test_unary_ufuncs.py TestUnaryUfuncsXPU.test_reference_numerics_small_sigmoid_xpu_complex32\n```\nThere should be related to bugs with the compiler. We will discuss on how to co-work with the compiler team for this issue.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\n", "root_cause": "Compiler bugs", "state": "open"}### Result:1194 failed to extract

### Merged Result:1193{"issue_number": 1193, "issue_description": "UT cases which failed on rolling driver and passed on lts driver:\ntest_distributions_xpu.py::TestDistributionsXPU::test_gamma_gpu_sample_xpu\ntest_ops_xpu.py::TestCommonXPU::test_python_ref__refs_div_trunc_rounding_xpu_float64\nfor test_gamma_gpu_sample_xpu, it will also pass on rolling driver to add \"clang::optnone\" label.\n![image](https://github.com/user-attachments/assets/c449e637-7521-4c7d-8ae7-848a83efcc48)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1193. The reporter of the issue is PenghuiCheng, and the assignee is , and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "", "resolution": "add \"clang::optnone\" label\nDuplicated issue, close it.", "root_cause": "unknown", "state": "closed"}### Result:1192 failed to extract### Result:1191 failed to extract### Result:1173 failed to extract

### Merged Result:1172{"issue_number": 1172, "issue_description": "Got this error on LNL Windows with 1202 wheel. subprocess.CalledProcessError: Command '['where', 'cl']' returned non-zero exit status 1.\nVS2022INSTALLDIR environment variable not set", "reporter": "daisyden", "assignee": "", "resolution": "\nSetting VS2022INSTALLDIR environment variable to the correct path", "root_cause": "Incorrect path set for VS2022INSTALLDIR environment variable", "state": "closed"}

### Merged Result:1171{"issue_number": 1171, "issue_description": "On LNL Windows with 1202 nightly wheel we got this error. No such problem on linux.\n\n### \ud83d\udc1b Describe the bug\n\nOn LNL Windows with 1202 nightly wheel we got this error. No such problem on linux.\n\n### \ud83d\udccb Steps to reproduce\n\n1. Run the test\n2. See the error\n\n### \ud83d\udcc4 Expected behavior\n\nNo error\n\n### \ud83d\udcc4 Actual behavior\n\nAssertionError: 'Assertion `maxind >= 0 && maxind < outputImageSize` failed' not found in '\nAssertHandler::printMessage\n'\n\n### \ud83d\udcc4 Environment\n\n- OS: Windows\n- Python version: 3.8.12\n- PyTorch version: 1.12.1\n- PyTorch XPU version: 1.12.1\n- CUDA version: 11.7\n- PyTorch XPU wheel: 1202\n\n### \ud83d\udcc4 Additional context\n\nTo execute this test, run the following from the base repo dir:\n    python test\nnn\test_pooling.py TestPoolingNNDeviceTypeXPU.test_MaxUnpool_index_errors_case2_xpu\n\nThis message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\n\n####  Assertion `maxind >= 0 && maxind < outputImageSize` failed\nAssertHandler::printMessage\n\n=============================================== short test summary info ===============================================\nFAILED nn\test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_MaxUnpool_index_errors_case2_xpu - AssertionError: 'Assertion `maxind >= 0 && maxind < outputImageSize` failed' not found in '\nAssertHandler::printMessage\n'\n========================================= 1 failed, 132 deselected in 10.05s ==========================================\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1171. The reporter of the issue is daisyden, and the assignee is gaopengff, and the state of the issue is open.", "reporter": "daisyden", "assignee": "gaopengff", "resolution": "\n", "root_cause": "compiler issue", "state": "open"}

### Merged Result:1170{"issue_number": 1170, "issue_description": "AMP will be out of memory on PVC, after use stock PT, OOB for HF models on PVC have OOM issue compared with IPEX 2.3. UR backend failed. UR backend returns:40 (UR_RESULT_ERROR_OUT_OF_RESOURCES). torch.OutOfMemoryError: XPU out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 64.00 GiB\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1170. The reporter of the issue is 1pikachu, and the assignee is , and the state of the issue is closed.", "reporter": "1pikachu", "assignee": "", "resolution": "\nThis seems reasonable. The stock PyTorch may have larger op and exceed the memory capacity (because of too many kernels fused together).", "root_cause": "The stock PyTorch may have larger op and exceed the memory capacity (because of too many kernels fused together).", "state": "closed"}

### Merged Result:1169{"issue_number": 1169, "issue_description": "torch.nextafter has an incorrect result for bf16 on XPU. IPEX could give a correct result for bf16 on XPU.", "reporter": "guangyey", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:1166{"issue_number": 1166, "issue_description": "Please check this test, you can  find some UT failures in log, but the result of preci is passed. \nThis should already done.", "reporter": "daisyden", "assignee": "RUIJIEZHONG66166", "resolution": "\nThis should already done.", "root_cause": "", "state": "closed"}### Result:1165 failed to extract

### Merged Result:1164{"issue_number": 1164, "issue_description": "Torchbench training performance on Rolling driver is slower than LTS driver. The gap is around 20%. The following is the performance comparison of <50% models.\nHW CPU frequency settings, not related with SW", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "HW CPU frequency settings, not related with SW", "state": "closed"}

### Merged Result:1163{"issue_number": 1163, "issue_description": "torch._standard_gamma() has accuracy gap compared to scipy and torch.cpu, test_distributions_xpu.py::TestDistributionsXPU::test_gamma_gpu_sample_xpu FAILED, Gamma(alpha=1.0, beta=0.1).sample() is biased, -0.06743384440339613 not less than -0.259\nTried cpu _standard_gamma tensor, no such issue.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/1161", "root_cause": "Seems this issue exists or not on different driver versions, investigation is WIP.", "state": "open"}

### Merged Result:1160{"issue_number": 1160, "issue_description": "When the two tensors are the same, what is the expected result? It is 1.0 or a number close to 1.0? This will lead to different result when apply trunc, lead to the UT failures.  \n\nFAILED test_ops_xpu.py::TestCommonXPU::test_python_ref__refs_div_trunc_rounding_xpu_float64 - Exception: Caused by reference input at index 16: SampleInput\nFAILED test_ops_xpu.py::TestCommonXPU::test_python_ref_executor__refs_div_trunc_rounding_executor_aten_xpu_float64 - Exception: Caused by reference input at index 16: SampleInput\nFAILED test_ops_xpu.py::TestCommonXPU::test_python_ref_torch_fallback__refs_div_trunc_rounding_xpu_float64 - Exception: Caused by reference input at index 16: SampleInput\n\n(Pdb) f1=torch.tensor(-501., device='xpu:0', dtype=torch.float64)\n(Pdb) e1=f1\n(Pdb) torch.div(f1, e1, rounding_mode='trunc')\ntensor(1., device='xpu:0', dtype=torch.float64)\n(Pdb) torch.div(f1, e1)\ntensor(0.9999999999999998889776975374843459576368331909179687500000000000,\n       device='xpu:0', dtype=torch.float64)\n(Pdb) torch.trunc(torch.div(f1, e1))\ntensor(0., device='xpu:0', dtype=torch.float64)", "reporter": "daisyden", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1159{"issue_number": 1159, "issue_description": "Huggingface model DebertaForQuestionAnswering && DebertaV2ForMaskedLM failed with RuntimeError: value cannot be converted to type at::BFloat16 without overflow. Script: `python benchmarks\\dynamo\\huggingface.py --accuracy -d xpu -n10 --inference --backend=eager --cold-start-latency --amp --amp-dtype float16 --only DebertaForQuestionAnswering`. Error Log Info: xpu  eval  DebertaForMaskedLM Traceback (most recent call last): File \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1159. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is open.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\n", "root_cause": "This is the issue from transformers. See the discussion at https://github.com/huggingface/transformers/pull/35336 .", "state": "open"}

### Merged Result:1158{"issue_number": 1158, "issue_description": "Script: `python benchmarks\\dynamo\\huggingface.py --accuracy -d xpu -n10 --inference --backend=eager --cold-start-latency --float32 --only BlenderbotSmallForCausalLM ` failed with UR Error. The error message is: RuntimeError: UR error. The issue is closed and the root cause is unknown.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1158. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "The issue is closed and the root cause is unknown.\nClose this issue, as the UR Error should be expected. The model itself is too large.", "root_cause": "The model itself is too large.", "state": "closed"}

### Merged Result:1157{"issue_number": 141539, "issue_description": "install pytorch 2.6 nightly on windows Arc 770 machines,\npython test.py:\ngot the error\n File \"C:\\Users\\huiyanca\\.conda\\envs\\torch-xpu-nightly\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\huiyanca\\.conda\\envs\\torch-xpu-nightly\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\", line 1124, in forward\n    result = _VF.lstm(\nNotImplementedError: The operator 'aten::_thnn_fused_lstm_cell' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\n\nThis operator has already been cherry-picked to release/2.6", "reporter": "yinghu5", "assignee": "", "resolution": "\nThis operator has already been cherry-picked to release/2.6", "root_cause": "", "state": "closed"}

### Merged Result:1152{"issue_number": 1152, "issue_description": "When input is nan, PVC and CPU return nan while ARC returns 0. However I write a small case and it also returns nan on ARC, looks not a compiler issue. Need more investigations.\nverified passed on 1222 wheel", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:1151{"issue_number": 1151, "issue_description": "Failed to build Pytorch XPU on Windows Server, the error message is 'C:\\\nThe issue is related to the long file path and the OS version is Windows 10 1607, which might be too old. Thus we will try on a newer OS machine.", "reporter": "DDEle", "assignee": "Stonepia", "resolution": "\nclose issue", "root_cause": "The issue is because of the long file path and the OS version is Windows 10 1607, which might be too old.", "state": "closed"}

### Merged Result:1150{"issue_number": 1150, "issue_description": "Some operators UT fails on XPU with \"Kernel is incompatible with all devices\" error.\nKernel is incompatible with all devices in devs", "reporter": "PenghuiCheng", "assignee": "fengyuan14", "resolution": "\nChange the scalar or immediate value from double to float", "root_cause": "Calling sycl::get_kernel_bundle before sycl::queue::submit", "state": "open"}

### Merged Result:1147{"issue_number": 1147, "issue_description": "topk calculation gives wrong result when on xpu. I find the issue when using both bfloat16 and float16 but not on float32. Following code results with a different results. If the .to('xpu') is removed, the answer is 0. \n\n```python\nimport torch\n\ntorch.set_printoptions(profile='full', precision=6, linewidth=240)\n\nsample = torch.rand((1, 2000), device='cpu', dtype=torch.bfloat16)\ncpu_sample = torch.sort(torch.topk(sample, max(sample.shape) // 2)[1])[0]\nxpu_sample = torch.sort(torch.topk(sample.to('xpu'), max(sample.shape) // 2)[1])[0]\n\nprint(cpu_sample.cpu() - xpu_sample.cpu())\n```\nWhen using torch.topk, the **indices** of tied elements are not guaranteed to **be stable** and may vary across different invocations. And if you check the calculated **values** by following code, you will find the difference between two devices is zero.", "reporter": "maciek226", "assignee": "xytintel", "resolution": "\nThe issue is closed.", "root_cause": "The indices of tied elements are not guaranteed to be stable and may vary across different invocations.", "state": "closed"}

### Merged Result:1141{"issue_number": 1141, "issue_description": "Support NestedTensor for XPU device\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1141. \nThe reporter of the issue is min-jean-cho, \nand the assignee is daisyden,\nand the state of the issue is open.\n\nThis is the github issue title Support NestedTensor for XPU device,\nand issue body Content of #1141 is : ### \ud83d\ude80 The feature, motivation and pitch\n\nSupport `NestedTensor` for `xpu` device\n\n- [x] https://github.com/pytorch/pytorch/pull/140461\n- [x] https://github.com/intel/torch-xpu-ops/pull/1140\n- [x] https://github.com/intel/torch-xpu-ops/pull/1142\n- [x] https://github.com/pytorch/pytorch/pull/145467\n- [x] Add NestedTensor XPU ops\n  - [x] `_nested_from_padded ` https://github.com/intel/torch-xpu-ops/pull/1045\n  - [x] `_nested_tensor_softmax_with_shape` https://github.com/intel/torch-xpu-ops/pull/1323 \n  - [x] device-agnostic NestedTensor ops https://github.com/intel/torch-xpu-ops/pull/1258\n\n- [ ] Add NestedTensor XPU Uts\n\n**Example**\n```\n>>> nt = torch.nested.nested_tensor([]).to(\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1141. The reporter of the issue is min-jean-cho, and the assignee is daisyden, and the state of the issue is open.", "reporter": "min-jean-cho", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:1137{"issue_number": 1137, "issue_description": "Run stable-diffusion-inf at gpu-models repo, got an error. Need more investigation to confirm whether it is caused by stable-diffusion.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1137. The reporter of the issue is daisyden, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "Stonepia", "resolution": "\nShould be related to Driver problem, we will re-test it with the new driver.", "root_cause": "Driver problem", "state": "closed"}

### Merged Result:1136{"issue_number": 1136, "issue_description": "The feature, motivation and pitch\n\nIPEX model resnet50 main.py called torch.xpu.set_fp32_math_mode(torch.xpu.FP32MathMode.FP32) but stock pytorch does not have the API. We could also need to understand the impact of this setting to resnet50.\n\n### Alternatives\n_No response_\n\n### Additional context\n_No response_,\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1136. The reporter of the issue is daisyden, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "Stonepia", "resolution": "\nThis should be related to the IPEX-specific API, since we discussed to use OOB model scripts rather than IPEX scripts, we will close this issue.", "root_cause": "", "state": "closed"}

### Merged Result:1135{"issue_number": 1135, "issue_description": "Run ssd-mobilenetv1 on LNL Windows with https://github.com/intel-innersource/frameworks.ai.pytorch.gpu-models/SSD-MobileNetv1 on stock PyTorch got error\n\nfail(AttributeError: module 'torch.xpu' has no attribute 'locations_to_boxes').\n\nThis is because the model used an ipex customized op at [link](https://github.com/intel-innersource/frameworks.ai.pytorch.gpu-models/blob/daisyden/stock_pt/SSD-MobileNetv1/vision/ssd/ssd.py#L94). Do we need to support it or use an official version on ssd-mobilenet in torch_vision?\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1135. The reporter of the issue is daisyden, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "Stonepia", "resolution": "\nSince we are going to use OOB model scripts instead of the IPEX scripts, we will close this issue.", "root_cause": "", "state": "closed"}

### Merged Result:1133{"issue_number": 1133, "issue_description": "The reporter of the issue is xytintel, the assignee is xytintel, and the state of the issue is closed. The issue title is XPU operator performance enhancement, and the issue body is Content of #1133 is : #### Performance enhancement [not exposed]\n- [batch_normalization: Introduce vectorization optimization in the batch norm elementwise kernel](https://github.com/intel/torch-xpu-ops/pull/933)\n- [upsample_biliear2d channel last backward optimization](https://github.com/intel/torch-xpu-ops/pull/950)\n- [group norm forward vectorization optimization](https://github.com/intel/torch-xpu-ops/pull/1116)\n- [max pool operator level optimization](https://github.com/intel/torch-xpu-ops/pull/1127)", "reporter": "xytintel", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:1129{"issue_number": 1129, "issue_description": "inductor may pad mm for some shapes i.e replace matmul with zeros, cat, matmul and slice. On A100 fp16 fp_GPT2, this feature can improve 30% performance. Check if it's useful on XPU. https://github.com/pytorch/pytorch/blob/main/torch/_inductor/fx_passes/pad_mm.py\nGeneral guideline from onednn team: pad to a multiple of 64 bytes (1 cache line) in the unit-stride dimension, but try to avoid multiples of large powers of 2 (say 2048 bytes). So for bf16, 37 elements in contiguous dimension = 74 bytes -> pad to 128 bytes, but 1023 elements = 2046 bytes -> pad to 2048 + 64 bytes to avoid a multiple of 2048. It's enough to pad the tensor strides; you do not need to pad the tensor size itself if you don't want to.", "reporter": "jianyizh", "assignee": "jianyizh", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:1128{"issue_number": 1128, "issue_description": "Current sdpa will go into math path, which will always use fp32 even inputs are 16 bit. Compare to cuda, more patterns can be matched and this will cause low performance before we have sdpa kernel.\nFor example, GPT 2 can have 35% improvement if we don't match sdpa. https://github.com/pytorch/pytorch/blob/v2.5.1/torch/_inductor/fx_passes/fuse_attention.py#L815", "reporter": "jianyizh", "assignee": "jianyizh", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1125{"issue_number": 1125, "issue_description": "The reporter of the issue is daisyden, and the assignee is xytintel, and the state of the issue is open. The error message is NotImplementedError: Could not run 'aten::_to_sparse_csr' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was not implemented.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1125. The reporter of the issue is daisyden, and the assignee is xytintel, and the state of the issue is open.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:1124{"issue_number": 1124, "issue_description": "Precision issues depend on oneAPI. For extreme value processing, Numpy and XPU results are inconsistent, std operations get different behavior on std::complex operarands for extremal cases. 'test_reference_numerics_extremal_exp2_xpu_complex128', 'test_reference_numerics_extremal_exp2_xpu_complex64', 'test_reference_numerics_normal_nn_functional_tanhshrink_xpu_complex64', 'test_reference_numerics_extremal__refs_acos_xpu_complex64', 'test_reference_numerics_extremal__refs_acosh_xpu_complex64', 'test_reference_numerics_extremal__refs_asin_xpu_complex64', 'test_reference_numerics_extremal__refs_log_xpu_complex64', 'test_reference_numerics_extremal__refs_tanh_xpu_complex128', 'test_reference_numerics_extremal__refs_tanh_xpu_complex64', 'test_reference_numerics_extremal_acos_xpu_complex64', 'test_reference_numerics_extremal_acosh_xpu_complex64', 'test_reference_numerics_extremal_asin_xpu_complex64', 'test_reference_numerics_extremal_log_xpu_complex64', 'test_reference_numerics_extremal_tanh_xpu_complex128', 'test_reference_numerics_extremal_tanh_xpu_complex64', 'test_reference_numerics_large__refs_acosh_xpu_complex64', 'test_reference_numerics_large_acosh_xpu_complex64', 'test_reference_numerics_large_tanh_xpu_complex32' are fixed.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "", "root_cause": "Precision issues depend on oneAPI. For extreme value processing, Numpy and XPU results are inconsistent, std operations get different behavior on std::complex operarands for extremal cases.", "state": "open"}

### Merged Result:1122{"issue_number": 1122, "issue_description": "We got a report that on Ubuntu 24.10, the installation following https://dgpu-docs.intel.com/driver/client/overview.html#installing-client-gpus-on-ubuntu-desktop-24-10 will fail.\n\n# Reproduce Step\n1. Start from the clean machine, and install the driver following the above.\n2. `lspci` and `clinfo` will hang.\n\n# Root Cause\nThere may some misalignment about the kernel and driver package version.\nAccording to report, default 6.11.0-8 kernel version will fail. and 6.11.0-9 should pass.\n\nThe user need to upgrade kernel using the following command.\n\n```\nsudo apt-get upgrade linux-generic linux-headers-generic linux-image-generic\n``` \n\nHowever, we need to make sure that the default installation instructions work OOB. We need to verify that.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1122. The reporter of the issue is Stonepia, and the assignee is , and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "", "resolution": "\nThis should be related to the driver installation unaligned with the kernel. So close this.", "root_cause": "There may some misalignment about the kernel and driver package version. According to report, default 6.11.0-8 kernel version will fail. and 6.11.0-9 should pass. The user need to upgrade kernel using the following command.\n\n```sudo apt-get upgrade linux-generic linux-headers-generic linux-image-generic``` \n\nHowever, we need to make sure that the default installation instructions work OOB. We need to verify that.", "state": "closed"}

### Merged Result:1121{"issue_number": 1121, "issue_description": "The straight forward thought is kernel bundle is not device specific under a specific platform context (Like GPU platform). So we should not use `dev` (Existing WA for the https://github.com/intel/llvm/issues/15127) as a hint.\n\n 18   auto kid = ::sycl::get_kernel_id<KernelClass>();\n 24   auto kbundle = ::sycl::get_kernel_bundle<::sycl::bundle_state::executable>(\n 25       ctx, {dev}, {kid}); // Should not be device specific\n 26\n 27   ::sycl::kernel k = kbundle.get_kernel(kid);\n 28   return k.get_info<::sycl::info::kernel_device_specific::work_group_size>(dev); // Device specific. In additional, launching a kernel on a specific device is device specific.\n", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1120{"issue_number": 1120, "issue_description": "FP8 matmul compute wrong result in OneDNN 3.5 when the matrix contains fp8 maximum or minimum. This bug is fixed in oneDNN 3.6. This OP will be suspended until stock pytorch update oneDNN.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "closed", "root_cause": "This bug is fixed in oneDNN 3.6.", "state": "closed"}

### Merged Result:1113{"issue_number": 1113, "issue_description": "Execute Triton XPU tutorial:\nhttps://raw.githubusercontent.com/intel/intel-xpu-backend-for-triton/refs/heads/main/python/tutorials/01-vector-add.py\npython 01-vector-add.py\n\nResult:\nterminate called after throwing an instance of 'sycl::_V1::exception'\n  what():  Native API failed. Native API returns: 37 (UR_RESULT_ERROR_UNINITIALIZED)\nAborted (core dumped)\nThe issue is no longer reproducible with the latest nightly wheels, closing this issue. Will need a separate issue for `AttributeError`.", "reporter": "pbchekin", "assignee": "ratnampa", "resolution": "\nThe issue is no longer reproducible with the latest nightly wheels.", "root_cause": "This issue is related to the use of nightly wheels and the native API failing. The reporter of the issue is pbchekin, and the assignee is ratnampa, and the state of the issue is closed.", "state": "closed"}

### Merged Result:1109{"issue_number": 1109, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1109. The reporter of the issue is jianyizh, and the assignee is xytintel, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1109. The reporter of the issue is jianyizh, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "jianyizh", "assignee": "xytintel", "resolution": "closed\nThe issue is closed.", "root_cause": "The issue is closed.", "state": "closed"}### Result:1108 failed to extract

### Merged Result:1094{"issue_number": 1094, "issue_description": "DNNL does not support bf16/f16 backward on the platform with avx2_vnni_2\nclosed as current CI does not have the same issue.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "closed\nclosed as current CI does not have the same issue.", "root_cause": "This issue is related to the DNNL backend and the platform configuration. The DNNL backend does not support bf16/f16 backward operations on the platform with avx2_vnni_2 configuration. The issue has been closed as a known limitation of the platform configuration.", "state": "closed"}

### Merged Result:1093{"issue_number": 1093, "issue_description": "test_reductions_xpu.py::TestReductionsXPU::test_mode_large_xpu_float32\nIt can be reproduced on this case, we expect to get 1 in each row of value variable, but got 0.\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='xpu:0')", "reporter": "daisyden", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:1092{"issue_number": 1092, "issue_description": "When running the example code on XPU, a segmentation fault error occurred. The debug message with gdb shows that the error is caused by the mlir::triton::intel::convertFp32ToBf16 function, which is not found in the specified file. The reporter of the issue is faaany, and the assignee is Stonepia, and the state of the issue is closed.\nThe issue is related to the installation and usage of `pytorch-triton-xpu` and `triton` packages, which caused a `Segmentation fault (core dumped)` error. The root cause is a bug in the Triton's latest commit, which was fixed by the commit `9b5b553c7c90b917eed839d69f9516087ebb970d`.", "reporter": "faaany", "assignee": "Stonepia", "resolution": "\nClose this as already implemented.", "root_cause": "A bug in the Triton's latest commit, which was fixed by the commit `9b5b553c7c90b917eed839d69f9516087ebb970d`.", "state": "closed"}

### Merged Result:1080{"issue_number": 1080, "issue_description": "OneDNN new Failures (vs Baseline)\n|         E2E   Cases         |                        Error Type                        |                                                                                   oneDNN       (Failed model)                                                                                   |                                                                  Baseline      (Failed model)                                                                 |\n|:---------------------------:|:--------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| hf_n10_inference_eager_fp32 | Native API   returns: -999       (Unknown PI error)      |                                      BlenderbotForCausalLM      DebertaV2ForMaskedLM      DebertaV2ForQuestionAnswering      MBartForConditionalGeneration                                      |                                       BlenderbotForCausalLM      DebertaV2ForMaskedLM      DebertaV2ForQuestionAnswering                                      |\n|                             | eager_two_runs_differ                                    |                                                                 PegasusForConditionalGeneration      RobertaForQuestionAnswering                                                                |                                                                                                                                                               |\n|                             | Quit without pass                                        |                                                                                                                                                                                                 | MBartForConditionalGeneration                                                                                                                                 |\n| hf_n10_inference_eager_fp16 | eager_two_runs_differ                                    | CamemBert      PegasusForConditionalGeneration                                                                                                                                                  |                                                                                                                                                               |\n| hf_n10_inference_eager_bf16 | eager_two_runs_differ                                    | ConditionalGeneration      M2M100ForConditionalGeneration      MBartForConditionalGeneration      XGLMForCausalLM                                                                               |                                                                                                                                                               |\n|   hf_n10_train_eager_fp32   | Native API returns: -999       (Unknown PI error)        | BartForConditionalGeneration      BlenderbotForCausalLM      DebertaV2ForMaskedLM      DebertaV2ForQuestionAnswering     MBartForConditionalGeneration      OPTForCausalLM      XGLMForCausalLM | BartForConditionalGeneration      BlenderbotForCausalLM      DebertaV2ForMaskedLM      MBartForConditionalGeneration      OPTForCausalLM      XGLMForCausalLM |\n|                             | Native API returns: -5       (PI_ERROR_OUT_OF_RESOURCES) |                                                                                                                                                                                                 | DebertaV2ForQuestionAnswering                                                                                                                                 |\n|                             | eager_two_runs_differ                                    | BertForMaskedLM      BlenderbotSmallForConditionalGeneration                                                                                                                                    |                                                                                                                                                               |\n|   hf_n10_train_eager_fp16   | eager_two_runs_differ                                    | MegatronBertForCausalLM      PLBartForConditionalGeneration                                                                                                                                     |                                                                                                                                                               |\n|   hf_n10_train_eager_bf16   | eager_two_runs_differ                                    | AllenaiLongformerBase      MegatronBertForQuestionAnswering      OPTForCausalLM                                                                                                                 |                                                                                                                                                               |\nOneDNN upgrade introduces new failures when testing UT and E2E (huggingface models)\ntest_autograd_xpu.py::TestAutograd::test_multi_grad_all_hooks - subpro...FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_addcdiv_fastpath_inplace_xpu_float16FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_addcdiv_fastpath_inplace_xpu_float32FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_addcdiv_slowpath_outplace_xpu_complex64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_addcmul_fastpath_inplace_xpu_complex64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_addcmul_fastpath_outplace_xpu_complex64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_add_fastpath_outplace_xpu_int64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_add_slowpath_outplace_xpu_complex64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_add_slowpath_outplace_xpu_int64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_div_fastpath_outplace_xpu_float32FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_expm1_slowpath_inplace_xpu_complex64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_lerp_fastpath_outplace_xpu_float32FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_lerp_slowpath_inplace_xpu_complex64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_lerp_slowpath_outplace_xpu_complex64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_parity__foreach_maximum_slowpath_inplace_xpu_int64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_pointwise_op_with_tensor_of_scalarlist_overload__foreach_addcdiv_is_fastpath_True_xpu_complex64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_pointwise_op_with_tensor_of_scalarlist_overload__foreach_addcdiv_is_fastpath_True_xpu_float32FAILEDtest_foreach_xpu.py::TestForeachXPU::test_pointwise_op_with_tensor_of_scalarlist_overload__foreach_addcmul_is_fastpath_True_xpu_complex64FAILEDtest_foreach_xpu.py::TestForeachXPU::test_pointwise_op_with_tensor_of_scalarlist_overload__foreach_addcmul_is_fastpath_True_xpu_float32\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1080. The reporter of the issue is libohao1201, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "libohao1201", "assignee": "Stonepia", "resolution": "\nclosed\n\nAfter triaging, there should not be related to oneDNN upgrade issue. There is no regression. We will track those issues in other thread. Close this issue.", "root_cause": "unknown", "state": "closed"}

### Merged Result:1078{"issue_number": 1078, "issue_description": "Run TestFakeTensor with xpu we got a lot of errors of shapes are not equal. For example:\n\n======================================================================\nERROR: test_fake_crossref_backward_amp_nn_functional_multilabel_soft_margin_loss_xpu_float32 (__main__.TestFakeTensorXPU)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/gta/miniforge3/envs/daisy_upstream/lib/python3.10/site-packages/torch/_subclasses/fake_utils.py\", line 182, in __torch_dispatch__\n    torch._prims.utils.compare_tensor_meta(\n  File \"/home/gta/miniforge3/envs/daisy_upstream/lib/python3.10/site-packages/torch/_prims_common/__init__.py\", line 156, in compare_tensor_meta\n    raise AssertionError(msg)\nAssertionError: Shapes torch.Size([0]) and torch.Size([5]) are not equal!\n\nAnother issue is in backward:\n======================================================================\nERROR: test_fake_crossref_backward_amp_nn_functional_multilabel_soft_margin_loss_xpu_float32 (__main__.TestFakeTensorXPU)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/gta/miniforge3/envs/daisy_upstream/lib/python3.10/site-packages/torch/_subclasses/fake_utils.py\", line 182, in __torch_dispatch__\n    torch._prims.utils.compare_tensor_meta(\n  File \"/home/gta/miniforge3/envs/daisy_upstream/lib/python3.10/site-packages/torch/_prims_common/__init__.py\", line 156, in compare_tensor_meta\n    raise AssertionError(msg)\nAssertionError: Shapes torch.Size([0]) and torch.Size([5]) are not equal!\n\nTo reproduce, change test/test_ops.py instantiate_device_type_tests(TestFakeTensor, globals()) ==> instantiate_device_type_tests(TestFakeTensor, globals(), allow_xpu=True) and run\n\nPYTORCH_TEST_WITH_SLOW=1 python  test_ops.py -k TestTags\n\nThe results for real tensors are aligned with CUDA, but not for fake tensors. The root cause is that there is some CUDA specific codes in aten::log_sigmoid_forward.", "reporter": "daisyden", "assignee": "chunhuanMeng", "resolution": "\npr link:https://github.com/pytorch/pytorch/pull/141333", "root_cause": "there is some CUDA specific codes in aten::log_sigmoid_forward", "state": "closed"}

### Merged Result:1077{"issue_number": 1077, "issue_description": "The copy_() function is not utilizing the full theoretical bandwidth. For example, it is only utilizing 40-50% of the theoretical bandwidth.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1077. The reporter of the issue is cfgfung, and the assignee is cfgfung, and the state of the issue is closed.", "reporter": "cfgfung", "assignee": "cfgfung", "resolution": "The issue has been closed.\n", "root_cause": "The problem is related to the vectorized kernels and the performance is not as expected.", "state": "closed"}

### Merged Result:1071{"issue_number": 1071, "issue_description": "AssertionError: \"Simulate error\" does not match \"grad can be implicitly created only for scalar outputs\" in test_reentrant_parent_error_on_cpu_xpu\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1071. The reporter of the issue is PenghuiCheng, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "guangyey", "resolution": "\nClosed as completed.", "root_cause": "The issue is caused by the timing of autograd reentrant feature, and it can't be reproduced on the local machine.", "state": "closed"}

### Merged Result:1061{"issue_number": 1061, "issue_description": "The test_compare_cpu_grid_sampler_2d_xpu_bfloat16 and test_compare_cpu_grid_sampler_2d_xpu_float16 tests are failing with the following error message: 'RuntimeError: expected scalar type Half but found Float' for test_compare_cpu_nn_functional_grid_sample_xpu_bfloat16 and test_compare_cpu_nn_functional_grid_sample_xpu_float16, the tests are comparing the output of grid_sample_2d function on CPU and XPU, and the input is in fp16 or bf16 format. The root cause is that the grid_sample_2d function on XPU does not support fp16 or bf16 input format, and the resolution is to use float32 input format for the tests.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1061. The reporter of the issue is xytintel, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "xytintel", "assignee": "daisyden", "resolution": "Use float32 input format for the tests.\ncompleted", "root_cause": "The grid_sample_2d function on XPU does not support fp16 or bf16 input format.", "state": "closed"}

### Merged Result:1059{"issue_number": 1059, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1059. The reporter of the issue is fengyuan14, and the assignee is majing921201, and the state of the issue is open.", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1056{"issue_number": 1056, "issue_description": "Support aten::_convert_weight_to_int4pack.\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1056. \nThe reporter of the issue is fengyuan14, \nand the assignee is xytintel,\nand the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1056. The reporter of the issue is fengyuan14, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "closed\nPR ready https://github.com/intel/torch-xpu-ops/pull/1035", "root_cause": "", "state": "closed"}

### Merged Result:1055{"issue_number": 1055, "issue_description": "We need op record_stream that is widely used in DDP\\FSDP, and issue body Content of #1055 is : ### \ud83d\ude80 The feature, motivation and pitch\nAs titled.\nCould we implement op `aten::record_stream`?\n\ncc @zhangxiaoli73 \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1055. The reporter of the issue is guangyey, and the assignee is , and the state of the issue is closed.", "reporter": "guangyey", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}### Result:1054 failed to extract

### Merged Result:1053{"issue_number": 1053, "issue_description": "index_select_xpu cause an IPEX UT fail.  In IPEX2.5, we override this Ops with IPEX implementation to make this UT pass. \n\nipex/tests/gpu/example/test_fp8_index_select.py::TestTorchMethod::test_index_select \n\n'index_select_xpu' not implemented for 'Float8_e4m3fn'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1053. The reporter of the issue is LuFinch, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "LuFinch", "assignee": "daisyden", "resolution": "\nmerged", "root_cause": "created a PR for index_select, https://github.com/intel/torch-xpu-ops/pull/1081/files", "state": "closed"}

### Merged Result:1052{"issue_number": 1052, "issue_description": "Embedding_bag_out does not have boundary check and causes IPEX UT fail\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1052. The reporter of the issue is LuFinch, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "LuFinch", "assignee": "daisyden", "resolution": "In IPEX2.5, we override this Ops with IPEX implementation to make this UT pass\npr merged", "root_cause": "Ops in torch-xpu-ops do not have this check and hence fail in this UT", "state": "closed"}

### Merged Result:1048{"issue_number": 1048, "issue_description": "test_non_standard_bool_values failed with view created from torch.randint(), but passed with view created from torch.ones(). Is it a problem related to c10 load? @xytintel", "reporter": "daisyden", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}### Result:1027 failed to extract

### Merged Result:1025{"issue_number": 1025, "issue_description": "Clarify branch policy of torch-xpu-ops repo - what's viable/strict branch?,\n\nIn particular, I see that PyTorch main branch currently points to the https://github.com/intel/torch-xpu-ops/tree/viable/strict of torch-xpu-ops while I was thought it should point to `main` unless we stabilize code prior to release in which case it would point to one of release branches.\n\n* https://github.com/pytorch/pytorch/blob/3f9f6048dabeb2adce6dd112371c3caff36e8c39/third_party/xpu.txt#L1\n\nExtract the github issue description with error message information from issue tile and issue body, if possible also extract the resolution and root cause information.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1025. The reporter of the issue is dvrogozh, and the assignee is EikanWang, and the state of the issue is open.", "reporter": "dvrogozh", "assignee": "EikanWang", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:1023{"issue_number": 1023, "issue_description": "To port upstream UT to XPU backend we requires the xpu backend support for these APIs : * torch.cuda.amp.autocast for test_ops.py test_fake_crossref_backward_amp * torch.testing._internal.common_device_type._has_sufficient_memory for test_nn.py::TestNNDeviceType::test_avg_pool_large_tensor\nClose as implemented.", "reporter": "daisyden", "assignee": "riverliuintel", "resolution": "\nClose as implemented.", "root_cause": "", "state": "closed"}

### Merged Result:1022{"issue_number": 1022, "issue_description": "The code provided in the issue is not working as expected. It seems that the sort function is not working as expected for bool type tensor. The output is not as expected. The code is supposed to sort the tensor and compare the result with the sorted tensor on CPU device. However, the output is not matching. The issue is reproducible on both XPU and CPU devices. The issue is closed, but the root cause is not clear. The reporter and assignee are not providing any further information.", "reporter": "guizili0", "assignee": "xytintel", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:1016{"issue_number": 1016, "issue_description": "Performance: Host overhead: Severe host overhead in sycl::get_kernel_bundle. 40us overhead is not acceptable for some single batch inference cases, since latency of kernels might be less than 10us. CUDA runtime usually spends ~6us for a kernel launch. https://github.com/intel/llvm/issues/15824", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1013{"issue_number": 1013, "issue_description": "Import torch not assert in windows, if install torch XPU on a host without driver installed.\nDone in stock PT main branch.", "reporter": "riverliuintel", "assignee": "ratnampa", "resolution": "closed\nDone in stock PT main branch.", "root_cause": "The reporter of the issue is riverliuintel, and the assignee is ratnampa, and the state of the issue is closed.", "state": "closed"}

### Merged Result:1012{"issue_number": 1012, "issue_description": "Logcumsumexp has different results between CPU and XPU on BF16/Complex64/Complex128, the issue is closed, the reporter is LuFinch, the assignee is , the state of the issue is closed, mismatched elements: 1 / 3 (33.3%) for complex64, the greatest absolute difference is nan at index (2,), the greatest relative difference is nan at index (2,), the input is [1e3 + 0j, 1e-18 + 1e4j, 1e2 + 1e-8j], the cpu output is [1000.+0.j, 1000.+0.j, 1000.+0.j], the cuda output is [1000.+0.j, 1000.+0.j, nan + nanj], the xpu output is [1000.+0.j, 1000.+0.j, nan + nanj], the root cause is the accumulated order of xpu scan kernel, the resolution is the xpu scan kernel would firstly reduce input[1], input[2], then reduce input[0], input[2] in this case, the cpu kernel will output [nan, nanj] when directly calculating logcumsumexp(input[1], input[2]).", "reporter": "LuFinch", "assignee": "", "resolution": "", "root_cause": "the accumulated order of xpu scan kernel", "state": "closed"}

### Merged Result:1011{"issue_number": 1011, "issue_description": "KLDivLoss function in pytorch always fallsback to CPU, and issue body Content of #1011 is : ### \ud83d\udc1b Describe the bug\n\nGreetings:\n\nI am trying to use the `torch.nn.KLDivLoss` function on Intel's Max 1550 GPU; however, the function keeps falling back to execute on the CPU.\n\nBelow is a reproducer:\n\n```py\nimport torch\n\nkl_div = torch.nn.KLDivLoss(reduction=\nThe reporter of the issue is jgtong, and the assignee is jgtong, and the state of the issue is closed.", "reporter": "jgtong", "assignee": "jgtong", "resolution": "\nThe issue was resolved by the commit 804a03b76e6b1270327f3f6ddbe58b6ffba5d30e in xpu-ops.", "root_cause": "The KLDivLoss function in PyTorch does not support the reduction='batchmean' option on the XPU backend. The XPU backend only supports reduction='sum' and reduction='none'. The reporter of the issue is jgtong, and the assignee is jgtong, and the state of the issue is closed.", "state": "closed"}

### Merged Result:1009{"issue_number": 1009, "issue_description": "The reporter of the issue is riverliuintel, and the assignee is fengyuan14, and the state of the issue is open.", "reporter": "riverliuintel", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1008{"issue_number": 1008, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1008. The reporter of the issue is riverliuintel, and the assignee is fengyuan14, and the state of the issue is closed.\n", "reporter": "riverliuintel", "assignee": "fengyuan14", "resolution": "closed\n", "root_cause": "", "state": "closed"}

### Merged Result:1007{"issue_number": 1007, "issue_description": "There are two scenarios for PyTorch XPU release. 1) pip install torch wheels in one host machine without GPU driver installed. Import torch will fallback to CPU. 2) install driver and pip install torch for AI workload run on GPU. 3) install driver, install deep-learning-essential bundle and pip install torch can work well on GPU. 4)  install driver, install deep-learning-essential bundle and source build pyTorch. OS: Linux, Windows\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1007. The reporter of the issue is riverliuintel, and the assignee is chuanqi129, and the state of the issue is closed.", "reporter": "riverliuintel", "assignee": "chuanqi129", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:1006{"issue_number": 1006, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1006. The reporter of the issue is riverliuintel, and the assignee is chuanqi129, and the state of the issue is closed.\nTorchvision and audio are enabled in Windows CD.", "reporter": "riverliuintel", "assignee": "chuanqi129", "resolution": "\ndone", "root_cause": "", "state": "closed"}

### Merged Result:1005{"issue_number": 1005, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1005. The reporter of the issue is riverliuintel, and the assignee is ZhiweiYan-96, and the state of the issue is open. The github issue title [PT2.6] LLM customized kernels (GEMM INT4 serves for Torchao), and issue body Content of #1005 is : ### \ud83d\ude80 The feature, motivation and pitch Integrate oneDNN GEMM INT4 kernels, and serves for Torchao LLM usage. It requires pass UT and example workloads usage. ### Alternatives _No response_ ### Additional context _No response_, Extract the github issue description with error message information from issue tile and issue body, if possible also extract the resolution and root cause information. 0} {", "reporter": "riverliuintel", "assignee": "ZhiweiYan-96", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:1004{"issue_number": 1004, "issue_description": "Analyze Triton kernels data and report to Triton XPU.\n\n1. Recollect reasonable competitive GPU performance data\n2. Use TorchInductor built-in benchmark tool to detect slower XPU triton kernels.\n\n### \ud83d\ude80 The feature, motivation and pitch\n\nAnalyze Triton kernels data and report to Triton XPU.\n\n1. Recollect reasonable competitive GPU performance data\n2. Use TorchInductor built-in benchmark tool to detect slower XPU triton kernels.\n\n### Alternatives\n\n_No response_\n\n### Additional context\n\n_No response_,\nscatter op issue: https://github.com/intel/intel-xpu-backend-for-triton/issues/2665", "reporter": "riverliuintel", "assignee": "retonym", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:1003{"issue_number": 1003, "issue_description": "Request INT8 quantization (PT2E) feature on Linux. It requires, implement PT2E infrastructure for Intel GPU path, complete essential oneDNN, Triton quantized INT8 ops, pass benchmark models quantization testing. And complete essential docs changes\nfunc done in PyTorch main branch.", "reporter": "riverliuintel", "assignee": "ZhiweiYan-96", "resolution": "closed\nfunc done in PyTorch main branch.", "root_cause": "", "state": "closed"}

### Merged Result:1002{"issue_number": 1002, "issue_description": "The reporter of the issue is riverliuintel, the assignee is etaf, and the state of the issue is closed.\ndone", "reporter": "riverliuintel", "assignee": "etaf", "resolution": "closed\nready in PT2.7", "root_cause": "", "state": "closed"}

### Merged Result:1001{"issue_number": 1001, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1001. The reporter of the issue is riverliuintel, and the assignee is xytintel, and the state of the issue is closed.\nThe coverage goal of the operation has been met.", "reporter": "riverliuintel", "assignee": "xytintel", "resolution": "closed\nThe coverage goal of the operation has been met.", "root_cause": "", "state": "closed"}

### Merged Result:1000{"issue_number": 1000, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/1000. The reporter of the issue is riverliuintel, and the assignee is , and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/1000. The reporter of the issue is riverliuintel, and the assignee is , and the state of the issue is closed.", "reporter": "riverliuintel", "assignee": "", "resolution": "closed\nclose this", "root_cause": "", "state": "closed"}

### Merged Result:999{"issue_number": 999, "issue_description": "The Windows build log size is too big to open in web, which impact issue triage in CI. Need to enhance windows build log and clean the warning message in Windows. \nThe excessive verbosity was caused by SYCL compiler `/clang:-MD` used with `-fsycl-host-compiler=cl.exe`, which invoked `-E`. `-E` emits the preprocessed source code to stdout, causing the excessive verbosity.", "reporter": "riverliuintel", "assignee": "min-jean-cho", "resolution": "closed\nThe issue has been resolved with SYCL compiler of oneAPI 2025.", "root_cause": "/clang:-MD used with `-fsycl-host-compiler=cl.exe`, which invoked `-E`.", "state": "closed"}

### Merged Result:998{"issue_number": 998, "issue_description": "The reporter of the issue is riverliuintel, the assignee is Stonepia, and the state of the issue is closed.", "reporter": "riverliuintel", "assignee": "Stonepia", "resolution": "closed", "root_cause": "", "state": "closed"}

### Merged Result:997{"issue_number": 997, "issue_description": "Reach comparable UT pass rate with PVC on LNL Windows\n99% pass rate", "reporter": "riverliuintel", "assignee": "Stonepia", "resolution": "\nDone", "root_cause": "", "state": "closed"}

### Merged Result:987{"issue_number": 987, "issue_description": "Build with new oneAPI will got failed with WERROR=1\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/987. The reporter of the issue is mengfei25, and the assignee is mengfei25, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "mengfei25", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/1070", "root_cause": "The error is caused by the incompatibility between the new oneAPI and the existing codebase. The reporter has provided a link to the github issue where the issue was discussed and a fix was proposed. The fix involves updating the code to be compatible with the new oneAPI.", "state": "closed"}

### Merged Result:986{"issue_number": 986, "issue_description": "The operator 'c10d::allgather_' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/986. The reporter of the issue is zhiyuan1i, and the assignee is Chao1Han, and the state of the issue is open.", "reporter": "zhiyuan1i", "assignee": "Chao1Han", "resolution": "\n", "root_cause": "zhiyuan1i does not have an Intel multi-GPU environment at hand to test the issue.", "state": "open"}

### Merged Result:982{"issue_number": 982, "issue_description": "Whether `CompositeExplicitAugograd` codegen flag is needed requires further investigation.\n\n![image](https://github.com/user-attachments/assets/acc52336-770f-4cef-a546-c75d28e8731c)\n\n\nThe issue is about the `CompositeExplicitAutograd` key in the `derivatives.yaml` file, which is used for kernels that work for all backends but require an explicit definition of backward function to support autograd. The most typical use is for delegating functions. The conclusion is that we don't need it.", "reporter": "xytintel", "assignee": "xytintel", "resolution": "\nThe issue is resolved and the conclusion is that we don't need it.", "root_cause": "", "state": "closed"}

### Merged Result:979{"issue_number": 979, "issue_description": "E2E Accuracy] timm jx_nest_base amp_fp16 inference accuracy failed randomly, Details in https://github.com/intel/torch-xpu-ops/actions/runs/11361002852 dev | name | batch_size | accuracy xpu | jx_nest_base | 8 | fail_accuracy xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | pass xpu | jx_nest_base | 8 | fail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/979. The reporter of the issue is mengfei25, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\n", "root_cause": "The issue is still a random accuracy issue on the local environment and Triton does not support deterministic.", "state": "closed"}

### Merged Result:978{"issue_number": 978, "issue_description": "2.5 aten::linear also introduced an additional aten::copy_, that make aten::linear latency dropped from 308us to 426us.\nAutocast difference between IPEX and torch-xpu-ops leads to the additional copy. According to the current requirement, it is not a defect.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed\nNot a defect", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/978. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed.", "state": "closed"}

### Merged Result:977{"issue_number": 977, "issue_description": "2.5 aten::layer_norm introduced 3 aten::copy_, that make the latency dropped from 150us to 401us.\nAdditional three copies are introduced by Autocast. torch-xpu-ops aligns Autocast policy with PyTorch CUDA, where LayerNorm requires FP32 in computation. And in IPEX, LayerNorm could stay on BF16 according to IPEX custom Autocast policy.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed\nClose won't fix, as currently we follow rules to align with CUDA impl and guarantee accuracy. If we have performance consideration in future, we can file new issue.", "root_cause": "Performance: LayerNorm: Worse host overhead due to additional copies introduced", "state": "closed"}

### Merged Result:970{"issue_number": 970, "issue_description": "CPU time as below,\n\noverride             aten::sum         2.52%      24.765ms         4.07%      39.978ms      60.849us \nnon-override         aten::sum         4.49%      46.832ms         5.74%      59.905ms      91.180us\n\nSame root cause, https://github.com/intel/llvm/issues/15824", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "\n", "root_cause": "https://github.com/intel/llvm/issues/15824", "state": "open"}

### Merged Result:969{"issue_number": 969, "issue_description": "Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg      Self XPU    Self XPU %     XPU total  XPU time avg    # of Calls\nnon override         aten::nonzero         5.50%      63.456ms        54.60%     630.302ms     489.365us       5.160ms         4.35%      34.468ms      26.761us          1288\noverride             aten::nonzero         5.40%      58.551ms        52.64%     570.870ms     443.222us       6.688ms         5.55%      34.737ms      26.970us          1288\nThe low performance is caused by SYCL API, which we used to query kernel specific max work group size. We file issue to compiler to track this issue.", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "\nThe low performance is caused by SYCL API, which we used to query kernel specific max work group size. We file issue to compiler to track this issue.", "root_cause": "The low performance is caused by SYCL API, which we used to query kernel specific max work group size.", "state": "open"}

### Merged Result:964{"issue_number": 964, "issue_description": "Unit test: Port all necessary unit tests from test/test_cuda.py, and issue body Content of #964 is : ### \ud83d\ude80 The feature, motivation and pitch\nWe need to expand our testing scope as much as possible due to the current requirement of 80% coverage for CUDA.\n### Alternatives\n_No response_\n### Additional context\n_No response_,", "reporter": "xytintel", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "open"}

### Merged Result:957{"issue_number": 957, "issue_description": "test_autograd_xpu.py::TestAutograd::test_node_ordering_when_none_returned - AssertionError: Torch not compiled with CUDA enabled\n\ntest_linalg_xpu.py::TestLinalgXPU::test__int_mm_errors_xpu - AssertionError: RuntimeError not raised by <lambda>\nreport in https://github.com/intel/torch-xpu-ops/issues/821 and https://github.com/intel/torch-xpu-ops/issues/814", "reporter": "PenghuiCheng", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:956{"issue_number": 956, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/956. The reporter of the issue is PenghuiCheng, and the assignee is , and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:954{"issue_number": 954, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/954. The reporter of the issue is gglin001, and the assignee is fengyuan14, and the state of the issue is open.\nClang compiler fails due to duplicate symbols in torchgen.gen headers and lld linker issues.", "reporter": "gglin001", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "Clang compiler fails due to duplicate symbols in torchgen.gen headers and lld linker issues.", "state": "open"}

### Merged Result:942{"issue_number": 942, "issue_description": "F.scaled_dot_product_attention need XETLA support to avoid the SD and Bert training regression in IPEX 2.5 test. IPEX got this failure: [PVC][PT2.5][Bundle0.5.3.36/2024.2.1] stable-diffusion train 10% perf regression. From oneDNN verbose SD convolution time in 2.5 is the same as 2.3. While there are some additional gemm ops in 2.5 as mentioned by Shufan, and we can see those ops takes a lot of time, like below table. The issue is introduced by F.scaled_dot_product_attention, we will need a patch from <a class=\"user-hover\" tabindex=\"-1\" contenteditable=\"false\" href=\"https://jira.devtools.intel.com/secure/ViewProfile.jspa?name=majing\" rel=\"majing\">Ma, Jing1</a> to enable XTLA otherwise a naive implementation is used. Bert has the similar issue\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/942. The reporter of the issue is daisyden, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "majing921201", "resolution": "\nClose it due to product plan change", "root_cause": "", "state": "closed"}

### Merged Result:941{"issue_number": 941, "issue_description": "ipex convolution switched to stock pytorch, because we don't have tf32 support this case got 46% regresion compared to ipex 2.3 \n\nRVP | resnet50_tf32_train_plain_nhwc | High | 256 | FAIL | 1650.43 | 1672.41 | 902.86 | -46%\n\nHere is a comparison of verbose:\n\nconvolution | jit:ir | forward_training | src_f32::blocked:acdb::f0 wei_f32::blocked:acdb::f0 bia_undef::undef::: dst_f32::blocked:acdb::f0 | attr-scratchpad:user attr-fpmath:tf32 | alg:convolution_direct | mb16_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1 | 0.087158\n\nconvolution | jit:ir | forward_training | src_f32::blocked:acdb::f0 wei_f32::blocked:acdb::f0 bia_undef::undef::: dst_f32::blocked:acdb::f0 | attr-scratchpad:user | alg:convolution_direct | mb16_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1 | 0.196045\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/941. The reporter of the issue is daisyden, and the assignee is ZhiweiYan-96, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:939{"issue_number": 939, "issue_description": "Performance: Improve UpsampleBilinear forward backward performance to be on-par as oneDNN., Comparing with oneDNN impl, torch-xpu-ops gets gap on the performance of UpsampleBilinear forward kernel and backward kernel.\nThe reporter of the issue is fengyuan14, and the assignee is majing921201, and the state of the issue is open.", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "\n", "root_cause": "The performance of the channels last kernel is on-par with cuda, but still has a gap with oneDNN.", "state": "open"}

### Merged Result:938{"issue_number": 938, "issue_description": "Performance: Evaluate MaxPool2d forward backward performance gap between IPEX.,\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/938. \n\nThe reporter of the issue is fengyuan14, \nand the assignee is fengyuan14,\nand the state of the issue is closed.\n\nThis is the github issue title Performance: Evaluate MaxPool2d forward backward performance gap between IPEX.,\n\nContent of #938 is : ### \ud83d\ude80 The feature, motivation and pitch\n- Backward: Torch-xpu-ops align with CUDA implementation, using deterministic algorithm.\n- Forward: To investigate the gap, after PTI is fixed.\n\n### Alternatives\n_No response_\n\n### Additional context\n_No response_,\nThe performance gap between pointnet_bf16_ipex25_1108 and pointnet_bf16_ipex25_1108_stock_pytorch is due to the use of IPEX's max_pool2d function, which has a special handling path for certain cases that stock PyTorch's max_pool2d does not have. This special handling path in IPEX provides better parallelism.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nThe root cause of the performance gap is that IPEX has a special handling path for certain cases which stock PyTorch's max_pool2d does not have. This special handling path in IPEX provides better parallelism. A pull request to fix this issue has been merged into the main branch.", "root_cause": "IPEX has a special handling path for certain cases which stock PyTorch's max_pool2d does not have. This special handling path in IPEX provides better parallelism.", "state": "closed"}

### Merged Result:937{"issue_number": 937, "issue_description": "Performance: Improve BatchNormalization forward/backward to align with oneDNN implementation.\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/937. \nThe reporter of the issue is fengyuan14, \nand the assignee is xytintel,\nand the state of the issue is closed.\n\nThis is the github issue title Performance: Improve BatchNormalization forward/backward to align with oneDNN implementation.,\nand issue body Content of #937 is : ### \ud83d\ude80 The feature, motivation and pitch\n\nBasing on the consideration of accuracy, we followed the PyTorch CUDA implementation, using Welford algorithm and similar kernel template. Will improve the kernel template with vectorized load/store.\n\n### Alternatives\n_No response_\n\n### Additional context\n_No response_,\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/937. The reporter of the issue is fengyuan14, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "closed\nMerged", "root_cause": "", "state": "closed"}

### Merged Result:928{"issue_number": 928, "issue_description": "test_dataloader UT failed in CI\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/928. The reporter of the issue is majing921201, and the assignee is PenghuiCheng, and the state of the issue is closed.", "reporter": "majing921201", "assignee": "PenghuiCheng", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:922{"issue_number": 922, "issue_description": "Unittest: New failures after PyTorch uplift\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/922. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nisin cases are passed on pytorch 41977a05314bbf537e1c5d6cf5916a368d1907d9 and torch-xpu-ops 999094bc948b3afd21162784dead1e765c60a376.", "root_cause": "Pr for unique: https://github.com/intel/torch-xpu-ops/pull/963", "state": "closed"}

### Merged Result:919{"issue_number": 919, "issue_description": "We have witnessed that when running models, there are some warnings from Triton, that would be like this:\n\n(I): Detected 9472 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 512 spills\n(I): Detected 20032 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 10816 spills\n(I): Detected 33600 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 25408 spills\n\nThis is because we didn't set the `grf_mode` in the triton config, and there are register spills exceeding the thresh_hold. Thus it triggers an automatic using large grf mode re-compile for the Triton kernel.\n\nThis is the expected behavior. We have two options:\n1. Set the `grf_mode=auto` in inductor side. So that when there is xpu, the `triton.Config` will have this kwarg.\n2. Discuss with the Triton team about hiding this from end users. These outputs should be treated as warnings.\n\nAfter the offline discussion, we think option 2 is better, because we need to keep from the inductor side,  that there will be no difference between XPU and CUDA/HIP. We wish to always keep the same config for all kinds of devices. The \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/919. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nhttps://github.com/intel/intel-xpu-backend-for-triton/pull/2385", "root_cause": "", "state": "closed"}

### Merged Result:918{"issue_number": 918, "issue_description": "AssertionError: Get None or [] without decomp\n", "reporter": "yuchengliu1", "assignee": "PenghuiCheng", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:913{"issue_number": 913, "issue_description": "The accuracy of the following models failed in the timm models: botnet26t_256, convmixer_768_32, convnext_base, cspdarknet53, eca_botnext26ts_256, eca_halonext26ts, fbnetv3_b, gluon_inception_v3, lcnet_050, levit_128, mixer_b16_224, mobilenetv2_100, mobilevit_s, poolformer_m36, res2net50_14w_8s, resnest101e, rexnet_100, sebotnet33ts_256, tf_efficientnet_b0, tinynet_a, tnt_s_patch16_224. The models failed in both training and inference.\nThe reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nThe issue was closed by the reporter and the assignee.", "root_cause": "The issue was related to the difference in the results between Ubuntu 22.04 and Ubuntu 24.04, and the reporter compared the results with Ubuntu 22.04 and found that the results were different.", "state": "closed"}

### Merged Result:912{"issue_number": 912, "issue_description": "torchbench accuracy failed\nE2E accuracy issue on Ubuntu 22.04", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nThe issue will be closed in Ubuntu 24.10", "root_cause": "The issue is related to the version of Ubuntu and the specific model being tested.", "state": "closed"}

### Merged Result:911{"issue_number": 911, "issue_description": "Accuracy failed for key name albert.embeddings.token_type_embeddings.weight.grad\nThe issue is related to the cosine similarity test for the huggingface models on Ubuntu 22.04 and Ubuntu 24.04. The test fails on Ubuntu 22.04 but passes on Ubuntu 24.04. The root cause is related to the layer norm backward process, which causes the cosine similarity test to fail. The solution is to modify the patch and compare the result directly, without putting fp64_outputs on xpu and putting new_result and correct_result on cpu to compare. The issue is closed.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nThe issue is closed.", "root_cause": "The root cause is related to the layer norm backward process, which causes the cosine similarity test to fail.", "state": "closed"}

### Merged Result:910{"issue_number": 910, "issue_description": "Failures on ARC windows, total 2127,FP64 related issue: 1910, others: 217\n- AssertionError: 'Assertion `cur_target >= 0 && cur_target < n_classes` failed' not found in 'PYTORCH_API_USAGE'\n- AssertionError: \"Kernel\\ is\\ incompatible\\ with\\ all\\ devices\\ in\\ devs\" does not match \"Required aspect fp64 is not supported on the device\"\n- AssertionError: \"not implemented for\" does not match \"Native API failed. Native API returns: -999 (Unknown PI error) -999 (Unknown PI error)\"\n- AssertionError: RuntimeError not raised\n- AssertionError: Scalars are not close!\n- AssertionError: Tensor-likes are not close!\n- AssertionError: Tensor-likes are not equal!\n- Exception: Caused by sample input at index\n- RuntimeError: Caught RuntimeError in DataLoader worker process 1.\n- RuntimeError: Comparing\n- RuntimeError: could not create a primitive descriptor for a convolution forward propagation primitive\n- RuntimeError: could not create a primitive descriptor for a deconvolution forward propagation primitive\n- RuntimeError: Default context is not supported on XPU on Windows. So we can NOT find its global index of the ATen device.\n- RuntimeError: Kernel is incompatible with all devices in devs\n- RuntimeError: Loader error\n- RuntimeError: Native API failed. Native API returns: -999 (Unknown PI error) -999 (Unknown PI error)\n- RuntimeError: Ninja is required to load C++ extensions\n- RuntimeError: Required aspect fp64 is not supported on the device\n- RuntimeError: Worker error\n- RuntimeError: XPU out of memory, please use `empty_cache` to release all unoccupied cached memory.\n- AssertionError: Scalars are not close!\n- Exception: Caused by sample input at index\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/910. The reporter of the issue is mengfei25, and the assignee is min-jean-cho, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "min-jean-cho", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:907{"issue_number": 907, "issue_description": "The issue is found in codegen PR, where `aten::_assert_async.msg` is called in op multinomial. It affects the uts in `extended/test_ops_xpu.py`\nThe issue is about the performance of the torch_xpu_ops library, specifically the problem with the `torch_xpu_ops.nn.functional.conv2d` function. The reporter of the issue is ZhiweiYan-96, and the assignee is ZhiweiYan-96. The state of the issue is closed. The root cause of the issue is that the convolution operation is not optimized for the XPU hardware, leading to performance degradation. The relevant PR that addresses this issue is PR #955, which has been merged and resolved the problem.", "reporter": "ZhiweiYan-96", "assignee": "ZhiweiYan-96", "resolution": "closed\nThe convolution operation is now optimized for the XPU hardware, leading to improved performance.", "root_cause": "The convolution operation is not optimized for the XPU hardware, leading to performance degradation.", "state": "closed"}### Result:906 failed to extract

### Merged Result:905{"issue_number": 905, "issue_description": "Looks like there is a random issue for Super_SloMo, and it will be passed with WHL install from prebuild but failed with source build. In latest weekly, WHL Passed: https://github.com/intel/torch-xpu-ops/actions/runs/10742335908 Source build Failed: https://github.com/intel/torch-xpu-ops/actions/runs/10741560513 And I tested WHL locally multiple times and it is passed randomly.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/905. The reporter of the issue is mengfei25, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:904{"issue_number": 904, "issue_description": "xpu train timm_efficientnet\n[WARNING] Failed to create Level Zero tracer: 2013265921\n(I): Detected 2048 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 1024 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 1024 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 1024 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\nE0907 00:54:25.073000 2172206 site-packages/torch/_dynamo/utils.py:1798] RMSE (res-fp64): 0.00040, (ref-fp64): 0.00010 and shape=torch.Size([4, 96, 1, 1]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000, use_larger_multiplier_for_smaller_tensor: 0\nE0907 00:54:25.074000 2172206 site-packages/torch/_dynamo/utils.py:1670] Accuracy failed for key name blocks.1.0.se.conv_reduce.weight.grad\nfail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/904. The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\nPassed in latest weekly test", "root_cause": "", "state": "closed"}

### Merged Result:901{"issue_number": 901, "issue_description": "Torchbench basic_gnn models performance regression, the speedup of torchbench_bfloat16_training/basic_gnn_gcn is 0.663553 in release/2.5.0, but 1.200854 in main, the speedup of torchbench_amp_bf16_training/basic_gnn_gin is 0.672736 in release/2.5.0, but 1.136112 in main, the speedup of torchbench_bfloat16_training/basic_gnn_gin is 0.659017 in release/2.5.0, but 1.162534 in main, the speedup of torchbench_amp_bf16_training/basic_gnn_gcn is 0.648738 in release/2.5.0, but 1.105941 in main, the speedup of torchbench_amp_bf16_training/basic_gnn_sage is 0.661256 in release/2.5.0, but 1.102461 in main, the speedup of torchbench_bfloat16_training/basic_gnn_edgecnn is 0.912495 in release/2.5.0, but 1.629383 in main, the speedup of torchbench_amp_bf16_training/basic_gnn_edgecnn is 0.93919 in release/2.5.0, but 1.634551 in main, the speedup of torchbench_bfloat16_training/basic_gnn_sage is 0.684153 in release/2.5.0, but 1.089889 in main.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/901. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nClose as this is too old.", "root_cause": "", "state": "closed"}

### Merged Result:900{"issue_number": 900, "issue_description": "xpu eval jx_nest_base\n[WARNING] Failed to create Level Zero tracer: 2013265921\n(I): Detected 1024 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 8192 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 8192 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 4096 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\n(I): Detected 4096 spills, recompiling the kernel using large GRF mode\n(I): Kernel has now 0 spills\nE0912 00:16:10.029000 3264502 site-packages/torch/_dynamo/utils.py:1802] RMSE (res-fp64): 0.00087, (ref-fp64): 0.00036 and shape=torch.Size([8, 1000]). res.dtype: torch.float16, multiplier: 2.000000, tol: 0.001000, use_larger_multiplier_for_smaller_tensor: 0\nfail_accuracy\nThe reporter of the issue is mengfei25, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nThe issue was resolved locally by jianyizh on pvc 1550.", "root_cause": "The issue was caused by the kernel recompiling using large GRF mode and the kernel has now 0 spills.", "state": "closed"}### Result:899 failed to extract

### Merged Result:891{"issue_number": 891, "issue_description": "Reproducing step:\n1. enable `test/inductor/test_torchinductor_opinfo.py` with this PR:\n  https://github.com/pytorch/pytorch/pull/134556\n2. `python test/inductor/test_torchinductor_opinfo.py -k addmm_xpu`\n\nError message:\n```\nunknown type name 'PO_1_BIN_ARG_DATA_T'\n```\nThe reporter of the issue is hoshibara, and the assignee is ZhiweiYan-96, and the state of the issue is closed.", "reporter": "hoshibara", "assignee": "ZhiweiYan-96", "resolution": "\nfixed", "root_cause": "https://github.com/pytorch/pytorch/pull/139721", "state": "closed"}

### Merged Result:890{"issue_number": 890, "issue_description": "The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/890. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "closed\nPR submitted https://github.com/intel/torch-xpu-ops/pull/965", "root_cause": "", "state": "closed"}

### Merged Result:889{"issue_number": 889, "issue_description": "when using to(torch.int8) on xpu, the result is different from cpu and cuda. the result on xpu is different from the result on cpu and cuda. the result on xpu is different from the result on cpu and cuda.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/889. The reporter of the issue is hoshibara, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "hoshibara", "assignee": "majing921201", "resolution": "\nclosed", "root_cause": "The root cause of the issue is the undefined behavior of overflow in XPU, as both IGC and SYCL spec don't define overflow behavior, so different values are expected.", "state": "closed"}

### Merged Result:887{"issue_number": 887, "issue_description": "Unittest: New failures on unfold. The failures are on the following tests: test_dtypes_nn_functional_unfold_xpu, test_non_standard_bool_values_nn_functional_unfold_xpu_bool, test_compare_cpu_nn_functional_unfold_xpu_bool, test_non_standard_bool_values_nn_functional_unfold_xpu_bool, test_nn_unfold_xpu. The issue is reported by fengyuan14 and assigned to daisyden. The issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:884{"issue_number": 884, "issue_description": "For OPTForCausalLM train on stock pytorch, aten::add cost time on pvc-1100 worse than A100 * ratio, please refer to https://jira.devtools.intel.com/browse/PYTORCHDGQ-5162?filter=-2.\nTwo performance limitations of hardware have been identified: 1. Instruction bound on PVC, 2. Memory/cache efficiency of broadcast case.", "reporter": "xiaowangintel", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "1. Instruction bound on PVC, 2. Memory/cache efficiency of broadcast case.", "state": "closed"}

### Merged Result:882{"issue_number": 882, "issue_description": "For OPTForCausalLM train on stock pytorch, aten::eq cost time on pvc-1100 worse than A100 * ratio, this is a github issue link https://github.com/intel/torch-xpu-ops/issues/882. The reporter of the issue is xiaowangintel, and the assignee is fengyuan14, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/882. The reporter of the issue is xiaowangintel, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "xiaowangintel", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:881{"issue_number": 881, "issue_description": "For OPTForCausalLM train on stock pytorch, aten::lt cost time on pvc-1100 worse than A100 * ratio, please refer to https://jira.devtools.intel.com/browse/PYTORCHDGQ-5160?filter=-2.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/881. The reporter of the issue is xiaowangintel, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "xiaowangintel", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:878{"issue_number": 878, "issue_description": "output of cdist op on XPU device is defferent with CPU op when p=2 and mode=2.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/878. The reporter of the issue is PenghuiCheng, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "xytintel", "resolution": "\nFixed in https://github.com/intel/torch-xpu-ops/pull/873", "root_cause": "", "state": "closed"}

### Merged Result:877{"issue_number": 877, "issue_description": "add conv and matrix multiple related ops in extended UT, Content of #877 is : ### \ud83d\ude80 The feature, motivation and pitch\nimprove test coverage\n### Alternatives\n_No response_\n### Additional context\n_No response_,\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/877. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "state": "open"}### Result:875 failed to extract

### Merged Result:861{"issue_number": 861, "issue_description": "On 22.04, this case cause segmentation fault, \"PYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 gdb - -args Python -m pytest -v test_torch_xpu.py -k test_to_with_tensor\".\nUser case defect. Need be aware of async execution and CPU tensor life cycle.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "This issue is caused by a bug in the torch-xpu-ops library. The root cause is that the tensor to be moved is not properly released before moving to another device. This can lead to a segmentation fault when the tensor is moved in non-blocking mode. The issue has been fixed in the latest version of the library.", "state": "closed"}

### Merged Result:849{"issue_number": 849, "issue_description": "Need `getStreamFromExternal` and `stream()` API  in XPUStreamfor AOT Inductor.\n", "reporter": "etaf", "assignee": "guangyey", "resolution": "closed\nClosed as it is completed.", "root_cause": "", "state": "closed"}

### Merged Result:845{"issue_number": 845, "issue_description": "Try two cases on cuda,\nfailed on xpu for \"AssertionError: Tensor-likes are not close!\", but I found cuda doesn't run these two cases, only run the case \"test_compare_cpu_nn_functional_adaptive_avg_pool3d_xpu_float32\", maybe we should align with cuda and skip these cases if cuda fails too. Please help to check.\n\ncuda fails too, I think we should skip these two cases.", "reporter": "chunhuanMeng", "assignee": "daisyden", "resolution": "\nskip these two cases", "root_cause": "", "state": "closed"}

### Merged Result:842{"issue_number": 842, "issue_description": "The Pow operator gives incorrect result in UT test_binary_ufuncs_xpu.py::TestBinaryUfuncsXPU::test_pow_xpu_float16. The failure is related to the cast of complex half type in kernel. If we convert with opmath_t{}, other ops like log will also gives incorrect results. #798 is to fix this failure. However, additional overhead was introduced in type cast so we need to record this.\nThis is a bug due to the compiler.", "reporter": "Kanya-Mo", "assignee": "Kanya-Mo", "resolution": "fix this failure\n", "root_cause": "cast of complex half type in kernel", "state": "closed"}

### Merged Result:839{"issue_number": 839, "issue_description": "There is lack of XPU support in of `toAccumulateType`.\nhttps://github.com/pytorch/pytorch/blob/97c8a0739e6eadbf99bfdb21a999db735658d13b/aten/src/ATen/AccumulateType.cpp#L5-L16\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/839. The reporter of the issue is fengyuan14, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "closed\nMerged in https://github.com/pytorch/pytorch/pull/134465", "root_cause": "", "state": "closed"}

### Merged Result:827{"issue_number": 827, "issue_description": "A reproducer for the behavior of `index_put_` which is inconsistency with other backends.\n\n```python\nimport torch\n\ninput = torch.randn(4, 4, device=\"xpu\")\nindex = torch.randint(4, (4,), device=\"xpu\").int()\nsrc = torch.randn(4, device=\"xpu\")\n\ntorch.index_put_(input, (index, index), src, True)\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/827. The reporter of the issue is guangyey, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "guangyey", "assignee": "Stonepia", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/597", "root_cause": "The root cause is [`checkIndexTensorTypes`](https://github.com/pytorch/pytorch/blob/de57a6e806e990fa1b6914ca2b4aabb91bc7bc81/aten/src/ATen/native/IndexingUtils.h#L51). Please help check the behavior of other ops which also use `checkIndexTensorTypes`. ", "state": "closed"}### Result:824 failed to extract

### Merged Result:821{"issue_number": 821, "issue_description": "### \ud83d\udc1b Describe the bug\n\nRetriage for PT2.6, old issue is https://github.com/intel/torch-xpu-ops/issues/577\n\n# XPU supported OP:\n - [x] linalg_vector_norm:\n  ```\n  # RuntimeError: Fail to enable Kineto Profiler on XPU due to error code: 200\n  \nFor the first two cases, the root cause there are cuda bias codes(https://github.com/pytorch/pytorch/blob/6afcec0c582cb852fcf673ea3b6ce12e4b9da01d/aten/src/ATen/native/ReduceOpsUtils.h#L223), we should avoid explicitly casting low precision inputs to fp32 like cuda, need to raise a pr in stock pytorch for adding condition for xpu.", "reporter": "yuchengliu1", "assignee": "PenghuiCheng", "resolution": "\npr merged", "root_cause": "cuda bias codes(https://github.com/pytorch/pytorch/blob/6afcec0c582cb852fcf673ea3b6ce12e4b9da01d/aten/src/ATen/native/ReduceOpsUtils.h#L223), need to raise a pr in stock pytorch for adding condition for xpu", "state": "closed"}

### Merged Result:817{"issue_number": 817, "issue_description": "bincount and uniform_ ops with hard coded fp64 will cause ARC test failures\nbincount is expected to use double, so we will skip it on ARC on xpu backend.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nhooks added", "root_cause": "sample inputs generated double data", "state": "closed"}

### Merged Result:816{"issue_number": 816, "issue_description": "For LayoutLMForSequenceClassification model on stock pytorch, index_select cost time on pvc-1100 worse than A100 * ratio, this is a github issue link https://github.com/intel/torch-xpu-ops/issues/816. The reporter of the issue is xiaowangintel, and the assignee is xiaowangintel, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/816. The reporter of the issue is xiaowangintel, and the assignee is xiaowangintel, and the state of the issue is closed.", "reporter": "xiaowangintel", "assignee": "xiaowangintel", "resolution": "\nfixed", "root_cause": "xpu performance is not targeted to PT 2.6", "state": "closed"}

### Merged Result:814{"issue_number": 814, "issue_description": "The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is closed.\nNo plan to support tunable in 2.6, close this issue.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "closed\nclosed", "root_cause": "No plan to support tunable in 2.6", "state": "closed"}

### Merged Result:811{"issue_number": 811, "issue_description": "The op is expected to fallback to CPU, see https://github.com/intel/torch-xpu-ops/blob/main/src/ATen/native/xpu/XPUFallback.template#L239, but it is not implemented in CPU backend. The error message is: The operation is not supported on the CPU backend.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/811. The reporter of the issue is daisyden, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nfixed", "root_cause": "The operation is not implemented in the CPU backend.", "state": "closed"}

### Merged Result:809{"issue_number": 809, "issue_description": "test_ops_fwd_gradients_xpu.py::TestFwdGradientsXPU::test_fn_fwgrad_bwgrad_nn_functional_conv3d_xpu_complex128 mFAILED\n\ntest_ops_fwd_gradients_xpu.py::TestFwdGradientsXPU::test_fn_fwgrad_bwgrad_nn_functional_conv3d_xpu_float64 mFAILED \ntest_ops_gradients_xpu.py::TestBwdGradientsXPU::test_fn_gradgrad_nn_functional_conv3d_xpu_complex128\ntest_ops_gradients_xpu.py::TestBwdGradientsXPU::test_fn_gradgrad_nn_functional_conv3d_xpu_float64 mFAILED\nnn/test_convolution_xpu.py::TestConvolutionNN::test_thnn_conv_strided_padded_dilated mFAILED\nThe regression seems to be fixed in onednn 3.7. I have wrote a small case with the same input in pytorch UT, and it passed with onednn3.7. However, pytorch cannot compile with pytorch 3.7.", "reporter": "daisyden", "assignee": "yuchengliu1", "resolution": "\nThe regression seems to be fixed in onednn 3.7.", "root_cause": "oneDNN dependence.", "state": "closed"}

### Merged Result:803{"issue_number": 803, "issue_description": "For LayoutLMForSequenceClassification model on stock pytorch, div cost time on pvc-1100 worse than A100 * ratio, this is a github issue link https://github.com/intel/torch-xpu-ops/issues/803. The reporter of the issue is xiaowangintel, and the assignee is fengyuan14, and the state of the issue is open.\nxpu performance is not targeted to PT 2.6", "reporter": "xiaowangintel", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "PT 2.6 is not targeted for xpu performance", "state": "open"}

### Merged Result:800{"issue_number": 800, "issue_description": "For LayoutLMForSequenceClassification model on stock pytorch, gelu cost time on pvc-1100 worse than A100 * ratio.\nxpu performance is not targeted to PT 2.6", "reporter": "xiaowangintel", "assignee": "retonym", "resolution": "\nClose as the problem no longer exists", "root_cause": "rerun this test and the perf for gelu is reasonable", "state": "closed"}

### Merged Result:795{"issue_number": 795, "issue_description": "For T5Small model on stock pytorch, inplace add cost time on pvc-1100 worse than A100 * ratio, refer to https://jira.devtools.intel.com/browse/PYTORCHDGQ-5017\nxpu performance is not targeted to PT 2.6", "reporter": "xiaowangintel", "assignee": "", "resolution": "\n", "root_cause": "xpu performance is not targeted to PT 2.6", "state": "open"}

### Merged Result:794{"issue_number": 794, "issue_description": "For Hugging Face benchmark model on stock pytorch, Softmax cost time on pvc-1100 worse than A100 * ratio.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/794. The reporter of the issue is xiaowangintel, and the assignee is jianyizh, and the state of the issue is closed.", "reporter": "xiaowangintel", "assignee": "jianyizh", "resolution": "\n", "root_cause": "The issue was closed as the reporter xiaowangintel did not provide a resolution.", "state": "closed"}

### Merged Result:789{"issue_number": 789, "issue_description": "For AllenaiLongformerBase model on stock pytorch, copy cost time on pvc-1100 worse than A100 * ratio, this is a github issue link https://github.com/intel/torch-xpu-ops/issues/789. The reporter of the issue is xiaowangintel, and the assignee is fengyuan14, and the state of the issue is open.\nxpu performance is not targeted to PT 2.6", "reporter": "xiaowangintel", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "xpu performance is not targeted to PT 2.6", "state": "open"}

### Merged Result:788{"issue_number": 788, "issue_description": "We have witnessed the following E2E models have pagefault because of the oneDNN version v3.4.2:\n\nHuggingface:\n   1. AllenaiLongformerBase\n       a. amp_bf16 & amp_fp16, inference, performance\n\nTorchbench:\n   1. cm3leon_generate\n       a. amp_bf16 & amp_fp16, inference, accuracy & performance\n       b. bfloat16 & float16, inference, accuracy\n   2. hf_Longformer\n       a. amp_bf16 & amp_fp16, inference, accuracy & performance\n\nBy upgrading oneDNN version to v3.5.3, the problem will be solved.\n\nTested oneDNN commit:\n\nv3.4.2: 1137e04ec0b5251ca2b4400a4fd3c667ce843d67\nv3.5.3: 66f0cb9eb66affd2da3bf5f8d897376f04aae6af", "reporter": "Stonepia", "assignee": "", "resolution": "By upgrading oneDNN version to v3.5.3, the problem will be solved.", "root_cause": "The problem is caused by the oneDNN version v3.4.2, upgrading to v3.5.3 solves the problem.", "state": "closed"}

### Merged Result:784{"issue_number": 784, "issue_description": "test_foreach.py::TestForeachCUDA::test_0dim_tensor_overload_exception_cuda is expected to report \"RuntimeError: scalar tensor expected to be on cuda:0 but is on cpu\" while xpu does not have such error message. \ncuda only report the error when alpha is specified. \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/784. The reporter of the issue is daisyden, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nFixed by this pr https://github.com/intel/torch-xpu-ops/pull/1065", "root_cause": "", "state": "closed"}

### Merged Result:783{"issue_number": 783, "issue_description": "PYTORCH_TEST_WITH_SLOW=1 pytest -v test_indexing_xpu.py -k test_advancedindex_xpu_float64 the reference size is 10, so the out of boundary alert should be reported. While on xpu it passed.  In the 2nd time when we call reference[torch.LongTensor([err_idx]).to(device)] there is a core dump. Assertion `index >= -sizes_[i] && index < sizes_[i] && \nNot an issue.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "\nNot an issue.", "root_cause": "The issue is caused by the out of boundary access in the torch.LongTensor index operation. The assertion failed because the index value is out of the expected range. The core dump is due to the assertion failure in the xpu backend.", "state": "closed"}

### Merged Result:781{"issue_number": 781, "issue_description": "Seems both cpu and xpu result are questionable. the output's imag is expected to be \"-1.0020e+23\" but got 1.0020e+23 on cpu, while the xpu result imag is -inf.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/781. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "The issue is related to the implementation of std::pow function in the compiler, which produces different results on CPU and XPU for the input -501.-1.0000e+20j. The square of this complex number should be -inf+1.0020e+23j on CPU, but -inf-infj on XPU.", "state": "open"}

### Merged Result:780{"issue_number": 780, "issue_description": "PYTORCH_TEST_WITH_SLOW=1 pytest -v test_ops_xpu.py -k test_compare_cpu_native_layer_norm_xpu_bfloat16 \nReport \"AssertionError: The values for attribute 'dtype' do not match: torch.float32 != torch.bfloat16\".\nFrom the log we can see the 2nd tensor returned does not have dtype specified that means it is torch.float32, while cpu returned torch.bfloat16.\n\n> /home/gta/daisyden/pytorch4/test/test_ops.py(296)test_compare_cpu()\n-> cuda_results = op(sample.input, *sample.args, **sample.kwargs)\n(Pdb) p sample.input\ntensor([[[-4.5000, -5.9062, -0.9531],\n         [-2.3594,  1.5625, -6.2812]]], device='xpu:0', dtype=torch.bfloat16)\n(Pdb) p sample.args\n((1, 2, 3), tensor([[[-6.7188,  5.6875,  0.7930],\n         [ 2.8750, -4.0938,  8.5000]]], device='xpu:0', dtype=torch.bfloat16), tensor([[[ 2.1719, -8.5000, -3.1406],\n         [-7.0000,  3.2656, -2.5000]]], device='xpu:0', dtype=torch.bfloat16), 0.5)\n(Pdb) n\n> /home/gta/daisyden/pytorch4/test/test_ops.py(297)test_compare_cpu()\n-> cpu_results = op(cpu_sample.input, *cpu_sample.args, **cpu_sample.kwargs)\n(Pdb) l\n292             for sample in samples:\n293                 import pdb\n294                 pdb.set_trace()\n295                 cpu_sample = sample.transform(to_cpu)\n296                 cuda_results = op(sample.input, *sample.args, **sample.kwargs)\n297  ->             cpu_results = op(cpu_sample.input, *cpu_sample.args, **cpu_sample.kwargs)\n298  \n299                 # output_process_fn_grad has a very unfortunate name\n300                 # We use this function in linalg extensively to postprocess the inputs of functions\n301                 # that are not completely well-defined. Think svd and muliplying the singular vectors by -1.\n302                 # CPU and CUDA implementations of the SVD can return valid SVDs that are different.\n(Pdb) p cuda_results\ntensor([[[  5.5000, -14.0625,  -2.5625],\n         [ -6.2812,  -3.3125, -11.9375]]], device='xpu:0',\n       dtype=torch.bfloat16), tensor([[[-3.0729]]], device='xpu:0'), tensor([[[0.3469]]], device='xpu:0'))\n(Pdb) n\n> /home/gta/daisyden/pytorch4/test/test_ops.py(304)test_compare_cpu()\n-> cuda_results = sample.output_process_fn_grad(cuda_results)\n(Pdb) p cpu_results\ntensor([[[  5.5000, -14.0625,  -2.5625],\n         [ -6.2812,  -3.3125, -11.9375]]], dtype=torch.bfloat16), tensor([[[-3.0781]]], dtype=torch.bfloat16), tensor([[[0.3477]]], dtype=torch.bfloat16))\ncuda has the same issue, we can skip the case.", "reporter": "daisyden", "assignee": "xytintel", "resolution": "\nskip the case", "root_cause": "cuda issue", "state": "closed"}

### Merged Result:776{"issue_number": 776, "issue_description": "convert float.min to int8 or int16, the output is different with numpy and cuda. Log: vals: (-3.4028234663852886e+38, -2, -1.5, -0.5, 0, 0.5, 1.5, 2) dtype: torch.int8 device: 'xpu:0' np.array(vals, dtype=np.float32).astype(torch_to_numpy_dtype_dict[dtype]): array([ 0, -2, -1,  0,  0,  0,  1,  2], dtype=int8) torch.tensor(vals, device=device, dtype=torch.float).to(dtype): tensor([-128,   -2,   -1,    0,    0,    0,    1,    2], device='xpu:0', dtype=torch.int8)\nduplicate to https://github.com/intel/torch-xpu-ops/issues/889.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "closed\n", "root_cause": "The reporter of the issue is PenghuiCheng, and the assignee is PenghuiCheng, and the state of the issue is closed.", "state": "closed"}

### Merged Result:774{"issue_number": 774, "issue_description": "The following tests fail on XPU: adaptive_max_pool2d, _foreach_norm, _embedding_bag_forward_only, nn.functional.avg_pool1d, nn.functional.local_response_norm, vdot, nanmean, _foreach_lgamma, _foreach_sigmoid, max_pool3d. The error messages are as follows: 'Expected out tensor to have dtype c10::BFloat16/c10::Half/float/double, but got long int instead', 'RuntimeError: output 1: meta disagrees with real impl', 'RuntimeError: output 2: meta disagrees with real impl', 'RuntimeError: false INTERNAL ASSERT FAILED at \nThe cases related to `_foreach_norm` have been fixed in main branch. The cases related to `adaptive_max_pool` also passed in main branch. Cases related to `embedding bag` is fixed with this https://github.com/intel/torch-xpu-ops/pull/1018. The pooling related cases are all passed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/774. The reporter of the issue is yuchengliu1, and the assignee is daisyden, and the state of the issue is open.", "reporter": "yuchengliu1", "assignee": "daisyden", "resolution": "\nThe pooling related cases are all passed.\n", "root_cause": "The issue is related to oneDNN's support for certain datatypes and operations, and the reporter and assignee are discussing the need to skip these cases until oneDNN is updated to support them.", "state": "open"}

### Merged Result:772{"issue_number": 772, "issue_description": "The reporter of the issue is PenghuiCheng, and the assignee is fengyuan14, and the state of the issue is open. Need quantization support, NotImplementedError: Could not run 'aten::_empty_affine_quantized' with arguments from the 'QuantizedXPU' backend. cases: test_view_ops_xpu.py::TestOldViewOpsXPU::test_flatten_xpu test_view_ops_xpu.py::TestOldViewOpsXPU::test_ravel_xpu\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/772. The reporter of the issue is PenghuiCheng, and the assignee is fengyuan14, and the state of the issue is open.", "reporter": "PenghuiCheng", "assignee": "fengyuan14", "resolution": "\nLowering the priority of the issue", "root_cause": "The Tensor with QuantizedXPU dispatch key implies quantization information, but in other quantization solutions, the scale and shift are not represented in a Tensor, and the scale and shift are put in a separate Tensor. The operator API or graph will introduce the scale and shift Tensors.", "state": "open"}

### Merged Result:771{"issue_number": 771, "issue_description": "nn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_pooling_bfloat16_xpu\nnn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_pool_large_size_xpu_bfloat16\nnn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_AdaptiveMaxPool3d_indices_xpu_float16\nnn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_max_pool_nan_inf_xpu_float16\nnn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_adaptive_pooling_empty_output_size_xpu_float16\nnn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_maxpool_indices_no_batch_dim_xpu_float16\nnn/test_pooling_xpu.py::TestPoolingNNDeviceTypeXPU::test_pool_large_size_xpu_float16\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/771. The reporter of the issue is PenghuiCheng, and the assignee is chunhuanMeng, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "chunhuanMeng", "resolution": "\nCases above have been passed in the main branch ,suggest to close this issue.", "root_cause": "", "state": "closed"}

### Merged Result:768{"issue_number": 768, "issue_description": "Refs op will use the original op dtypes, we can also align the dtypesIfXPU of refs ops with cuda to avoid issues like the below. The two cases are skipped by cuda but not by torch-xpu-ops.\n- [ ] # NameError: name 'nanj' is not defined. Did you mean: 'nan'? \n\n\"test_python_ref_executor__refs_logaddexp_executor_aten_xpu_complex128\", \"test_python_ref_executor__refs_logaddexp_executor_aten_xpu_complex64\",\nThese two cases does not run in current vision", "reporter": "daisyden", "assignee": "yuchengliu1", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:767{"issue_number": 767, "issue_description": "File \"/home/penghuic/stock_pytorch/torch/nn/utils/rnn.py\", line 93, in __new__\n    *_packed_sequence_init_args(\nFile \"/home/penghuic/stock_pytorch/torch/nn/utils/rnn.py\", line 254, in _packed_sequence_init_args\n    assert isinstance(data, (list, tuple)) and len(data) == 2\nAssertionError\nduplicated https://github.com/intel/torch-xpu-ops/issues/745", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:761{"issue_number": 761, "issue_description": "1. NotImplementedError: Could not run 'aten::_to_copy' with arguments from the 'NestedTensorXPU' backend\n2. We have no mechanism to handle SDPBackend::ERROR so far. Will give a fully support when we support all SDPBackends.\n3. AssertionError: False is not true\n4. Double and complex datatype matmul is not supported in oneDNN\nThe issue depends on SDP implementation. We are evaluating a choice of XPU.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "\n", "root_cause": "SDP implementation", "state": "open"}### Result:754 failed to extract

### Merged Result:753{"issue_number": 753, "issue_description": "models\n- [ ] LayoutLMForSequenceClassification amp_fp16\n- [ ] DebertaForQuestionAnswering float16\n- [ ] DebertaV2ForQuestionAnswering float16\n\nxpu  eval  LayoutLMForSequenceClassification \nE0811 04:36:32.669000 134825 torch/_dynamo/utils.py:1555] RMSE (res-fp64): 0.00373, (ref-fp64): 0.00084 and shape=torch.Size([1, 2]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000\nE0811 04:36:32.669000 134825 torch/_dynamo/utils.py:1447] Accuracy failed for key name logits\nfail_accuracy\nxpu  eval  DebertaForQuestionAnswering \nE0811 03:59:14.697000 113914 torch/_dynamo/utils.py:1555] RMSE (res-fp64): 0.00889, (ref-fp64): 0.00107 and shape=torch.Size([]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000\nfail_accuracy\nxpu  eval  DebertaV2ForQuestionAnswering \nE0811 04:00:28.630000 114111 torch/_dynamo/utils.py:1555] RMSE (res-fp64): 0.00952, (ref-fp64): 0.00170 and shape=torch.Size([]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000\nfail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/753. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:752{"issue_number": 752, "issue_description": "Observed E2E performance on MTL, amp will be out of memory and machine will be disconnected.\nreproduce\n`bash inductor_xpu_test.sh huggingface amp_bf16 inference performance xpu 0 static`\n\nOOM and machine disconnect be related to the Driver bug.", "reporter": "mengfei25", "assignee": "Stonepia", "resolution": "\nClose the issue", "root_cause": "Driver bug", "state": "closed"}

### Merged Result:750{"issue_number": 750, "issue_description": "Error message like \n\n```\nL0 build module failed. Log:\nerror: bf conversion instruction not supported!\n\nin kernel: 'triton_poi_fused__to_copy_2'\nerror: backend compiler failed build.\n\nTraceback (most recent call last):\n  File \"/home/gta/actions-runner/actions-runner/_work/torch-xpu-ops/repro.py\", line 34, in <module>\n    async_compile.wait(globals())\n  File \"/opt/conda/envs/e2e_ci/lib/python3.10/site-packages/torch/_inductor/async_compile.py\", line 261, in wait\n    scope[key] = result.result()\n  File \"/opt/conda/envs/e2e_ci/lib/python3.10/site-packages/torch/_inductor/codecache.py\", line 3745, in result\n    self.kernel.precompile()\n  File \"/opt/conda/envs/e2e_ci/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 234, in precompile\n    compiled_binary, launcher = self._precompile_config(\n  File \"/opt/conda/envs/e2e_ci/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py\", line 442, in _precompile_config\n    binary._init_handles()\n  File \"/opt/conda/envs/e2e_ci/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 376, in _init_handles\n    self.module, self.function, self.n_regs, self.n_spills = driver.active.utils.load_binary(\nRuntimeError: Triton Error [ZE]: 0x70000004\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/750. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nThe issue is verified with latest triton release/2.5.0 branch, and it should work with PyTorch with commit id later than https://github.com/pytorch/pytorch/commit/fbd020fce649ddb44bd9a578dabb5834c5d0f186", "root_cause": "", "state": "closed"}### Result:746 failed to extract

### Merged Result:745{"issue_number": 745, "issue_description": "PI_ERROR_INVALID_QUEUE after copying device 0 tensor to device 1. The error occurs when trying to print the tensor b after moving it from device 0 to device 1. The error message is: RuntimeError: Native API failed. Native API returns: -36 (PI_ERROR_INVALID_QUEUE). The reporter of the issue is daisyden, and the assignee is fengyuan14, and the state of the issue is closed.\nCannot launch kernel successfully on PVC Tile 1 after querying `info::kernel_device_specific::work_group_size`. Got runtime error.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nThe issue is common for all platform where there are devices more than one. The most important and most common case for us is client case, a client platform/desktop has an iGPU and an dGPU.", "root_cause": "Cannot launch kernel successfully on PVC Tile 1 after querying `info::kernel_device_specific::work_group_size`. Got runtime error.", "state": "closed"}

### Merged Result:737{"issue_number": 737, "issue_description": "The reporter of the issue is fengyuan14, and the assignee is PenghuiCheng, and the state of the issue is closed.\n", "reporter": "fengyuan14", "assignee": "PenghuiCheng", "resolution": "closed\ncompleted", "root_cause": "", "state": "closed"}

### Merged Result:731{"issue_number": 731, "issue_description": "TestAutograd::test_profiler UT failure after enable PTI. Error message: Fail to enable Kineto Profiler on XPU due to error code: 200. To execute this test, run the following from the base repo dir: PYTORCH_TEST_WITH_SLOW=1 python test/test_autograd.py TestAutograd.test_profiler. This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/731. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "closed"}### Result:729 failed to extract

### Merged Result:728{"issue_number": 728, "issue_description": "WARNING:common:fp64 golden ref were not generated for detectron2_fasterrcnn_r_101_c4. Setting accuracy check to cosine\nWARNING:common:current_device=xpu; error:dets should have the same type as scores\nE0803 06:43:58.822000 3103334 torch/_dynamo/utils.py:1450] Accuracy failed for key name pred_classes\nE0803 06:43:58.823000 3103334 torch/_dynamo/utils.py:1450] Accuracy failed for key name instances\nfail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/728. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "The models are not included in the Meta PyTorch dashboard due to low priority and the failure of detectron2 installation on A100. These models are not included in the Meta dashboard and are not targeted for PT2.6.", "state": "closed"}

### Merged Result:727{"issue_number": 727, "issue_description": "torchbench_amp_bf16_training xpu train tacotron2, the model failed to run because of the following error: RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [XPUBFloat16Type [4, 80, 724]], which is output 0 of torch::autograd::CopyBackwards, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/727. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nclose due to a100 also failed", "root_cause": "A100 is also failed", "state": "closed"}

### Merged Result:726{"issue_number": 726, "issue_description": "torchbench_amp_bf16_training xpu train hf_distil_whisper Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run  ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 302, in load_model  benchmark = benchmark_cls(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__  obj = type.__call__(cls, *args, **kwargs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/hf_distil_whisper/__init__.py\", line 12, in __init__  raise NotImplementedError(\"Training is not implemented.\") NotImplementedError: Training is not implemented. model_fail_to_load\nNot an xpu issue", "reporter": "mengfei25", "assignee": "", "resolution": "\nNot an xpu issue", "root_cause": "", "state": "closed"}

### Merged Result:725{"issue_number": 725, "issue_description": "torchbench_amp_bf16_training xpu train detectron2_fcos_r_50_fpn Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run  ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 302, in load_model  benchmark = benchmark_cls(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__  obj = type.__call__(cls, *args, **kwargs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/detectron2_fcos_r_50_fpn/__init__.py\", line 15, in __init__  super().__init__(variant=\"COCO-Detection/fcos_R_50_FPN_1x.py\", test=test, device=device,  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/framework/detectron2/model_factory.py\", line 137, in __init__  raise NotImplementedError(  NotImplementedError: FCOS train is not supported by upstream detectron2. See GH Issue: https://github.com/facebookresearch/detectron2/issues/4369. model_fail_to_load loading model: 0it [00:13, ?it/s] \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/725. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "FCOS train is not supported by upstream detectron2. See GH Issue: https://github.com/facebookresearch/detectron2/issues/4369.", "state": "open"}### Result:724 failed to extract

### Merged Result:723{"issue_number": 723, "issue_description": "torchbench_amp_bf16_training xpu train pyhpc_turbulent_kinetic_energy Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run  ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 302, in load_model  benchmark = benchmark_cls(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__  obj = type.__call__(cls, *args, **kwargs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 137, in __init__  self._determine_batch_size(batch_size)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 262, in _determine_batch_size  raise NotImplementedError(  NotImplementedError: Model's DEFAULT_TRAIN_BSIZE is not implemented. model_fail_to_load\n\nloading model: 0it [00:00, ?it/s] \nloading model: 0it [00:07, ?it/s] \n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/723. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\nclose due to a100 failed", "root_cause": "Model's DEFAULT_TRAIN_BSIZE is not implemented.", "state": "closed"}

### Merged Result:722{"issue_number": 722, "issue_description": "Torchbench pyhpc and maml training accuracy failed. The error message is: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn. The above exception was the direct cause of the following exception: RuntimeError: Eager run failed. eager_fail_to_run. loading model: 0it [00:00, ?it/s] loading model: 0it [00:01, ?it/s] The issue is closed.\nA100 has same issue", "reporter": "mengfei25", "assignee": "", "resolution": "\nclosed", "root_cause": "This issue is caused by the fact that the element 0 of the tensors does not require grad and does not have a grad_fn. This is a known issue in PyTorch and has been reported in the GitHub issue #722. The reporter of the issue is mengfei25, and the assignee is . The state of the issue is closed.", "state": "closed"}

### Merged Result:721{"issue_number": 721, "issue_description": "torchbench_amp_bf16_training xpu train doctr_reco_predictor Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2512, in validate_model self.model_iter_fn(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 449, in forward_and_backward_pass loss = self.compute_loss(pred)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 432, in compute_loss return reduce_to_scalar_loss(pred)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in <genexpr> return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in <genexpr> return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in <genexpr> return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 124, in reduce_to_scalar_loss raise NotImplementedError(\nA100 is also failed", "reporter": "mengfei25", "assignee": "", "resolution": "\nclose due to a100 failed", "root_cause": "", "state": "closed"}

### Merged Result:720{"issue_number": 720, "issue_description": "torchbench_amp_bf16_training xpu train doctr_det_predictor Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2512, in validate_model self.model_iter_fn(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 449, in forward_and_backward_pass loss = self.compute_loss(pred)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 432, in compute_loss return reduce_to_scalar_loss(pred)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in <genexpr> return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 111, in <genexpr> return sum(reduce_to_scalar_loss(x) for x in out) / len(out)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in reduce_to_scalar_loss return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 121, in <genexpr> return sum(reduce_to_scalar_loss(value) for value in out.values()) / len(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_dynamo/testing.py\", line 124, in reduce_to_scalar_loss raise NotImplementedError(\nA100 is also failed", "reporter": "mengfei25", "assignee": "", "resolution": "\nclose due to a100 failed", "root_cause": "", "state": "closed"}

### Merged Result:719{"issue_number": 719, "issue_description": "TorchDynamo optimized model failed to run because of following error\nfail_to_run\n\nAssertionError: expected size 4==5, stride 1==1 at dim=0\nA PyTorch benchmarking issue related to the torchrec_dlrm model, which fails to load due to an attribute error in the fbgemm component. The error message indicates that the '_OpNamespace' object has no attribute 'permute_2D_sparse_data'. The issue is related to the environment setup and the dependency of FPGEMM, which is known to have issues. The root cause is the incompatibility between the fbgemm component and the current environment setup, leading to the failure to load the model. The issue has been marked as low priority due to not being included in the Meta PyTorch dashboard.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\nThe issue has been resolved by updating the environment setup and addressing the dependency of FPGEMM. The model can now be loaded successfully.", "root_cause": "The root cause of the issue is the incompatibility between the fbgemm component and the current environment setup, leading to the failure to load the model. The issue is related to the environment setup and the dependency of FPGEMM, which is known to have issues.", "state": "closed"}

### Merged Result:718{"issue_number": 718, "issue_description": "torchbench_amp_bf16_training xpu train opacus_cifar10 Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2512, in validate_model self.model_iter_fn(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 450, in forward_and_backward_pass self.grad_scaler.scale(loss).backward()  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/_tensor.py\", line 522, in backward torch.autograd.backward(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 346, in backward torch.autograd.backward(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/autograd/graph.py\", line 812, in _engine_run_backward return Variable._execution_engine.run_backward(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 98, in __call__ return self.hook(module, *args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/opacus/grad_sample/grad_sample_module.py\", line 328, in capture_backprops_hook activations, backprops = self.rearrange_grad_samples(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/opacus/grad_sample/grad_sample_module.py\", line 384, in rearrange_grad_samples raise ValueError  ValueError: No activations detected for <class 'torch.nn.modules.linear.Linear'>, run forward after add_hooks(model) The above exception was the direct cause of the following exception: Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 362, in load_model self.validate_model(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2514, in validate_model raise RuntimeError(\"Eager run failed\") from e  RuntimeError: Eager run failed eager_fail_to_run loading model: 0it [00:00, ?it/s] loading model: 0it [00:06, ?it/s] \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/718. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nclose the issue, due to a100 also failed.", "root_cause": "No activations detected for <class 'torch.nn.modules.linear.Linear'>, run forward after add_hooks(model)", "state": "closed"}### Result:717 failed to extract

### Merged Result:716{"issue_number": 716, "issue_description": "torchbench_amp_bf16_inference xpu eval hf_clip Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2512, in validate_model self.model_iter_fn(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 439, in forward_pass return mod(*inputs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl return self._call_impl(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl return forward_call(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 1110, in forward vision_outputs = self.vision_model(  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl return self._call_impl(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl return forward_call(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 849, in forward hidden_states = self.embeddings(pixel_values)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl return self._call_impl(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl return forward_call(*args, **kwargs)  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py\", line 188, in forward batch_size = pixel_values.shape[0] AttributeError: 'str' object has no attribute 'shape' The above exception was the direct cause of the following exception: Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 362, in load_model self.validate_model(model, example_inputs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2514, in validate_model raise RuntimeError(\"Eager run failed\") from e RuntimeError: Eager run failed\nA100 also has this issue", "reporter": "mengfei25", "assignee": "", "resolution": "\nclose the issue", "root_cause": "The error is caused by the fact that the input to the model is a string instead of a tensor. The model expects a tensor as input, but a string is passed instead. This can happen if the input data is not properly preprocessed or if there is a bug in the data loading process. To fix this, the input data should be preprocessed to ensure that it is in the correct format (a tensor) before being passed to the model. ", "state": "closed"}

### Merged Result:715{"issue_number": 715, "issue_description": "torchbench_amp_bf16_inference\n- [ ] `moco`\n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4626, in run\n    ) = runner.load_model(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 309, in load_model\n    benchmark = benchmark_cls(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__\n    obj = type.__call__(cls, *args, **kwargs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/moco/__init__.py\", line 80, in __init__\n    raise NotImplementedError(f\"{device} not supported\")\nNotImplementedError: xpu not supported\n\nmodel_fail_to_load\n\nloading model: 0it [00:00, ?it/s]You are using a model of type moondream1 to instantiate a model of type phi. This is not supported for all configurations of models and can yield errors.\n\nloading model: 0it [00:17, ?it/s]\"} {\nA100 pass", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "closed\nduplicated as https://github.com/intel/torch-xpu-ops/issues/489", "root_cause": "The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is closed.", "state": "closed"}

### Merged Result:714{"issue_number": 714, "issue_description": "The operator 'customflash::custom_flash_aligned' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nTorchXPUOps: A100 model failed in torchbench", "reporter": "mengfei25", "assignee": "", "resolution": "\nclose the issue, since a100 also failed", "root_cause": "A100 model failed in torchbench", "state": "closed"}### Result:713 failed to extract

### Merged Result:712{"issue_number": 712, "issue_description": "The original model code forces the use of CUDA. model_fail_to_load\nsimple_gpt on A100 are failed", "reporter": "mengfei25", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:711{"issue_number": 711, "issue_description": "The eval test only supports CPU. model_fail_to_load\nThe model still fails with a new error message after the previous error was resolved. The new error message is: expected scalar type Float but found Half. The previous error message was: expected scalar type Float but found Half. The root cause of the new error is that the model is using a half precision tensor in a place where a float tensor is expected.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nThe model needs to be updated to use float tensors instead of half precision tensors in the relevant places.", "root_cause": "The model is using a half precision tensor in a place where a float tensor is expected.", "state": "open"}

### Merged Result:710{"issue_number": 710, "issue_description": "Implement Aten::_foreach_norm when `ord == inf`, the current implementation fallback to cpu, we should have the implementation when `ord == inf` to same as cuda. The reporter of the issue is chunhuanMeng, and the assignee is chunhuanMeng, and the state of the issue is closed.\nThe issue is about the performance of the torch_xpu_ops module, specifically the problem of the module not being able to run on the XPU device. The reporter of the issue is chunhuanMeng, and the assignee is chunhuanMeng, and the state of the issue is closed.", "reporter": "chunhuanMeng", "assignee": "chunhuanMeng", "resolution": "\nThe PR was merged", "root_cause": "The issue was caused by a bug in the torch_xpu_ops module, which was fixed by the PR merged by xytintel.", "state": "closed"}

### Merged Result:708{"issue_number": 708, "issue_description": "Model list:\n- [ ] `convnext_base`\n\nE0804 00:49:00.458000 519441 torch/_dynamo/utils.py:1558] RMSE (res-fp64): nan, (ref-fp64): 0.00512 and shape=torch.Size([128]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.010000\nE0804 00:49:00.458000 519441 torch/_dynamo/utils.py:1450] Accuracy failed for key name stem.0.bias.grad\nfail_accuracy\nlow priority for fp16 training not included in Meta PyTorch dashboard", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nClose the issues, since A100 accuracy also fails", "root_cause": "A100 is also failed", "state": "closed"}

### Merged Result:707{"issue_number": 707, "issue_description": "Model list:\n- [ ] `fbnetv3_b`\n\nE0804 06:29:58.153000 934284 torch/_dynamo/utils.py:1558] RMSE (res-fp64): 0.30015, (ref-fp64): 0.05598 and shape=torch.Size([360]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.040000\nE0804 06:29:58.154000 934284 torch/_dynamo/utils.py:1450] Accuracy failed for key name blocks.4.3.bn1.running_var\nfail_accuracy\nlow priority for ampbf16 training not included in Meta PyTorch dashboard", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nclose the issue, due to A100 also fails", "root_cause": "A100 is also failed", "state": "closed"}### Result:706 failed to extract

### Merged Result:705{"issue_number": 705, "issue_description": "E0804 04:47:13.347000 901510 torch/_dynamo/utils.py:1558] RMSE (res-fp64): nan, (ref-fp64): 0.00000 and shape=torch.Size([128]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.010000\nE0804 04:47:13.348000 901510 torch/_dynamo/utils.py:1450] Accuracy failed for key name bn1.bias.grad\nfail_accuracy\nlow priority for ampbf16 training not included in Meta PyTorch dashboard", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\ncheck the latest weekly report, this model pass now.", "root_cause": "", "state": "closed"}

### Merged Result:704{"issue_number": 704, "issue_description": "WARNING:common:fp64 golden ref were not generated for GPTNeoForSequenceClassification. Setting accuracy check to cosine\nWARNING:common:current_device=xpu; error:value cannot be converted to type float without overflow\nE0802 16:58:50.605000 3518147 torch/_dynamo/utils.py:1450] Accuracy failed for key name logits\nfail_accuracy\nThose 2 models belongs to skip list before, commit https://github.com/pytorch/pytorch/commit/8458980bbf78714a0fbe703785c100cad523fade change the skip logic. We'll submit PR to skip them again, but we need to double check whether we have potential real issue in here.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/704. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "state": "closed"}

### Merged Result:703{"issue_number": 703, "issue_description": "WARNING:common:current_device=xpu; error:value cannot be converted to type float without overflow\nE0802 18:11:51.919000 3841258 torch/_dynamo/utils.py:1450] Accuracy failed for key name transformer.h.0.attn.attention.k_proj.weight.grad\nfail_accuracy\nThose 2 models belongs to skip list before, commit https://github.com/pytorch/pytorch/commit/8458980bbf78714a0fbe703785c100cad523fade change the skip logic. We'll submit PR to skip them again, but we need to double check whether we have potential real issue in here.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "commit https://github.com/pytorch/pytorch/commit/8458980bbf78714a0fbe703785c100cad523fade change the skip logic", "state": "closed"}### Result:701 failed to extract### Result:699 failed to extract

### Merged Result:698{"issue_number": 698, "issue_description": "MTL: Reduction: Computation error in work group reduction on SLM when SIMD=8, The issue is filed for tracking. Will retrieve original logic if the issue is fixed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/698. The reporter of the issue is fengyuan14, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "\nClose issue as already been fixed.", "root_cause": "possibly an issue of compiler software stack, SYCL compiler or IGC", "state": "closed"}

### Merged Result:686{"issue_number": 686, "issue_description": "test_ops_xpu.py::TestCommonXPU::test_dtypes_nanmean_xpu - Exception: Caused by sample input at index 23: SampleInput(input=Tensor[size=(3, 2, 1, 2), device=\nUT failures with rolling build and LTS launch\n24-08-04T02:00:00.6674467Z Absolute difference: 9.183549615799121e-41\n2024-08-04T02:00:00.6674872Z Relative difference: 1.0\n2024-08-04T02:00:00.6675011Z\n2024-08-04T02:00:00.6675169Z To execute this test, run the following from the base repo dir:\n2024-08-04T02:00:00.6675805Z PYTORCH_TEST_WITH_SLOW=1 python test/test_binary_ufuncs.py -k TestBinaryUfuncsXPU.test_nextafter_bfloat16_xpu_bfloat16\n2024-08-04T02:00:00.6676218Z\n2024-08-04T02:00:00.6676413Z This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0\n2024-08-04T02:00:00.6677297Z FAILED test_binary_ufuncs_xpu.py::TestBinaryUfuncsXPU::test_non_contig_expand_logaddexp_xpu_complex128 - RuntimeError: \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/686. The reporter of the issue is mengfei25, and the assignee is majing921201, and the state of the issue is closed. The issue title is UT failures with rolling build and LTS launch, and the issue body is argument: 'private' and the message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0. The test to be executed is PYTORCH_TEST_WITH_SLOW=1 python test/nn/test_module_hooks.py -k TestStateDictHooks.test_register_state_dict_post_hook.\nThe torch-xpu-ops version is out of date, a lot of cases are skipped in latest test suites.", "reporter": "mengfei25", "assignee": "majing921201", "resolution": "The latest version has support it.\n\n\n\nThe torch-xpu-ops version is out of date", "root_cause": "torch-xpu-ops [de744d9](https://github.com/intel/torch-xpu-ops/commit/de744d9a92d5294041127235d1630466128eff1f) used in the test do not enable nanmean, so the backward_dtypes didn't align with cuda in infrstructure.", "state": "closed"}

### Merged Result:685{"issue_number": 685, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/685. The reporter of the issue is fengyuan14, and the assignee is xytintel, and the state of the issue is open. The github issue title Reduction: Enhance reduction kernel with supporting data type dynamic cast, and issue body Content of #685 is : ### \ud83d\ude80 The feature, motivation and pitch It is a performance requirement. The existing CUDA implementation in PyTorch supports data type dynamic cast, so that there won't be an extra kernel to align data types of input and output. ### Alternatives _No response_ ### Additional context _No response_, Extract the github issue description with error message information from issue tile and issue body, if possible also extract the resolution and root cause information. 0 {\nNot an urgent case, as the usage is rare. Lower the priority.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:683{"issue_number": 683, "issue_description": "TestMathBitsXPU issues\n\n- accuracy issue of test_neg_view_nn_functional_rrelu_xpu_float64 - PASSED on \ntorch              2.6.0a0+git64ccebd\ntorch-xpu-ops 3b245e2faeda3982f3147b3216fdee021051985a\n\n- RuntimeError: value cannot be converted to type float without overflow\n  TestMathBitsXPU has 2 cases with RuntimeError: value cannot be converted to type float without overflow - v2.6\n  \"test_conj_view_addbmm_xpu_complex64\",\n  \"test_neg_conj_view_addbmm_xpu_complex128\",   - duplicated with #436  \nRuntimeError: value cannot be converted to type float without overflow", "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": "\n", "root_cause": "The addbmm implementation of mkldnn has explicit cast to float like 'alpha.to<float>()', when the alpha and beta are complex, it causes the error.", "state": "closed"}

### Merged Result:676{"issue_number": 676, "issue_description": "TestMatmulCudaXPU.test_cublas_addmm_size_1000_xpu_float32 failed with the following error message:\n\nAssertionError: Tensor-likes are not close!\n\nMismatched elements: 9 / 1003002 (0.0%)\nGreatest absolute difference: 711.126220703125 at index (472, 999) (up to 0.1 allowed)\nGreatest relative difference: 2.7107455730438232 at index (472, 997) (up to 0.1 allowed)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/676. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\npassed with latest code", "root_cause": "", "state": "closed"}

### Merged Result:674{"issue_number": 674, "issue_description": "21 tests failed due to page fault. The tests are related to elementwise operations and involve functions like pairwise_distance, sgn, abs, l1_loss, smooth_l1_loss, and softsign. The tests are run with specific environment variables to enable debugging and profiling. The error message indicates a page fault during the execution of these tests.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/674. The reporter of the issue is Stonepia, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "fengyuan14", "resolution": "\nFixing: https://github.com/intel/torch-xpu-ops/pull/702, https://github.com/intel/torch-xpu-ops/pull/689", "root_cause": "", "state": "closed"}

### Merged Result:673{"issue_number": 673, "issue_description": "18 test cases fail with a PageFault error when running the pytest command with specific environment variables. The error occurs in the `UnrolledElementwiseKernel` function. The test cases are related to forward mode AD operations on complex and float64 data types on XPU.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/673. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nhttps://github.com/intel/torch-xpu-ops/pull/702, https://github.com/intel/torch-xpu-ops/pull/689", "root_cause": "", "state": "closed"}

### Merged Result:672{"issue_number": 672, "issue_description": "18 tests failed due to page fault. The tests are test_stable_sort_against_numpy_xpu_bfloat16, test_stable_sort_against_numpy_xpu_float16, test_stable_sort_against_numpy_xpu_float32, test_stable_sort_against_numpy_xpu_float64, test_stable_sort_against_numpy_xpu_int16, test_stable_sort_against_numpy_xpu_int32, test_stable_sort_against_numpy_xpu_int64, test_stable_sort_against_numpy_xpu_int8, test_stable_sort_against_numpy_xpu_uint8. The error message is 'PageFault' in the kernel 'ElementwiseGlobalRangeKernel'. The issue is reproducible with the following command: export DisableScratchPages=1 export NEOReadDebugKeys=1 export PYTORCH_TEST_WITH_SLOW=1 python -m pytest -v test_ops_fwd_gradients_xpu.py -k test_fn_fwgrad_bwgrad_abs_xpu_float64. The command for more detail: export SYCL_PI_TRACE=-1 export ZE_SERIALIZE=2 export OverrideImmediateCmdListSynchronousMode=1. The root cause is unknown, and the issue is closed with no resolution provided.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/672. The reporter of the issue is Stonepia, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "Stonepia", "assignee": "Stonepia", "resolution": "\nFixed by https://github.com/intel/torch-xpu-ops/pull/734 https://github.com/intel/torch-xpu-ops/pull/735", "root_cause": "", "state": "closed"}

### Merged Result:669{"issue_number": 669, "issue_description": "The test_module_xpu.py::TestModuleXPU::test_non_contiguous_tensors_nn_ConvTranspose3d_xpu_float32 test failed with the following error message: AssertionError: Tensor-likes are not close!\nMismatched elements: 1 / 540 (0.2%)\nGreatest absolute difference: 2.9087066650390625e-05 at index (0, 4, 2, 1, 0)\nGreatest relative difference: 2.5884704882628284e-05 at index (0, 4, 2, 1, 0)\nThe failure occurred for item [0]\nTo execute this test, run the following from the base repo dir:\nPYTORCH_TEST_WITH_SLOW=1 python test/test_modules.py -k TestModuleXPU.test_non_contiguous_tensors_nn_ConvTranspose3d_xpu_float32\nThe issue is reported by mengfei25 and assigned to daisyden, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/669. The reporter of the issue is mengfei25, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "daisyden", "resolution": "closed\nskip this case firstly and working on tolerance change", "root_cause": "The issue is related to the non-contiguous tensors and the ConvTranspose3d operation, and the root cause is not clear from the provided information.", "state": "closed"}### Result:667 failed to extract### Result:666 failed to extract

### Merged Result:664{"issue_number": 664, "issue_description": "While attempting to use the log_softmax operation on an XPU device, an error occurs indicating that the kernel is incompatible with all devices, despite recent commits purportedly adding support for this operation on XPU. The error message is: RuntimeError: Kernel is incompatible with all devices in devs. Steps to reproduce: 1. Set up a PyTorch environment with XPU support 2. Create a random tensor on the XPU device 3. Attempt to apply torch.nn.functional.log_softmax to the tensor Expected behavior: The log_softmax operation should execute successfully on the XPU device. Actual behavior: The operation fails with the RuntimeError stating the kernel is incompatible with all devices.\nsimilar to https://github.com/intel/torch-xpu-ops/issues/628 and to pull request https://github.com/intel/torch-xpu-ops/pull/511 It must set:\nexport OverrideDefaultFP64Settings=1\nexport IGC_EnableDPEmulation=1\n", "reporter": "zhiyuan1i", "assignee": "daisyden", "resolution": "The issue has been fixed in a recent commit. The root cause was that the kernel was not properly registered for the XPU device.\naccording to @PenghuiCheng 's verification, let's close the issue.", "root_cause": "The kernel was not properly registered for the XPU device. This was due to a recent change in the XPU backend that broke the compatibility of the log_softmax kernel with the XPU device. The fix was to register the kernel properly for the XPU device.", "state": "closed"}### Result:663 failed to extract### Result:662 failed to extract

### Merged Result:661{"issue_number": 661, "issue_description": "The test infrastructure for signbit function in test_unary_ufuncs does not align with typesIfCUDA, and the test cases for bool data type are missing. The test cases for bool data type should be added to align with typesIfCUDA.\nCuda can work on bool", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nCuda can work on bool", "root_cause": "The test cases for bool data type are missing, and the test infrastructure should align with typesIfCUDA.", "state": "closed"}

### Merged Result:658{"issue_number": 658, "issue_description": "TypeError: TestStateDictHooks.test_register_state_dict_post_hook() missing 1 required positional argument: 'private'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/658. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nThis case is skipped in latest pytorch.", "root_cause": "case is skiped in below version: torch-xpu-ops:fb8e6e9ef0240523c32a856a45220fc5cb55012c pytorch: e5560d10f4ee621b5952f61950761bac1d105afd", "state": "closed"}

### Merged Result:654{"issue_number": 654, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/654. The reporter of the issue is Stonepia, and the assignee is , and the state of the issue is closed.\nFor the existing code, I changed the first part in PR : https://github.com/intel/torch-xpu-ops/pull/655. The second part is not solved yet.", "reporter": "Stonepia", "assignee": "", "resolution": "\nThe first part of the issue is resolved by the PR https://github.com/intel/torch-xpu-ops/pull/655.", "root_cause": "The second part of the issue is not solved yet.", "state": "closed"}

### Merged Result:653{"issue_number": 653, "issue_description": "these cases are target 2.5\n - [x] LossNLL2d has no correct assert\n    \"test_cross_entropy_loss_2d_out_of_bounds_class_index_xpu_float16\",\n    \"test_cross_entropy_loss_2d_out_of_bounds_class_index_xpu_float32\" - [Jianghang](https://github.com/intel/torch-xpu-ops/pull/665)\n - [x] native_group_norm : RuntimeError: Expected X.is_contiguous(memory_format) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)\n    \"test_GroupNorm_memory_format_xpu\", - https://github.com/intel/torch-xpu-ops/pull/677\n - [x] upsamplingNearest2d: Failed: Unexpected success\n    \"test_upsamplingNearest2d_launch_fail_xpu\", - grid size check \n - [x] do not aline with cuda #656 fix it\n\"test_upsamplingBiMode2d_consistency\",\n\"test_upsamplingBiLinear2d_consistency_interp_size_bug\",\n - [x] cause by cuda hard code\n\"test_device_mask_xpu\", #656 fix it\n\"test_overwrite_module_params_on_conversion_cpu_device_xpu\",\n(https://github.com/pytorch/pytorch/blob/1fb498d6e34e0e9b43b2c26dc0a18a4fc3a52605/aten/src/ATen/native/cuda/UpSampleNearest2d.cu#L303) is specially for cuda, keep in skip list", "reporter": "yuchengliu1", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:645{"issue_number": 645, "issue_description": "Failed test cases:\npytest -sv third_party/torch-xpu-ops/test/xpu/extended/test_ops_xpu.py::TestCommonXPU::test_compare_cpu__refs_rsub_xpu_float16\npytest -sv third_party/torch-xpu-ops/test/xpu/test_nn_xpu.py::TestNNDeviceTypeXPU::test_grid_sample_large_xpu\npytest -sv third_party/torch-xpu-ops/test/xpu/test_nn_xpu.py::TestNNDeviceTypeXPU::test_variable_sequence_xpu_float16\npytest -sv third_party/torch-xpu-ops/test/xpu/test_tensor_creation_ops_xpu.py::TestTensorCreationXPU::test_float_to_int_conversion_finite_xpu_int64\npytest -sv third_party/torch-xpu-ops/test/xpu/test_reductions_xpu.py::TestReductionsXPU::test_all_any_vs_numpy_xpu_complex64\npytest -sv third_party/torch-xpu-ops/test/xpu/test_reductions_xpu.py::TestReductionsXPU::test_min_xpu_bool\npytest -sv third_party/torch-xpu-ops/test/xpu/test_indexing_xpu.py::TestIndexingXPU::test_index_put_accumulate_large_tensor_xpu\npytest -sv third_party/torch-xpu-ops/test/xpu/ test_dataloader.py\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/645. The reporter of the issue is mengfei25, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "daisyden", "resolution": "\n", "root_cause": "The issue is related to MTL and the test cases are failing on MTL. The test cases are passing on ARC and cuda. The issue is tracked and will be fixed in the future.", "state": "closed"}

### Merged Result:644{"issue_number": 644, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/644. The reporter of the issue is yuchengliu1, and the assignee is , and the state of the issue is closed. Extract the github issue title aten::_thnn_fused_gru_cell // CPU fallback could not cover it, and issue body Content of #644 is : , the resolution and root cause information.\nSorry, open the issue by accidental touch.", "reporter": "yuchengliu1", "assignee": "", "resolution": "\n", "root_cause": "Accidental touch when opening the issue.", "state": "closed"}

### Merged Result:640{"issue_number": 640, "issue_description": "Warning: Warning only once for all operators, other operators may also be overrided. Overriding a previously registered kernel for the same operator and the same dispatch key operator: aten::norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!) dispatch key: XPU previous kernel: registered at /home/dvrogozh/git/pytorch/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30476 new kernel: registered at /home/dvrogozh/git/pytorch/pytorch/build/aten/src/ATen/xpu/RegisterXPU.cpp:7169 (function operator())\nThat's regression after 6eca3940f2a1d1bce884e0c4b929157c0fa3f88a by @yucai-intel, #557.", "reporter": "dvrogozh", "assignee": "", "resolution": "\n", "root_cause": "Should be a typo when resolving conflict.", "state": "closed"}

### Merged Result:636{"issue_number": 636, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/636. The reporter of the issue is daisyden, and the assignee is huaiyuzh, and the state of the issue is closed.\nThis issues is used to  retrieve the fine grain cases when embedding_renorm_ added.", "reporter": "daisyden", "assignee": "huaiyuzh", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:632{"issue_number": 632, "issue_description": "Squeezenet1_1 accuracy minify\nThe issues should related to conv + relu + adaptive_avgpool in the backward pattern. However, it is really hard to reproduce in unit test. Still working on this problem.", "reporter": "retonym", "assignee": "retonym", "resolution": "\nFix PR: https://github.com/intel/torch-xpu-ops/pull/668", "root_cause": "pytorch PR: https://github.com/pytorch/pytorch/pull/84541", "state": "open"}### Result:631 failed to extract

### Merged Result:629{"issue_number": 629, "issue_description": "The operation failed with error: The operator 'aten::masked_select on the XPU backend is falling back to run on the CPU. (Triggered internally at /home/lzy/workspace/pytorch/build/aten/src/ATen/xpu/RegisterXPU.cpp:6313.)'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/629. The reporter of the issue is zhiyuan1i, and the assignee is xytintel, and the state of the issue is closed.", "reporter": "zhiyuan1i", "assignee": "xytintel", "resolution": "\nPR ready: https://github.com/intel/torch-xpu-ops/pull/649", "root_cause": "", "state": "closed"}

### Merged Result:628{"issue_number": 628, "issue_description": "When trying to create a random tensor and convert it to bfloat16, I receive the following error:\n\n```python\nout = torch.randn(batch_size, vocab_size, device='xpu').to(torch.bfloat16)\n# RuntimeError: Required aspect fp64 is not supported on the device\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/628. The reporter of the issue is zhiyuan1i, and the assignee is riverliuintel, and the state of the issue is closed.\nrandom. Even your operator is FP64 irrelevant, runtime compilation gets failure due to per-source build. Per-source build means all kernels including FP64 involved in the same source file are built when calling one of them. Driver raises building error. AOT build (building machine code statically) should help on this.", "reporter": "zhiyuan1i", "assignee": "riverliuintel", "resolution": "\nThe issue was closed by the assignee.\nThis feature has been implemented in PT2.5 and long time no response, close this ticket.", "root_cause": "The XPU device does not support fp64 operations. However, the `torch.randn()` function seems to be attempting to use fp64 internally before converting to the desired dtype.", "state": "closed"}

### Merged Result:626{"issue_number": 626, "issue_description": "With both key and value data type being 64 bits, there will be occasional computation issues on MTL machines. \nneed to investigate the sort kernel refinement in 2.6.", "reporter": "xytintel", "assignee": "xytintel", "resolution": "\n", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/626. The reporter of the issue is xytintel, and the assignee is xytintel, and the state of the issue is closed.", "state": "closed"}

### Merged Result:623{"issue_number": 623, "issue_description": "Failure case: test_nextafter_bfloat16_xpu_bfloat16. https://github.com/pytorch/pytorch/blob/c2425a3b572328c6c1fdadc080f8a83c6357f945/test/test_binary_ufuncs.py#L2949 We aligned CPU and CUDA implementation by using std::nextafter. But got failure, AssertionErro: Scalars are not equal! Expected 9.183549615799121e-41 but got 0.0. Absolute difference: 9.183549615799121e-41 Relative difference: 1.0\ncompiler dependency, move to 2.8.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\ncompiler dependency, move to 2.8.", "root_cause": "The issue is caused by the difference in `std::nextafter` implementation between CPU (GCC) and XPU (SYCL). The failure is due to the fact that `std::nextafter` returns 0.0 for some inputs on XPU, which causes the assertion error.", "state": "open"}### Result:622 failed to extract

### Merged Result:618{"issue_number": 618, "issue_description": "Some features requirement for test_autograd_xpu.py\n\n### \ud83d\ude80 The feature, motivation and pitch\n\n1. module 'torch._C' has no attribute '_scatter'\ncases:\nTestAutograd.test_checkpointing_without_reentrant_dataparallel,\nTestMultithreadAutograd.test_dataparallel_saved_tensors_hooks\n\n2. AttributeError: module 'torch.xpu' has no attribute------passed on latest version\ncases:\nTestAutograd.test_graph_save_on_cpu_cuda,  -----passed\nTestAutograd.test_checkpointing_without_reentrant_memory_savings,-----passed\n\n3. NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend.\ncases:\ntest_sparse_mask_autograd_xpu\ntest_sparse_ctor_getter_backward_xpu_float64\ntest_sparse_ctor_getter_backward_xpu_complex128\ntest_sparse_backward_xpu_float64\ntest_sparse_backward_xpu_complex128\n\n4. c10::NotImplementedError\ncases:\nTestAutogradMultipleDispatchXPU::test_autograd_multiple_dispatch_registrations_xpu\n\n5. RuntimeError: Double and complex datatype matmul is not supported in oneDNN\ncases:\ntest_autograd_xpu.py::TestAutogradDeviceTypeXPU::test_mv_grad_stride_0_xpu\n\n### Alternatives\n_No response_\n\n### Additional context\n_No response_,\nLow priority features, No.1 is not an XPU specific issue. and not ATen operator (torch-xpu-ops) related. Scatter in c10d. No.2 is not operator (torch-xpu-ops) related. No.3 is a Sparse related feature. The direction of Sparse support is on-demand. Our priority follows requirements what we meet in HF/TIMM/TB models. No.4 is not an operator implementation issue. No.5 oneDNN issues are tracked in a separate issue.", "reporter": "PenghuiCheng", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:614{"issue_number": 614, "issue_description": "New failures occur when PyTorch uplifts. Guilty commit should be between f053be2a97e1f6f9b2252cb800edd46f720af502 and d44c30e2f90d9ebe829875324f0ac662d04833a8. \ntest_compare_cpu_nn_functional_batch_norm_xpu_float16 \ntest_compare_cpu_std_mean_xpu_bfloat16 \ntest_compare_cpu_var_mean_xpu_bfloat16 \nall within the threashold.\nThe two cases can pass with updated threshold.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\nUpdated threshold", "root_cause": "", "state": "closed"}

### Merged Result:613{"issue_number": 613, "issue_description": "UT test error: RuntimeError: 0 <= device && static_cast<size_t>(device) < device_allocators.size() INTERNAL ASSERT FAILED\n\nCases:\n- TestDataLoaderDeviceTypeXPU.test_nested_tensor_multiprocessing_context_forkserver_xpu\n- TestDataLoaderDeviceTypeXPU.test_nested_tensor_multiprocessing_context_spawn_xpu\n\nThis issue is reported by PenghuiCheng and assigned to guangyey. The issue is closed.\nThis should not exists now. Close it.", "reporter": "PenghuiCheng", "assignee": "guangyey", "resolution": "\nThis should not exists now. Close it.", "root_cause": "", "state": "closed"}### Result:611 failed to extract### Result:603 failed to extract

### Merged Result:602{"issue_number": 602, "issue_description": "RMSE (res-fp64): 0.00285, (ref-fp64): 0.00003 and shape=torch.Size([512]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.010000\nAccuracy failed for key name norm.bias.grad\nRMSE (res-fp64): 0.02741, (ref-fp64): 0.01008 and shape=torch.Size([128, 1, 7, 7]). res.dtype: torch.float16, multiplier: 2.000000, tol: 0.010000\nAccuracy failed for key name stages.0.blocks.0.conv_dw.weight.grad\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/602. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\ncould pass with `_adaptive_avg_pool2d_backward` fallback to cpu", "root_cause": "jx_nest_base pass locally, convnext_base also fails on A100", "state": "closed"}

### Merged Result:601{"issue_number": 601, "issue_description": "### \ud83d\udc1b Describe the bug\n\ntriage it by https://github.com/intel/torch-xpu-ops/commit/8c620937f890fef3caf55c96c48bfb70f8d39949.\n\n# Target 2.5\n- [x] RuntimeError: output 1: meta disagrees with real impl:\n    \nThe issue is about the performance of the torch_xpu_ops library, specifically the problem of the slow execution of certain operations. The reporter of the issue is yuchengliu1, and the assignee is fengyuan14. The issue is closed with the state of closed. The comments for this issue are not provided.", "reporter": "yuchengliu1", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:598{"issue_number": 598, "issue_description": "TestCommonXPU::test_out_bincount_xpu_int64\n2024-07-11T08:48:47.7389771Z FATAL: Unexpected page fault from GPU at 0x56443f92d000, ctx_id: 1 (CCS) type: 0 (NotPresent), level: 3 (PML4), access: 1 (Write), banned: 1, aborting.\n\nThe issue is fixed in the latest Pytorch and torch-xpu-ops bundle.", "reporter": "mengfei25", "assignee": "Stonepia", "resolution": "\nThe issue is fixed in the latest Pytorch and torch-xpu-ops bundle.", "root_cause": "", "state": "closed"}

### Merged Result:594{"issue_number": 594, "issue_description": "AssertionError: Tensor-likes are not close!\nFor extreme value processing, Numpy and XPU results are inconsistent, std operations get different behavior on std::complex operarands for extremal cases.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nNot fixed", "root_cause": "Inconsistent results between Numpy and XPU for extreme value processing, different behavior for std::complex operands in std operations for extremal cases.", "state": "closed"}

### Merged Result:593{"issue_number": 593, "issue_description": "some case xfailed in cuda because of cuda bug. However XPU calculated correctly, and need not to xfail like cuda\n\nCase list as below:\n\"test_reference_numerics_large_rsqrt_xpu_complex32\"\n\"test_errors_histogramdd_xpu\"\n\"test_noncontiguous_samples__batch_norm_with_update_xpu_float32\"\n\"test_dispatch_symbolic_meta_outplace_all_strides__batch_norm_with_update_xpu_float32\"\n\"test_out_histc_xpu_float32\"\n\"test_out_warning_logcumsumexp_xpu\"\n\"test_python_ref__refs_mul_xpu_complex32\"\n\"test_python_ref_torch_fallback__refs_mul_xpu_complex32\"\n\"test_type_promotion_logaddexp_xpu\"\n\nUnexpected success on PVC and XFAIL on MTL device\n\"test_modules_xpu.py::TestModuleXPU::test_cpu_gpu_parity_nn_ConvTranspose1d_xpu_complex32\"\n\"test_modules_xpu.py::TestModuleXPU::test_cpu_gpu_parity_nn_ConvTranspose2d_xpu_complex32\"\n\"test_modules_xpu.py::TestModuleXPU::test_memory_format_nn_AvgPool2d_xpu_float32\"\n\"test_modules_xpu.py::TestModuleXPU::test_memory_format_nn_AvgPool2d_xpu_float64\" \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/593. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nDone in PR #608", "root_cause": "", "state": "closed"}

### Merged Result:592{"issue_number": 592, "issue_description": "AssertionError: True is not false\nlow priority for 2.5", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:590{"issue_number": 590, "issue_description": "RuntimeError: \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/590. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nwill be fixed with #571", "root_cause": "", "state": "closed"}

### Merged Result:589{"issue_number": 589, "issue_description": "AssertionError: Tensor-likes are not equal! :\n    \"test_quick__batch_norm_with_update_xpu_bfloat16\",\n    \"test_quick__batch_norm_with_update_xpu_float16\",\n\nTo execute this test, run the following from the base repo dir:\n    PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=0 PYTORCH_TEST_WITH_SLOW=1 python test/test_decomp.py -k TestDecompXPU.test_quick__batch_norm_with_update_xpu_bfloat16\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/589. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nThe issue is closed.", "root_cause": "The issue is closed.", "state": "closed"}

### Merged Result:586{"issue_number": 586, "issue_description": "The existing solution: https://github.com/intel/torch-xpu-ops/pull/581\nTo split libtorch_xpu.so into multiple libraries,\n1. libtorch_xpu.so, including operator level program. It is host only.\n2. libtorch-xpu-ops-sycl-ker-partx.so, including kernel level program, both device code and host code.\n\n### Alternatives\n_No response_\n\n### Additional context\n_No response_,\ndevice code compression", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed\ndevice code compression mitigates panic", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/586. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed.", "state": "closed"}

### Merged Result:585{"issue_number": 585, "issue_description": "Pytorch compilation fail on assertion\nPytorch compilation fail on assertion\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/585. The reporter of the issue is ZzEeKkAa, and the assignee is Stonepia, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/585. The reporter of the issue is ZzEeKkAa, and the assignee is Stonepia, and the state of the issue is closed.\nThe reporter of the issue is ZzEeKkAa, and the assignee is Stonepia, and the state of the issue is closed.\nThe reporter of the issue is ZzEeKkAa, and the assignee is Stonepia, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/585. The reporter of the issue is ZzEeKkAa, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "ZzEeKkAa", "assignee": "Stonepia", "resolution": "\n\n\nclosed\nclosed\n\nThe issue is resolved by changing the argument to TORCH_SELECTIVE_NAME to include `::` and not include `(`.\nThe issue was resolved by updating the commit in pytorch similar to the PR: https://github.com/pytorch/pytorch/pull/130333.", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/585. The reporter of the issue is ZzEeKkAa, and the assignee is Stonepia, and the state of the issue is closed.", "state": "closed"}

### Merged Result:584{"issue_number": 584, "issue_description": "AssertionError: Jiterator is only supported on CUDA and ROCm GPUs, none are available.\n\nAssertionError: Scalars are not close!\n\nRuntimeError: device type of values (xpu) must be CPU or CUDA or Meta\n\nRuntimeError: value cannot be converted to type float without overflow\n\nRuntimeError: could not create a primitive\n\nRuntimeError: could not create a primitive descriptor for a deconvolution forward propagation primitive\n\nAssertionError: Tensor-likes are not equal!\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/584. The reporter of the issue is yuchengliu1, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "fengyuan14", "resolution": "\nclosed", "root_cause": "The issue is closed and no further action is required.", "state": "closed"}

### Merged Result:583{"issue_number": 583, "issue_description": "Unexpected success\nCPU fallback fails. Implementation difference between CPU and CUDA. Expect success on CPU and expect fail on CUDA. When we use CPU fallback and align expected fail list with CUDA, these cases fail.\n\n# AssertionError: RuntimeError not raised\n# RuntimeError: Tried to instantiate dummy base class CUDAGraph\n\nunexpected success caused by those OP fallback to cpu at current stage. Need to double check", "reporter": "yuchengliu1", "assignee": "fengyuan14", "resolution": "\ntest_pointwise_op_with_tensor_of_scalarlist_overload__foreach_addcdiv_is_fastpath_True_xpu_float16 passed now", "root_cause": "those OP fallback to cpu at current stage", "state": "closed"}

### Merged Result:582{"issue_number": 582, "issue_description": "some cases in test_linalg.py use triton. Pre-ci has installed triton. But these case pass in local but fail in pre-ci.\n\n# these case passed in a env with triton, but triton did not install in pre-ci.\n    \"test_compile_int4_mm_m_32_k_32_n_48_xpu\",\n    \"test_compile_int4_mm_m_32_k_32_n_64_xpu\",\n    \"test_compile_int4_mm_m_32_k_64_n_48_xpu\",\n    \"test_compile_int4_mm_m_32_k_64_n_64_xpu\",\n    \"test_compile_int4_mm_m_64_k_32_n_48_xpu\",\n    \"test_compile_int4_mm_m_64_k_32_n_64_xpu\",\n    \"test_compile_int4_mm_m_64_k_64_n_48_xpu\",\n    \"test_compile_int4_mm_m_64_k_64_n_64_xpu\",\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/582. The reporter of the issue is yuchengliu1, and the assignee is mengfei25, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "mengfei25", "resolution": "\n", "root_cause": "pre-ci installed triton", "state": "closed"}

### Merged Result:578{"issue_number": 578, "issue_description": "RuntimeError: value cannot be converted to type float without overflow\nJacobian computed with forward mode mismatch for output 0 with respect to input 0\nNotImplementedError: Could not run 'aten::_to_dense' with arguments from the 'SparseXPU' backend.\nWhile considering the real part of complex inputs only, Jacobian computed with forward mode mismatch for output 0 with respect to input 0, test_fn_fwgrad_bwgrad_linalg_norm_xpu_complex128", "reporter": "yuchengliu1", "assignee": "fengyuan14", "resolution": "\nrrelu, sparsity, norm issues are fixed, addbmm issue depends on oneMKL.", "root_cause": "While considering the real part of complex inputs only, Jacobian computed with forward mode mismatch for output 0 with respect to input 0, test_fn_fwgrad_bwgrad_linalg_norm_xpu_complex128", "state": "closed"}

### Merged Result:577{"issue_number": 577, "issue_description": "Re-triage it by https://github.com/intel/torch-xpu-ops/commit/cbb4ab17a781c77108443f12f7ce254a345f1a14. Old issue is https://github.com/intel/torch-xpu-ops/issues/317\n\n# addmm.out, addmv.out, linalg_lstsq, norm.out, vdot&dot lack XPU support and fallback to CPU\n    \"test_addmm_sizes_xpu_complex128\",\n    \"test_addmm_sizes_xpu_complex64\",\n    \"test_blas_alpha_beta_empty_xpu_complex128\",\n    \"test_blas_alpha_beta_empty_xpu_complex64\",\n    \"test_linalg_lstsq_input_checks_xpu_complex128\",\n    \"test_linalg_lstsq_input_checks_xpu_complex64\",\n    \"test_linalg_lstsq_input_checks_xpu_float32\",\n    \"test_linalg_lstsq_input_checks_xpu_float64\",\n    \"test_dot_invalid_args_xpu\",\n    \"test_vdot_invalid_args_xpu\",\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/577. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\n", "root_cause": "", "state": "closed"}### Result:576 failed to extract

### Merged Result:572{"issue_number": 572, "issue_description": "Implement aten::_unique for XPU,\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/572. \nThe reporter of the issue is fengyuan14, \nand the assignee is fengyuan14,\nand the state of the issue is closed.\n\nThis is the github issue title Implement aten::_unique for XPU,\n\nContent of #572 is : ### \ud83d\ude80 The feature, motivation and pitch\n\nTest TestCompositeComplianceXPU.test_backward_index_fill_xpu_float32 requires the operator.\n\n### Alternatives\n\nPT 2.6\n\n### Additional context\n\n_No response_,\nThe operator is implemented, and the case is enabled.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nThe operator is implemented, and the case is enabled.", "root_cause": "", "state": "closed"}

### Merged Result:570{"issue_number": 570, "issue_description": "The operator 'aten::__lshift__.Scalar' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/570. The reporter of the issue is ZzEeKkAa, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "ZzEeKkAa", "assignee": "fengyuan14", "resolution": "closed\nClosing it, since it was implemented in #688", "root_cause": "The operator 'aten::__lshift__.Scalar' is not currently implemented for the XPU device.", "state": "closed"}

### Merged Result:551{"issue_number": 551, "issue_description": "FATAL: Unexpected page fault from GPU at 0xff0000014c000000, ctx_id: 1 (CCS) type: 0 (NotPresent), level: 1 (PDE), access: 0 (Read), banned: 1, aborting.\nFATAL: Unexpected page fault from GPU at 0xff0000014c000000, ctx_id: 1 (CCS) type: 0 (NotPresent), level: 1 (PDE), access: 0 (Read), banned: 1, aborting.\nAbort was called at 287 line in file:\n./shared/source/os_interface/linux/drm_neo.cpp\nRun failed with return code:  -6\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/551. The reporter of the issue is mengfei25, and the assignee is Stonepia, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "Stonepia", "resolution": "\n", "root_cause": "This issue might be the same as other models (thus, it should be fixed).", "state": "closed"}

### Merged Result:549{"issue_number": 549, "issue_description": "New failures occur when PyTorch uplifts. Guilty commit should be between f053be2a97e1f6f9b2252cb800edd46f720af502 and d44c30e2f90d9ebe829875324f0ac662d04833a8. test_compare_cpu_sub_xpu_float16, also failed on cuda, test_symnode_hashing, test_index_ind_dtype_xpu\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/549. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\ntest_index_ind_dtype_xpu can pass with 10c7f037fe3271cb3865816c216007ba403f5347 and reverted PR https://github.com/intel/torch-xpu-ops/commit/4db0b0cd1ca51d9cfd890be2eb3527b165782220, because checkIndexTensorTypes interface changes is reverted by https://github.com/pytorch/pytorch/commit/fb696bf26457a60583f4c43f1a6547b16725c016. So we should also revert torch-xpu-ops 4db0b0cd1ca51d9cfd890be2eb3527b165782220, add by the 2nd parameter of checkIndexTensorTypes.", "root_cause": "The issue of test_symnode_hashing has disappeared in latest version of pytorch and torch-xpu-ops.", "state": "closed"}

### Merged Result:544{"issue_number": 544, "issue_description": "Got numerical difference compared with CPU results. It is hard to say who is better on accuracy.\n\n# Mismatched elements: 7 / 1048576 (0.0%)\n# Greatest absolute difference: 0.4922053598013041 at index (765, 860) (up to 1e-07 allowed)\n# Greatest relative difference: 0.15330001655652495 at index (765, 860) (up to 1e-07 allowed)\n\nNot a critical error. Low priority.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:536{"issue_number": 536, "issue_description": "Implement aten::_embedding_bag_backward, retrieve this case when this op has been implemented , test_backward_nn_functional_embedding_bag_xpu_float32\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/536. The reporter of the issue is chunhuanMeng, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "chunhuanMeng", "assignee": "fengyuan14", "resolution": "closed\nwe can close this issue", "root_cause": "The reporter of the issue is chunhuanMeng, and the assignee is fengyuan14, and the state of the issue is closed.", "state": "closed"}

### Merged Result:528{"issue_number": 528, "issue_description": "After enabling XPU adaptive pooling 2d, accuracy of Eager is better than Inductor (before that, CPU fallback == Inductor), and the gap could not be accepted by default Dynamo benchmark tolerance. E0702 17:27:03.278000 140425655064384 torch/_dynamo/utils.py:1470] RMSE (res-fp64): 0.00068, (ref-fp64): 0.00004 and shape=torch.Size([1152]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.001000 E0702 17:27:03.279000 140425655064384 torch/_dynamo/utils.py:1384] Accuracy failed for key name blocks.6.0.bn1.bias.grad fail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/528. The reporter of the issue is fengyuan14, and the assignee is riverliuintel, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "riverliuintel", "resolution": "Loose the tolerance for the model temporarily.\nverified pass with latest code", "root_cause": "This issue is caused by a precision error of Inductor on TorchBench/timm_efficientnet ampbf16 training. The reporter of the issue is fengyuan14, and the assignee is riverliuintel, and the state of the issue is closed.", "state": "closed"}

### Merged Result:523{"issue_number": 523, "issue_description": "When output_size == 1, CPU and CUDA are using reduce mean, but we are using adaptive_avg_pool. The story is we preferred oneDNN implementation before. The issue is recorded to evaluate whether the difference should be kept. The current implementation is to register wrapper variant `aten::adaptive_avg_pool2d` to retrieve the logic for XPU.\nWithout the logic (mean for output_size == 1), a model in TorchBench crashes due to lack of deterministic impl in adaptive avg pool2d.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nThe behavior of XPU `adaptive_avg_pool{2d/3d}` has been aligned to CUDA/CPU after [pytorch#132217](https://github.com/pytorch/pytorch/pull/132217) is merged. Our WA for `output_size == 1` will not be executed. The redundant implementation in torch-xpu-ops has been removed by #851.", "root_cause": "The logic for mean operation when output_size is 1 was missing in the XPU implementation of adaptive avg pool2d, leading to a crash in TorchBench due to lack of deterministic implementation.", "state": "closed"}

### Merged Result:510{"issue_number": 510, "issue_description": "torchbench_amp_bf16_training xpu train functorch_maml_omniglot fail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/510. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nclosed", "root_cause": "RMSE (res-fp64): 0.00109, (ref-fp64): 0.00024 and shape=torch.Size([]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000", "state": "closed"}

### Merged Result:509{"issue_number": 509, "issue_description": "torchbench_bfloat16_training xpu train phlippe_resnet got fail_accuracy, RMSE (res-fp64): 0.00734, (ref-fp64): 0.00047 and shape=torch.Size([]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.001000\nNot very large absolute error, and this model could pass if increasing tol to 5*1e-3", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nPublic PR to raise tolerance: https://github.com/pytorch/pytorch/pull/134192", "root_cause": "Not very large absolute error", "state": "closed"}

### Merged Result:508{"issue_number": 508, "issue_description": "torchbench_bfloat16_training xpu train functorch_dp_cifar10 got fail_accuracy, the error message is RMSE (res-fp64): 0.00109, (ref-fp64): 0.00027 and shape=torch.Size([64]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.001000, Accuracy failed for key name bn1.bias.grad, the issue is closed, the reporter is mengfei25, the assignee is weishi-deng\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/508. The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "The issue is caused by the convolution_backward but we're still looking for the fix.", "state": "closed"}

### Merged Result:507{"issue_number": 507, "issue_description": "torchbench_bfloat16_training\nxpu  train squeezenet1_1\nE0626 09:48:28.341000 140268361156416 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.06469, (ref-fp64): 0.01171 and shape=torch.Size([4, 1000]). res.dtype: torch.bfloat16, multiplier: 3.000000, tol: 0.001000\nfail_accuracy\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/507. The reporter of the issue is mengfei25, and the assignee is retonym, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "The issue is caused by _adaptive_avg_pool2d_backward op. This op doesn't have deterministic implementation for both xpu and cuda. However the fail_accuracy only happens in xpu device", "state": "closed"}

### Merged Result:506{"issue_number": 506, "issue_description": "RuntimeError: Input type (float) and bias type (c10::BFloat16) should be the same\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4177, in run\n    ) = runner.load_model(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 380, in load_model\n    self.validate_model(model, example_inputs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2296, in validate_model\n    raise RuntimeError(\"\"\"Eager run failed\"\"\" ) from e\nRuntimeError: Eager run failed\n\neager_fail_to_run\n\nloading model: 0it [00:00, ?it/s]\nloading model: 0it [01:18, ?it/s]\n\nBoth fp16 and bf16 have the problem", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\nClose the issue, since A100 also encounters the issue", "root_cause": "Input type (float) and bias type (c10::BFloat16) should be the same", "state": "open"}### Result:505 failed to extract

### Merged Result:504{"issue_number": 504, "issue_description": "torchbench_float32_training xpu train demucs got fail_accuracy, RMSE (res-fp64): 0.03316, (ref-fp64): 0.00065 and shape=torch.Size([]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000\nKnown issue. Use to be closed because it's a common issue for all backends: CPU, CUDA, XPU.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nwill close this issue, and only track issues their with adding fp32 label", "root_cause": "similar issue in https://github.com/intel/torch-xpu-ops/issues/488.", "state": "closed"}

### Merged Result:503{"issue_number": 503, "issue_description": "W0626 13:18:03.033000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:18:03.033000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:18:03.212000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:18:03.213000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:18:03.390000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:18:03.391000 139736640825152 torch/_inductor/utils.py:1221] [3/0_2] DeviceCopy in input program\nW0626 13:19:19.452000 139736640825152 torch/_dynamo/utils.py:1452] Found nan in reference. Consider running in higher precision.\nE0626 13:19:19.452000 139736640825152 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.02577, (ref-fp64): nan and shape=torch.Size([4, 3, 12, 16, 85]). res.dtype: torch.float16, multiplier: 2.000000, tol: 0.001000\nfail_accuracy\nNo accuracy issue in latest torch xpu ops", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nNo accuracy issue", "root_cause": "", "state": "closed"}

### Merged Result:502{"issue_number": 502, "issue_description": "torchbench_float16_training xpu train functorch_dp_cifar10 Accuracy failed for key name bn1.bias.grad, RMSE (res-fp64): 0.00107, (ref-fp64): 0.00007 and shape=torch.Size([64]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000\nSimilar to https://github.com/intel/torch-xpu-ops/issues/508", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\nclosed", "root_cause": "duplicate", "state": "closed"}

### Merged Result:501{"issue_number": 501, "issue_description": "torchbench_float16_training xpu train squeezenet1_1 got fail_accuracy, RMSE (res-fp64): 0.00654, (ref-fp64): 0.00153 and shape=torch.Size([4, 1000]). res.dtype: torch.float16, multiplier: 2.000000, tol: 0.001000\nSimilar issue in https://github.com/intel/torch-xpu-ops/issues/507, add fp16 datatype label in that issue", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:500{"issue_number": 500, "issue_description": "torchbench_float16_training xpu train Background_Matting\n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2294, in validate_model\n    self.model_iter_fn(model, example_inputs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 456, in forward_and_backward_pass\n    pred = mod(*cloned_inputs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1566, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1575, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 219, in forward\n    input = module(input)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1566, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1575, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/modules/padding.py\", line 359, in forward\n    return F.pad(input, self.padding, 'reflect')\n  File \"/home/sdp/miniforge3/envs/e2e_ci/lib/python3.10/site-packages/torch/nn/functional.py\", line 4552, in pad\n    return torch._C._nn.pad(input, pad, mode, value)\nRuntimeError: \"reflection_pad2d\" not implemented for 'Half'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4177, in run\n    ) = runner.load_model(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 380, in load_model\n    self.validate_model(model, example_inputs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2296, in validate_model\n    raise RuntimeError(\"\"\"Eager run failed\"\"\" ) from e\nRuntimeError: Eager run failed\n\neager_fail_to_run\n\nloading model: 0it [00:00, ?it/s] \nloading model: 0it [00:04, ?it/s] \n", "reporter": "mengfei25", "assignee": "", "resolution": "", "root_cause": "reflection_pad2d not implemented for 'Half'", "state": "closed"}

### Merged Result:499{"issue_number": 499, "issue_description": "Demucs RuntimeError: Input type (float) and bias type (c10::Half) should be the same\nsame issue with https://github.com/intel/torch-xpu-ops/issues/506 could track the problem in the single issue", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\n", "root_cause": "", "state": "closed"}### Result:496 failed to extract

### Merged Result:495{"issue_number": 495, "issue_description": "The operator 'aten::norm.dtype_out' is not currently implemented for the XPU device. Please open a feature on https://github.com/intel/torch-xpu-ops/issues. You can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU implementation as a fallback for XPU unimplemented operators. WARNING: this will bring unexpected performance compared with running natively on XPU.\nThis operator is not implemented with xpu backend. @fengyuan14  please help to check this issue. Thanks.", "reporter": "mengfei25", "assignee": "fengyuan14", "resolution": "The issue has been closed.\nThe issue is resolved by the pull request #556.", "root_cause": "The operator 'aten::norm.dtype_out' is not currently implemented for the XPU device.", "state": "closed"}

### Merged Result:494{"issue_number": 494, "issue_description": "Torch_multimodal_clip NotImplementedError: The operator 'aten::norm.dtype_out' is not currently implemented for the XPU device.,\n\nThe above exception was the direct cause of the following exception:\n\nRuntimeError: Eager run failed\n\neager_fail_to_run\nThe reporter of the issue is mengfei25, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "fengyuan14", "resolution": "\npass in latest weekly", "root_cause": "The operator 'aten::norm.dtype_out' is not currently implemented for the XPU device.", "state": "closed"}

### Merged Result:493{"issue_number": 493, "issue_description": "torchbench_amp_fp16_training xpu train timm_regnet E0626 18:18:36.100000 139652021139264 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.00227, (ref-fp64): 0.00064 and shape=torch.Size([]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000 fail_accuracy float16 E0626 13:14:09.343000 139963949791040 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.00150, (ref-fp64): 0.00032 and shape=torch.Size([224]). res.dtype: torch.float16, multiplier: 3.000000, tol: 0.001000 E0626 13:14:09.343000 139963949791040 torch/_dynamo/utils.py:1392] Accuracy failed for key name s3.b4.se.fc1.bias.grad fail_accuracy\nNot very large absolute error, and this model could pass if increasing tol to 1e-2", "reporter": "mengfei25", "assignee": "jianyizh", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:492{"issue_number": 492, "issue_description": "Timm_efficientdet NotImplementedError: The original model code forces the use of CUDA., Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4177, in run\n    ) = runner.load_model(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 320, in load_model\n    benchmark = benchmark_cls(\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__\n    obj = type.__call__(cls, *args, **kwargs)\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/timm_efficientdet/__init__.py\", line 55, in __init__\n    raise NotImplementedError(\"The original model code forces the use of CUDA.\")\nNotImplementedError: The original model code forces the use of CUDA.\nmodel_fail_to_load\nThis model requests us to add xpu support for both the benchmark repo and third-party repo efficientdet-pytorch as it writes hard code with cuda like: (in https://github.com/rwightman/efficientdet-pytorch/blob/master/effdet/data/loader.py).", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "The code in efficientdet-pytorch uses cuda and does not have xpu support.", "state": "open"}

### Merged Result:491{"issue_number": 491, "issue_description": "Pytorch_CycleGAN_and_pix2pix RuntimeError: \"reflection_pad2d\" not implemented for 'Half',\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/491. The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "The issue is on the cpu implementation. Our implementation in IPEX has enabled the fp16 support.", "state": "closed"}

### Merged Result:490{"issue_number": 490, "issue_description": "FastNLP_Bert Accuracy failed for key name bert.model.encoder.embeddings.LayerNorm.weight.grad, and the error message is RMSE (res-fp64): 0.00383, (ref-fp64): 0.00017 and shape=torch.Size([768]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000\nE0626 17:50:10.261000 140234806880064 torch/_dynamo/utils.py:1392] Accuracy failed for key name bert.model.encoder.embeddings.LayerNorm.weight.grad\nfail_accuracy\nNo accuracy issue in latest torch xpu ops", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nNo accuracy issue", "root_cause": "", "state": "closed"}

### Merged Result:489{"issue_number": 489, "issue_description": "torchbench_amp_fp16_training xpu train moco Traceback (most recent call last):  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 4177, in run  ) = runner.load_model(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/torchbench.py\", line 320, in load_model  benchmark = benchmark_cls(  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/util/model.py\", line 39, in __call__  obj = type.__call__(cls, *args, **kwargs)  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/benchmark/torchbenchmark/models/moco/__init__.py\", line 80, in __init__  raise NotImplementedError(f\"{device} not supported\") NotImplementedError: xpu not supported model_fail_to_load\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/489. The reporter of the issue is mengfei25, and the assignee is weishi-deng, and the state of the issue is open.", "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:488{"issue_number": 488, "issue_description": "torchbench_amp_fp16_training\nxpu  train demucs\nE0626 17:48:23.409000 140063059666752 torch/_dynamo/utils.py:1478] RMSE (res-fp64): 0.03316, (ref-fp64): 0.00065 and shape=torch.Size([]). res.dtype: torch.float32, multiplier: 3.000000, tol: 0.001000\nfail_accuracy\nThis model failure is not the random seed setting issue which we fixed before. It should be a new issue caused by recent changes. Previously, the model could pass after fixing random seed issue.", "reporter": "mengfei25", "assignee": "retonym", "resolution": "\nhttps://github.com/pytorch/pytorch/pull/134302 to disable reorder_for_locality.", "root_cause": "The random kernels output different value if their orders are changed on xpu.", "state": "closed"}### Result:484 failed to extract### Result:483 failed to extract### Result:475 failed to extract

### Merged Result:470{"issue_number": 470, "issue_description": "RuntimeError: Double and complex datatype matmul is not supported in oneDNN\nRuntimeError: Long/Short is not supported in oneDNN!\nThe test_decomp suite is failing on XPU with the following errors:\n\n- AssertionError: Jiterator is only supported on CUDA and ROCm GPUs, none are available.\n- core dump\n- NotImplementedError: Could not run 'aten::_thnn_fused_gru_cell' with arguments from the 'CPU' backend.\n- AssertionError: False is not true : aten.mish_backward was not decomposed, saw calls for: aten.softplus.default, aten.add.Tensor, aten.empty_like.default, aten.sigmoid.default, aten.sub_.Tensor, aten.fill_.Scalar, aten.tanh.default,en.mul.Tensor. If your op is  CompositeImplicitAutograd you should skip this test by updating CROSS_REF_EXCLUDE_SET.\n- RuntimeError: NULL pointer argument in memory copy operation. -30 (PI_ERROR_INVALID_VALUE).\n- RuntimeError: device type of values (xpu) must be CPU or CUDA or Meta.\n- RuntimeError: could not create a primitive.\n- RuntimeError: could not create a primitive descriptor for a deconvolution forward propagation primitive.\n- RuntimeError: \nNULL pointer argument in memory copy operation. -30 (PI_ERROR_INVALID_VALUE) :", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nclosed\nThe issue has been closed by the reporter.\nclosed", "root_cause": "Double and complex datatype matmul is not supported in oneDNN", "state": "closed"}

### Merged Result:469{"issue_number": 469, "issue_description": "RuntimeError: ceil is not supported for complex inputs\nRuntimeError: floor is not supported for complex inputs\nRuntimeError: trunc is not supported for complex inputs\nRuntimeError: \nRuntimeError: ceil is not supported for complex inputs", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nclosed", "root_cause": "The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "state": "closed"}

### Merged Result:468{"issue_number": 468, "issue_description": "Implement interpolate_bilinear and interpolate_bicubic, cases which are skipped: 'test_dtypes_nn_functional_interpolate_bilinear_xpu', 'test_dtypes_nn_functional_interpolate_bicubic_xpu', root cause: fallback to cpu's implementation but use the dtypes claim by xpu\nall passed", "reporter": "chunhuanMeng", "assignee": "majing921201", "resolution": "closed\nall passed", "root_cause": "fallback to cpu's implementation but use the dtypes claim by xpu", "state": "closed"}

### Merged Result:464{"issue_number": 464, "issue_description": "test_fn_grad__unsafe_masked_index_xpu_complex128\n\ntest_fn_grad__unsafe_masked_index_xpu_float64\n\ntest_fn_gradgrad__unsafe_masked_index_put_accumulate_xpu_complex128\n\ntest_fn_gradgrad__unsafe_masked_index_put_accumulate_xpu_float64\n\n\nThe case failed because with the same op output, the torch.autograd.grad() cannot return exactly the same result.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nThe PR #474 is created to align with cuda.", "root_cause": "The xpu implementation logic is different from cuda's, specifically in the logic of determining whether to use deterministic algorithms.", "state": "closed"}

### Merged Result:461{"issue_number": 461, "issue_description": "Index put case fails due to no support of FP8 data types. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/461. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is open.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "FP8 is not a goal of PT2.5 or PT2.6", "state": "open"}

### Merged Result:455{"issue_number": 455, "issue_description": "Cases skip due to grid_sampler_3d is not implemented\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/455. The reporter of the issue is majing921201, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "majing921201", "assignee": "majing921201", "resolution": "\nImplemented in https://github.com/intel/torch-xpu-ops/pull/898", "root_cause": "grid_sampler_3d is not implemented", "state": "closed"}

### Merged Result:436{"issue_number": 436, "issue_description": "The following dtypes did not work in backward but are listed by the OpInfo: {torch.bfloat16} cases: TestCommonXPU.test_dtypes_view_as_complex_xpu TestCommonXPU.test_dtypes_view_as_real_xpu\nThe reporter of the issue is PenghuiCheng, and the assignee is ZhiweiYan-96, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "ZhiweiYan-96", "resolution": "closed\nThe issue is closed.", "root_cause": "The issue is related to the MKLDNN implementation of addbmm, which does not support complex and real data types. The reporter PenghuiCheng reported the issue, and the assignee ZhiweiYan-96 closed the issue. The issue is closed because the reporter and assignee agreed that the issue is related to the MKLDNN implementation and not a bug in the code.", "state": "closed"}

### Merged Result:435{"issue_number": 435, "issue_description": "Sigmoid op didn't be supported with complex32 which didn't align with CUDA behavior. Error message: RuntimeError: \"sigmoid_xpu\" not implemented for 'ComplexHalf'. Cases: TestCommonXPU.test_complex_half_reference_testing_sigmoid_xpu_complex32, TestCommonXPU.test_complex_half_reference_testing_sigmoid_xpu_complex32, TestCommonXPU.test_dtypes_sigmoid_xpu, TestCommonXPU.test_python_ref__refs_sigmoid_xpu_complex32, TestCommonXPU.test_python_ref_errors__refs_where_xpu, TestCommonXPU.test_python_ref_executor__refs_sigmoid_executor_aten_xpu_complex32, TestCommonXPU.test_python_ref_torch_fallback__refs_sigmoid_xpu_complex32\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/435. The reporter of the issue is PenghuiCheng, and the assignee is PenghuiCheng, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "\nfixed", "root_cause": "", "state": "closed"}

### Merged Result:432{"issue_number": 432, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, the assignee is mengfei25, and the state of the issue is open. Content of #432 is : This issue just for track Nightly test result and inform the result to maintainers.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/432. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.", "reporter": "chuanqi129", "assignee": "mengfei25", "resolution": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "root_cause": "", "state": "open"}

### Merged Result:427{"issue_number": 427, "issue_description": "CUDA add one kernel for performance optimization to support partial channel last case. performance enhancement is not in 2.5 scope\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/427. The reporter of the issue is majing921201, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "majing921201", "assignee": "majing921201", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:426{"issue_number": 426, "issue_description": "This issue just for track on-demand test result and inform the result to scheduler.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. This issue is reported by chuanqi129 and assigned to mengfei25. The state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/426. The reporter of the issue is chuanqi129, and the assignee is mengfei25, and the state of the issue is open.", "reporter": "chuanqi129", "assignee": "mengfei25", "resolution": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "root_cause": "The issue is related to the model dm_nfnet_f0, and the failure is caused by the change in the transformers, timm, torchbench, torchvision, and torchaudio repositories. The failure occurs on 2024-07-08 and 2024-07-09, and the test is successful on other dates. The root cause is the change in the repositories' commits, which affects the performance of the model.", "state": "open"}

### Merged Result:414{"issue_number": 414, "issue_description": "TorchBench Bf16 yolov3 fails, the accuracy is 533, the pass rate is 93.75%.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/414. The reporter of the issue is fengyuan14, and the assignee is etaf, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "etaf", "resolution": "\nThe model can pass accuracy test on currently pinned triton now.", "root_cause": "The accuracy issue is introduced by https://github.com/pytorch/pytorch/pull/128269, which triggered a triton accuracy bug.", "state": "closed"}

### Merged Result:412{"issue_number": 412, "issue_description": "We have improved and aligned the condition of device choice in test infrastructure. After that, we cannot correctly skip fine-gran cases. New failure in fine-gran cases, test_compare_cpu_abs_xpu_bool, which was skipped before the commit above.\nThe reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\nThe case is skipped in latest code", "root_cause": "", "state": "closed"}

### Merged Result:410{"issue_number": 410, "issue_description": "Inductor case expose a SegmentFault in XPU resize_as. \n\nI've created an isolate case:\n\n```\nimport torch\n# device=\"xpu\" #segmentfault\ndevice=\"cpu\" # ok\n# device=\"cuda\" ok\nx = torch.ones(1, 2, device=device)\ny = torch.ones(1, 1, 3, 2, 3, device=device)\nout = torch.ops.aten.resize_as(x, y)\nprint(\"x:\", x)\nprint(\"out size:\", out.size())\nprint(\"out\", out)\n\n# cpu result:\n# x:\n#  tensor([[1., 1.]])\n# out size:\n#  torch.Size([1, 1, 3, 2, 3])\n# out\n#  tensor([[[[ 1.0000e+00,  1.0000e+00,  0.0000e+00],\n#            [ 0.0000e+00, -2.8010e+32,  4.5898e-41]],\n# \n#           [[-5.5715e+20, -2.2881e+15, -4.8480e+32],\n#            [ 4.5898e-41, -7.0579e-22, -5.4955e-21]],\n# \n#           [[-2.8596e+32,  4.5898e-41,  5.2044e+27],\n#            [-2.9831e-31, -5.2708e+32,  4.5898e-41]]]])\n######\n# cuda result:\n# x:\n#  tensor([[1., 1.]], device='cuda:0')\n# out size:\n#  torch.Size([1, 1, 3, 2, 3])\n# out\n#  tensor([[[[1., 1., 0.],\n#            [0., 0., 0.]],\n# \n#           [[0., 0., 0.],\n#            [0., 0., 0.]],\n# \n#           [[0., 0., 0.],\n#            [0., 0., 0.]]]], device='cuda:0')\n\n```", "reporter": "etaf", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:408{"issue_number": 408, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/408. The reporter of the issue is fengyuan14, and the assignee is etaf, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/408. The reporter of the issue is fengyuan14, and the assignee is etaf, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "etaf", "resolution": "\nThis issue have been fixed in latest triton main branch.", "root_cause": "", "state": "closed"}

### Merged Result:397{"issue_number": 397, "issue_description": "UT got failed with python 3.10\nUT got failed with python 3.10, UfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_int32, UfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_int64, UfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_int8, UfuncsXPU.test_reference_numerics_small_special_spherical_bessel_j0_xpu_uint8, TestAutocastGPU.test_cast_cache_is_global\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/397. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "", "resolution": "\n\n", "root_cause": "TestAutocastGPU.test_cast_cache_is_global is caused by pytorch, 0606 nightly is fine", "state": "closed"}

### Merged Result:386{"issue_number": 386, "issue_description": "The operator has been implemented in torch-xpu-ops. Need reevaluate the skipped cases (in run_test_with_skip.py).\nhttps://github.com/intel/torch-xpu-ops/pull/371\n\n### Alternatives\n_No response_\n\n### Additional context\n_No response_\nRuntime error", "reporter": "fengyuan14", "assignee": "majing921201", "resolution": "closed\n", "root_cause": "The reporter of the issue is fengyuan14, and the assignee is majing921201, and the state of the issue is closed.", "state": "closed"}

### Merged Result:384{"issue_number": 384, "issue_description": "test_autocast_xpu::TestAutocastGPU::test_cast_cache_is_global FAILED  [ 25%] Failure in pre-ci, https://github.com/intel/torch-xpu-ops/actions/runs/9413187922/job/25929444890?pr=366, Extracted from the issue title and body. The reporter of the issue is fengyuan14, and the assignee is guangyey, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/384. The reporter of the issue is fengyuan14, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "guangyey", "resolution": "\nfixed in https://github.com/pytorch/pytorch/pull/128383", "root_cause": "", "state": "closed"}

### Merged Result:380{"issue_number": 380, "issue_description": "To retrieve the fine gran case when embedding_renorm_ added. The error message is: 'RuntimeError: operator aten::embedding_renorm_ is not implemented'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/380. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "The issue is closed\nImplemented in https://github.com/intel/torch-xpu-ops/pull/885", "root_cause": "The operator aten::embedding_renorm_ is not implemented yet.", "state": "closed"}

### Merged Result:379{"issue_number": 379, "issue_description": "Implement  op `aten::_upsample_nearest_exact3d.out` and `aten::upsample_nearest3d_backward.grad_input`, and cases: 'test_compare_cpu_nn_functional_interpolate_nearest-exact_xpu_bfloat16', 'test_compare_cpu_nn_functional_interpolate_nearest-exact_xpu_float16', 'test_compare_cpu_nn_functional_interpolate_nearest-exact_xpu_float32', 'test_compare_cpu_nn_functional_interpolate_nearest-exact_xpu_float64', 'test_compare_cpu_nn_functional_interpolate_nearest-exact_xpu_uint8', 'test_backward_nn_functional_interpolate_nearest-exact_xpu_float32', 'test_forward_ad_nn_functional_interpolate_nearest-exact_xpu_float32', 'test_operator_nn_functional_interpolate_nearest-exact_xpu_float32', 'test_view_replay_nn_functional_interpolate_nearest-exact_xpu_float32', 'test_backward_nn_functional_interpolate_nearest_xpu_float32', 'test_backward_nn_functional_upsample_nearest_xpu_float32', and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/379. The reporter of the issue is chunhuanMeng, and the assignee is chunhuanMeng, and the state of the issue is closed.", "reporter": "chunhuanMeng", "assignee": "chunhuanMeng", "resolution": "\nWith this pr, cases above all passed\uff0chttps://github.com/intel/torch-xpu-ops/pull/869", "root_cause": "", "state": "closed"}

### Merged Result:375{"issue_number": 375, "issue_description": "The following code some times hung and sometimes get Native API failed. Native API returns: -2 (PI_ERROR_DEVICE_NOT_AVAILABLE) -2\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/375. The reporter of the issue is etaf, and the assignee is ZhiweiYan-96, and the state of the issue is closed.", "reporter": "etaf", "assignee": "ZhiweiYan-96", "resolution": "\nClose as this no longer exists", "root_cause": "should be issue in oneDNN, can reproduce error with benchdnn", "state": "closed"}

### Merged Result:372{"issue_number": 372, "issue_description": "These `nil_loss2d_*` ops are not currently implemented for XPU backend and are not marked for \"explicit\" CPU fallback. This means that running model with any of these ops will fail at runtime by default (with not implemented error) and will require to set `PYTORCH_DEBUG_XPU_FALLBACK=1`.\n\n- [x] `aten::nll_loss2d_backward`\n- [x] `aten::nll_loss2d_forward`\n\nThis issue affects XPU enabling for Huggingface - https://github.com/huggingface/transformers/issues/31237#issuecomment-2148067845. See table in this comment for the list of affected examples and models.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/372. The reporter of the issue is dvrogozh, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "dvrogozh", "assignee": "fengyuan14", "resolution": "\nThe issue is resolved and closed", "root_cause": "", "state": "closed"}

### Merged Result:367{"issue_number": 367, "issue_description": "11 tests failed in the UT with the latest driver 803.58. The tests failed in the following categories: Finegrain, XPU OP UT, TestCommonXPU.test_compare_cpu__refs_rsub_xpu_float16, TestCommonXPU.test_noncontiguous_samples_pca_lowrank_xpu_complex64, TestCommonXPU.test_noncontiguous_samples_svd_lowrank_xpu_complex64, TestCommonXPU.test_variant_consistency_eager_pca_lowrank_xpu_complex64, TestCommonXPU.test_variant_consistency_eager_svd_lowrank_xpu_complex64, TestMathBitsXPU.test_conj_view_pca_lowrank_xpu_complex64, TestMathBitsXPU.test_conj_view_svd_lowrank_xpu_complex64, TestModuleXPU.test_to_nn_GRUCell_swap_True_set_grad_False_xpu_float32, TestModuleXPU.test_to_nn_GRU_eval_mode_swap_True_set_grad_False_xpu_float32, TestModuleXPU.test_to_nn_GRU_train_mode_swap_True_set_grad_False_xpu_float32, TestAutograd.test_multi_grad_all_hooks. The issue is closed and the assignee is daisyden.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/367. The reporter of the issue is mengfei25, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "mengfei25", "assignee": "daisyden", "resolution": "\nclosed", "root_cause": "", "state": "closed"}

### Merged Result:365{"issue_number": 365, "issue_description": "When running the training script for model resnet50 with AMP FP16 and GradScaler, the error NotImplementedError for op 'aten::_amp_foreach_non_finite_check_and_unscale_' occurs. The error message is as follows: NotImplementedError: This function is not implemented for the current dtype. The reporter of the issue is ZhaoqiongZ, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "ZhaoqiongZ", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:363{"issue_number": 363, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/363. The reporter of the issue is fengyuan14, and the assignee is yuchengliu1, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/363. The reporter of the issue is fengyuan14, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "yuchengliu1", "resolution": "\nenable in https://github.com/intel/torch-xpu-ops/pull/571", "root_cause": "", "state": "closed"}

### Merged Result:358{"issue_number": 358, "issue_description": "The operator 'aten::_foreach_mul_.Scalar' is not currently implemented for the XPU device. Fallback to CPU can help on running the training process, but will have a performance implications.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/358. The reporter of the issue is ZhaoqiongZ, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "ZhaoqiongZ", "assignee": "fengyuan14", "resolution": "The issue is closed.\nNot included", "root_cause": "The operator 'aten::_foreach_mul_.Scalar' is not currently implemented for the XPU device.", "state": "closed"}

### Merged Result:357{"issue_number": 357, "issue_description": "The reporter of the issue is yuchengliu1, and the assignee is majing921201, and the state of the issue is closed. The error message is NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend. The error message also includes RuntimeError: device type of values (xpu) must be CPU or CUDA or Meta. The issue is related to the sparse tensor operations on XPU backend. The issue is closed and the root cause is not provided.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/357. The reporter of the issue is yuchengliu1, and the assignee is majing921201, and the state of the issue is closed. The error message is not provided in the issue title and issue body.\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/357. The reporter of the issue is yuchengliu1, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "majing921201", "resolution": "closed\n\n\nclosed", "root_cause": "The reporter reported that the operator 'aten::_sparse_coo_tensor_with_dims_and_tensors' has been supported, but the cases failed in other places. The root cause is that the operator is only available for certain backends and the backend used in the test cases is not one of the supported backends.", "state": "closed"}

### Merged Result:348{"issue_number": 348, "issue_description": "1.  torch.backends.mkldnn.flags() got an unexpected keyword argument 'deterministic'---fixed\n2. Tensor-likes are not close! -----fixed\n3. check_random_bounds handles only integral, floating-point and boolean types---fixed\n4. largeTensorTest didn't support XPU device ---fixed\n5. NotImplementedError: The operators \nThe issue is about the convolution operation in torch-xpu-ops, which is not working as expected. The reporter PenghuiCheng has reported that the first four issues have been fixed, but the fifth issue is not in the 2.5 plan. The reporter also mentioned that there is a PR for convolution ut, but it is not merged yet.", "reporter": "PenghuiCheng", "assignee": "ZhiweiYan-96", "resolution": "fixed\n", "root_cause": "The fifth issue is not in the 2.5 plan and there is a PR for convolution ut, but it is not merged yet.", "state": "open"}

### Merged Result:342{"issue_number": 342, "issue_description": "The test_multihead_attention test case fails because the _check_arg_device function does not support xpu. The error message is: 'RuntimeError: expected scalar type Float but found Long'.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/342. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "The issue has been fixed in the latest version of the code.\nclosed", "root_cause": "The _check_arg_device function does not have xpu support and the input tensor is of type Long instead of Float.", "state": "closed"}

### Merged Result:339{"issue_number": 339, "issue_description": "The test_packed_sequence test is failing with the following error message: 'RuntimeError: expected scalar type Float but found Double'. This issue is related to the issue reported in #339. The reporter of the issue is daisyden, and the assignee is daisyden. The state of the issue is closed.\nnn/utils/rnn.py PackedSequence() need to add an xpu() function.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "Device guard is correctly used. Will check backend.", "state": "closed"}

### Merged Result:327{"issue_number": 327, "issue_description": "CPU/CUDA bias code in aten::mode_out. To upstream to make the operator device compatible. \n\n_No response_, \n_No response_, \n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/327. \nThe reporter of the issue is fengyuan14, \nThe assignee is fengyuan14, \nThe state of the issue is closed.\nmode only supports CPU AND CUDA device type, got: xpu", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nThe PR is merged", "root_cause": "The issue is related to the PR https://github.com/pytorch/pytorch/pull/137575", "state": "closed"}

### Merged Result:325{"issue_number": 325, "issue_description": "Double and complex datatype matmul is not supported in oneDNN\nWhile considering the real part of complex inputs only, Jacobian computed with forward mode mismatch for output 0 with respect to input 0, \nRuntimeError: could not create a primitive descriptor for a deconvolution forward propagation primitive \nRuntimeError: input tensor must have at least one element, but got input_sizes = [1, 0, 1] \nRuntimeError: value cannot be converted to type float without overflow\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/325. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed. The issue title is [To Evaluate] Issues in test_ops_fwd_gradients, and the issue body is pu_complex128. The error message is RuntimeError: input tensor must have at least one element, but got input_sizes = [1, 0, 1] for test_fn_fwgrad_bwgrad_nn_functional_group_norm_xpu_float64, RuntimeError: DispatchStub: unsupported device typexpu for test_inplace_forward_mode_AD_conj_physical_xpu_complex128.\nRuntimeError: DispatchStub: unsupported device type xpu", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nclosed\n\nclosed", "root_cause": "Double and complex datatype matmul is not supported in oneDNN", "state": "closed"}

### Merged Result:322{"issue_number": 322, "issue_description": "Some tests in test_matmul_cuda.py are skipped due to FP8 support. FP8 is only supported on H100+ and sm_89 and MI300+ devices. Mixed dtypes linear is only supported on SM 8.x devices. The tests skipped are: TestFP8MatmulCudaXPU::test_float32_output_errors_with_bias_xpu, TestFP8MatmulCudaXPU::test_float8_basics_xpu, TestFP8MatmulCudaXPU::test_float8_bias_relu_edgecase_xpu, TestFP8MatmulCudaXPU::test_float8_bias_xpu, TestFP8MatmulCudaXPU::test_float8_scale_fast_accum_xpu, TestFP8MatmulCudaXPU::test_float8_scale_xpu, TestFP8MatmulCudaXPU::test_non_divisible_leading_dim_bias_False_xpu, TestFP8MatmulCudaXPU::test_non_divisible_leading_dim_bias_True_xpu, TestFP8MatmulCudaXPU::test_scaled_mm_vs_emulated_bfloat16_xpu, TestFP8MatmulCudaXPU::test_scaled_mm_vs_emulated_float16_xpu, TestFP8MatmulCudaXPU::test_scaled_mm_vs_emulated_float32_xpu, TestMixedDtypesLinearCudaXPU::test_mixed_dtypes_linear_xpu_bfloat16, TestMixedDtypesLinearCudaXPU::test_mixed_dtypes_linear_xpu_float16.\nThe test has been rework. The last failures are as below:\n# AssertionError: ", "reporter": "yuchengliu1", "assignee": "fengyuan14", "resolution": "\nLow priority", "root_cause": "FP8 support", "state": "open"}

### Merged Result:320{"issue_number": 320, "issue_description": "This issue is related to the masked_UT operations in the torch-xpu-ops library. The error message indicates that the 'aten::_sparse_coo_tensor_with_dims_and_tensors' operator is not available for the 'SparseXPU' backend. The error occurs during the evaluation of several test cases, which involve masked operations on sparse tensors. The error message also mentions that the device type of the values must be CPU, CUDA, or Meta, which is not the case for the XPU backend. The issue is marked as closed, and the root cause is not explicitly provided, but it seems to be related to the backend support for certain operations.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/320. The reporter of the issue is yuchengliu1, and the assignee is majing921201, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "majing921201", "resolution": "The issue is closed, and no resolution details are provided.\n", "root_cause": "The backend support for certain sparse tensor operations is missing or not fully implemented for the XPU backend.", "state": "closed"}

### Merged Result:317{"issue_number": 317, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/317. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.\nThe issue is that the addr function does not support xpu for complex128, complex64, float16, float32, float64, int16, int32, int64, int8, uint8 data types. The reporter of the issue is yuchengliu1, and the assignee is yuchengliu1, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "yuchengliu1", "resolution": "\nThe issue is resolved and the new issue is https://github.com/intel/torch-xpu-ops/issues/577", "root_cause": "The issue is that the addr function does not support xpu for complex128, complex64, float16, float32, float64, int16, int32, int64, int8, uint8 data types.", "state": "closed"}

### Merged Result:304{"issue_number": 304, "issue_description": "This issue will be auto-comment by actions when nightly failure detected for notify relevant owners awareness.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/304. The reporter of the issue is mengfei25, and the assignee is , and the state of the issue is closed.\nXPU OPS Nightly Successful 2024-06-18, See: https://github.com/intel/torch-xpu-ops/actions/runs/9561980938", "reporter": "mengfei25", "assignee": "", "resolution": "\n\nclosed", "root_cause": "https://github.com/intel/torch-xpu-ops/issues/304", "state": "closed"}

### Merged Result:302{"issue_number": 302, "issue_description": "1. To enable memory check in test framework, we need to have the counterpart of the two cuda functions:\ntorch.cuda.memory_allocated()\ntorch.cuda.mem_get_info()\n2. To enable CudaSyncGuard in test framework, we depend on the counterpart of\ntorch.cuda.set_sync_debug_mode\n3. To enable largeTensorTest in test framework, we depend on the counterpart of\ntorch.cuda.memory.mem_get_info\n4. To run test_storage_meta_errors() need torch.TypedStorage.xpu support\n5. To run test_dtypetensor_warnings need the counterpart of torch.cuda.FloatTensor and torch.cuda.DoubleTensor support in xpu backend.\n6. float() need to have an is_xpu() interface, test_broadcast()\n7. xpu is missed in torch.backends, see https://pytorch.org/docs/stable/backends.html. So that we cannot write a counterpart of this:\n@unittest.skipIf(torch.backends.cuda.is_built() or IS_SANDCASTLE, \"CUDA is built, can't test CUDA not built error\")\n8 AttributeError: module 'torch.xpu' has no attribute 'FloatTensor'\n9 Error message related\nAssertionError: RuntimeError not raised by <lambda>\n10, floating point exception in index_add, test_dim_function_empty_xpu\n11. AssertionError: Scalars are not equal!\n12. AssertionError: 'XPU.used\t\t true' not found in '[TORCH_VITAL] Dataloader.enabled\t\t True\n[TORCH_VITAL] Dataloader.basic_unit_test\t\t TEST_VALUE_STRING\n[TORCH_VITAL] CUDA.used\t\t False\n'\n13. AttributeError: module 'torch.xpu' has no attribute 'amp'\n14. NotImplementedError: Could not run 'aten::_copy_from_and_resize' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_copy_from_and_resize' is only available for these backends: [XPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher'.\n15. AssertionError: Tensor-likes are not close!\n16. RuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.\n17. AssertionError: False is not true\n18. RuntimeError: Expected a 'cpu' device type for generator but found 'xpu'\n19. RuntimeError: _share_fd_: only available on CPU\n20. RuntimeError: Expected a 'cpu' device type for generator but found 'xpu'\n21. RuntimeError: _share_fd_: only available on CPU\n22. AssertionError: \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/302. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n", "root_cause": "The TensorInfo will cause the error: https://github.com/intel/torch-xpu-ops/blob/main/src/comm/TensorInfo.h#L193. The issue is caused by the assumption of int64 in the template of getTensorInfo when index dtype is int32.", "state": "closed"}

### Merged Result:296{"issue_number": 296, "issue_description": "1. https://github.com/intel/torch-xpu-ops/issues/261 - closed\n2. Tensor isn't pinned with DataLoader(..., pin_memory=True,..)\ncase: TestDataLoader.test_sequential_pin_memory\ncommand:\n```python\nPYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test/xpu/test_dataloader_xpu.py\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/296. The reporter of the issue is PenghuiCheng, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "guangyey", "resolution": "\nClosed as it has been improved.", "root_cause": "", "state": "closed"}

### Merged Result:294{"issue_number": 294, "issue_description": "### \ud83d\udc1b Describe the bug\n\n1. RuntimeError: Unsupported memory formatPreserve\n\n2. NotImplementedError: Could not run 'aten::_empty_affine_quantized' with arguments from the 'QuantizedXPU' backend.\ntest_memory_format_resize_as_xpu is fixed by remove the xpu dispatch of resize_as_. Preci is WIP.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\nremove the xpu dispatch of resize_as_", "root_cause": "Not specified", "state": "closed"}

### Merged Result:281{"issue_number": 281, "issue_description": "XPU Tensor fails in copy-on-write cases, Assertion error: False is not true : Keyword argument 'output grad 0' during backward call unexpectedly materializes. Either set `supports_cow_input_no_materialize_backward=False` in this operation's OpInfo, add the arg to the OpInfo's `allow_cow_input_materialize_backward` list, or change the implementation to avoid materialization.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/281. The reporter of the issue is fengyuan14, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "guangyey", "resolution": "The issue is closed.\n", "root_cause": "The reporter of the issue is fengyuan14, and the assignee is guangyey, and the state of the issue is closed.", "state": "closed"}

### Merged Result:280{"issue_number": 280, "issue_description": "AssertionError: Jiterator is only supported on CUDA and ROCm GPUs, none are available.\nAssertionError: Scalars are not equal!\nAssertionError: Tensor-likes are not close!\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/280. The reporter of the issue is yuchengliu1, and the assignee is xytintel, and the state of the issue is closed. The issue title is [To Evaluate] Issues in test_unary_ufuncs.py, and the issue body remal_exp2_xpu_complex128, test_reference_numerics_extremal_exp2_xpu_complex64, test_reference_numerics_extremal_exp_xpu_complex128, test_reference_numerics_extremal_exp_xpu_complex64, test_reference_numerics_extremal_log_xpu_complex64, test_reference_numerics_extremal_nn_functional_tanhshrink_xpu_complex64, test_reference_numerics_extremal_tanh_xpu_complex128, test_reference_numerics_extremal_tanh_xpu_complex64, test_reference_numerics_large__refs_atan_xpu_complex128, test_reference_numerics_large__refs_atan_xpu_complex64, test_reference_numerics_large_atan_xpu_complex128, test_reference_numerics_large_atan_xpu_complex64, test_reference_numerics_normal__refs_nn_functional_tanhshrink_xpu_complex64, test_reference_numerics_normal_nn_functional_tanhshrink_xpu_complex64, test_reference_numerics_small__refs_atan_xpu_complex128, test_reference_numerics_small__refs_atan_xpu_complex64, test_reference_numerics_small_atan_xpu_complex128, test_reference_numerics_small_atan_xpu_complex64\nAssertionError: Scalars are not equal!", "reporter": "yuchengliu1", "assignee": "xytintel", "resolution": "\n\nclosed", "root_cause": "AssertionError: Scalars are not equal!", "state": "closed"}### Result:275 failed to extract### Result:271 failed to extract

### Merged Result:267{"issue_number": 267, "issue_description": "Log:\n```python\nFile \"/home/gta/penghuic/pytorch_stock/third_party/torch-xpu-ops/test/xpu/../../../../test/test_content_store.py\", line 34, in test_basic\n    writer.write_tensor(\"x\", x)\nFile \"/home/gta/penghuic/pytorch_stock/torch/utils/_content_store.py\", line 183, in write_tensor\n    h = self.write_storage(storage)\nFile \"/home/gta/penghuic/pytorch_stock/torch/utils/_content_store.py\", line 165, in write_storage\n    torch.save(storage, target)\nFile \"/home/gta/penghuic/pytorch_stock/torch/serialization.py\", line 670, in save\n    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\nFile \"/home/gta/penghuic/pytorch_stock/torch/serialization.py\", line 882, in _save\n    pickler.dump(obj)\nFile \"/home/gta/penghuic/pytorch_stock/torch/serialization.py\", line 867, in persistent_id\n    location = location_tag(storage)\nFile \"/home/gta/penghuic/pytorch_stock/torch/serialization.py\", line 426, in location_tag\n    raise RuntimeError(\"don't know how to determine data location of \")\nRuntimeError: don't know how to determine data location of torch.storage.UntypedStorage\nCases:\ntest_content_store_xpu.py::TestContentStoreXPU::test_basic_xpu\ntest_content_store_xpu.py::TestContentStoreXPU::test_load_tensor_xpu\nCommand:\nPYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test/xpu/test_content_store_xpu.py\n\nThere is no failure involving existing XPU ops.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "\nMove to 2.5", "root_cause": "The error is caused by the use of UntypedStorage in torch.storage, which is not supported by the current implementation of torch.save. The reporter has reported this issue to the torch-xpu-ops repository, and the issue is closed.", "state": "closed"}

### Merged Result:264{"issue_number": 264, "issue_description": "torch.random.fork_rng(devices=rng_device) does not support XPU backend\nTestRandomTensorCreationXPU.test_randperm_xpu\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/3ffa121d-749c-4dc8-822d-da9fb088b546) - PASSED\n\ntorch.xpu.FloatTensor is not supported. so tests like test_tensor_factory_gpu_type_inference and test_constructor_dtype had to be disabled. - PASSED\n\nmultiple device seems not supported, when I used ZE_AFFINITY_MASK to use tile 0 and tile 1, seems still only xpu:0 is detected.\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/ad1e0bd9-823f-45f1-90b1-ce833ae6443e)\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/3b3f7b8e-86d2-4ff8-8214-0b4e05bbf52b)\nThis makes test_copy_from_tensor_mult_devices and test_copy_from_dlpack_mult_devices cannot work.\n\nRuntimeError: \nThe reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open. The error message is 'RuntimeError: \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/264. The reporter of the issue is daisyden, and the assignee is daisyden, and the state of the issue is open.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "\n\n", "root_cause": "test_kaiser_window_xpu: DispatchStub support. It is a composite operator. But it is implemented by DispatchStub. XPU doesn't support DispatchStub.", "state": "open"}

### Merged Result:262{"issue_number": 262, "issue_description": "Provide a way to debug explicit CPU fallback, the commit 5bf9e0cc768f7a3b13d829118683275f324399f1 muted debug logs of 'explicit' CPU fallbacks. This complicated debug for 3d party contributors trying to evaluate XPU backend capabilities - now I am forced to revert noted commit to understand which operations are not currently implemented by XPU. Please:\n1. Explain what 'explicit CPU fallback' means - this seems to be internal to xpu team classification which is unclear and confusing\n2. Extend PYTORCH_DEBUG_XPU_FALLBACK=1 to track any CPU fallback happening in XPU backend. Note: I am fine if 'explicit' fallback will be muted by default, but I really need a way to be able to track it.\n\n```commit 5bf9e0cc768f7a3b13d829118683275f324399f1 (origin/meng_max_2d)\nAuthor: Feng Yuan <feng1.yuan@intel.com>\nDate:   Mon Apr 29 13:05:51 2024 +0800\n\n    Register operator's implementation lazily. (#177)\n\n    1. Avoid dangling operator's implementation (m.impl(torchvision::nms) is\n    ahead of `import torchvision` sometime)\n    2. Mute debug log of explicit CPU fallback.\n    3. Add torchvision.roi_align/_roi_align_backward example case\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/262. The reporter of the issue is dvrogozh, and the assignee is , and the state of the issue is closed.", "reporter": "dvrogozh", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}### Result:261 failed to extract

### Merged Result:259{"issue_number": 259, "issue_description": "Accuracy issue in TestDropoutNNDeviceTypeXPU.test_Dropout1d_xpu and TestDropoutNNDeviceTypeXPU.test_Dropout3d_xpu\nThe freeze_rng_state function does not work on xpu.", "reporter": "daisyden", "assignee": "daisyden", "resolution": "closed\nThe freeze_rng_state function does not work on xpu.", "root_cause": "freeze_rng_state does not work on xpu.", "state": "closed"}

### Merged Result:258{"issue_number": 258, "issue_description": "oneDNN issues // Please check `run_test_with_skip.py`\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/258. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:256{"issue_number": 256, "issue_description": "test_cpu_gpu_parity_nn_CrossEntropyLoss_xpu_float16 // Should be open when nll_loss2d is enabled.\noneDNN failures // Please check `run_test_with_skip`\nLack of aten::_thnn_fused_gru_cell // Need evaluate whether operator is needed.\nCPU fallback failures // To evaluate when there is an XPU implementation.\ntest_cpu_gpu_parity_nn_CrossEntropyLoss_xpu_float16 // Should be open when nll_loss2d is enabled.", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:254{"issue_number": 254, "issue_description": "RuntimeError: Double and complex datatype matmul is not supported in oneDNN\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/254. The reporter of the issue is daisyden, and the assignee is riverliuintel, and the state of the issue is closed. The error message is RuntimeError: Double and complex datatype matmul is not supported in oneDNN.\nTracked in #253", "reporter": "daisyden", "assignee": "riverliuintel", "resolution": "\nThe issue is closed.\ntesting", "root_cause": "The reporter reported that double and complex datatype matmul is not supported in oneDNN.", "state": "closed"}### Result:253 failed to extract

### Merged Result:249{"issue_number": 249, "issue_description": "1. TestMathBitsXPU , RuntimeError: DispatchStub: unsupported device type expu - Fixed\n\n2. TestMathBitsXPU, RuntimeError: input tensor must have at least one element, but got input_sizes = [1, 0, 1] - 2.6\n\n3. accuracy issue\n\n4. RuntimeError: value cannot be converted to type float without overflow\n\nThe issue is related to the conversion of values to float without overflow, the unsupported matmul operation for double and complex data types in oneDNN, and the failure to create a primitive descriptor for a deconvolution forward propagation primitive.", "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": "Fixed\nThe issue has been closed by the assignee.", "root_cause": "In aten/src/ATen/native/DispatchStub.cpp function DispatchStubImpl::get_call_ptr() need to add a XPU path.", "state": "closed"}

### Merged Result:248{"issue_number": 248, "issue_description": "TestMathBitsXPU has 2 cases with RuntimeError: value cannot be converted to type float without overflow\n\n\"test_conj_view_addbmm_xpu_complex64\",\n\"test_neg_conj_view_addbmm_xpu_complex128\",\n\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/37828ad7-b3bc-44c8-938a-a56b1c9add3b)\n\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/248. The reporter of the issue is daisyden, and the assignee is , and the state of the issue is closed.", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:246{"issue_number": 246, "issue_description": "### \ud83d\udc1b Describe the bug\n1. RuntimeError: PyTorch was compiled without CUDA support\n2. module 'torch._C' has no attribute '_scatter'\n3. AttributeError: module 'torch.xpu' has no attribute\n4. NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend.\n5. c10::NotImplementedError\n6. segment fault\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/246. The reporter of the issue is PenghuiCheng, and the assignee is PenghuiCheng, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "fixed\nEvaluated. There is no failure exposing issues of existing XPU ops. Move to 2.5.", "root_cause": "PyTorch was compiled without CUDA support", "state": "closed"}

### Merged Result:245{"issue_number": 245, "issue_description": "module 'torch._C' has no attribute '_scatter'", "reporter": "PenghuiCheng", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:244{"issue_number": 244, "issue_description": "AttributeError: module 'torch.xpu' has no attribute", "reporter": "PenghuiCheng", "assignee": "", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:242{"issue_number": 242, "issue_description": "TestCompositeComplianceXPU.test_view_replay_to_sparse_xpu_float32\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/5d72a704-655a-4385-a4f1-e38124e4fdd8)\n\n3 cases in TestMathBitsXPU have sparse op issue", "reporter": "daisyden", "assignee": "", "resolution": "\nThe issue is tracked in #240", "root_cause": "The operator 'aten::_sparse_coo_tensor_with_dims_and_tensors' is not available for the 'SparseXPU' backend", "state": "closed"}

### Merged Result:241{"issue_number": 241, "issue_description": "RuntimeError in nn_functional* ops op  creation\nThe first issue is tracked in #253 \nThe 2nd issue is tracked in #249", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:240{"issue_number": 240, "issue_description": "1. AssertionError: Jiterator is only supported on CUDA and ROCm GPUs, none are available.\nTestCompositeComplianceXPU.test_view_replay_jiterator_unary_xpu_float32\n2. More than 400 cases in TestCompositeComplianceXPU.test_cow_input got these errors:\n3. sparse is not supported\nTestCompositeComplianceXPU.test_view_replay_to_sparse_xpu_float32\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/240. The reporter of the issue is daisyden, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nEnable the suite in fine gran cases. Evaluated.", "root_cause": "", "state": "closed"}

### Merged Result:239{"issue_number": 239, "issue_description": "TestCompositeComplianceXPU.test_view_replay_addbmm_xpu_float32\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/c9e23d64-05ee-4a5c-90b5-09410cf4033f)\n\nTestCompositeComplianceXPU.test_view_replay_addmm_xpu_float32\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/131fbd64-9b3e-41a6-9639-5fbaafe90e4a)\n\nTestCompositeComplianceXPU.test_view_replay_addmv_xpu_float32\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/e8ceca2d-e51a-4706-8b8a-65cfcc5d8fae)\n\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/b26388a3-183e-41cf-abc4-552edd1819ed)\n\ntracked in #253", "reporter": "daisyden", "assignee": "", "resolution": "\ntracked in #253", "root_cause": "", "state": "closed"}

### Merged Result:238{"issue_number": 238, "issue_description": "More than 400 cases in TestCompositeComplianceXPU.test_cow_input got these errors:\n\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/3e3a7496-cb09-436d-85d5-67e4e5efbd83)\n\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/59a30297-919d-4198-81b4-c6cdf6ec2402)\n\n![image](https://github.com/intel/torch-xpu-ops/assets/27668899/1216855f-a2c7-429d-bebf-51213bea8ee7)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/238. The reporter of the issue is daisyden, and the assignee is , and the state of the issue is closed.", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:237{"issue_number": 237, "issue_description": "RuntimeError: could not create a primitive descriptor for a deconvolution forward propagation primitive, timeout 10000 python run_test_with_skip.py 2>&1|tee TestCompositeCompliance_test_forward_ad.log\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/237. The reporter of the issue is daisyden, and the assignee is , and the state of the issue is closed.", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:236{"issue_number": 236, "issue_description": "TestCompositeComplianceXPU.test_forward_ad_nn_functional_rrelu_xpu_float32 and TestCompositeComplianceXPU.test_forward_ad_nn_functional_max_unpool1d_xpu_float32 have accuracy issues. The error message is not provided in the issue title, but it is mentioned in the issue body.\nTracked in #233", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:235{"issue_number": 235, "issue_description": "RuntimeError: could not create a primitive in test_forward_ad, TestCompositeComplianceXPU.test_forward_ad_addmm_xpu_float32, TestCompositeComplianceXPU.test_forward_ad_addbmm_xpu_float32, TestCompositeComplianceXPU.test_forward_ad_addmv_xpu_float32\nTracked in #253", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:234{"issue_number": 234, "issue_description": "NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend.\nThe reporter of the issue is PenghuiCheng, and the assignee is , and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "", "resolution": "\n", "root_cause": "According to our priority, before PyTorch 2.5, we will support Sparse operators on-demand. If the operator is required by our prioritized operator list (3 benchmarks + MPS), we will implement it, or will deprioritized it. We can skip it in unit test first.", "state": "closed"}

### Merged Result:233{"issue_number": 233, "issue_description": "TestCompositeComplianceXPU.test_forward_ad_nn_functional_rrelu_xpu_float32\nRuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.\nTestCompositeComplianceXPU.test_forward_ad_nn_functional_max_unpool1d_xpu_float32\nRuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.\nTestCompositeComplianceXPU.test_backward_var_mean_xpu_float32\nTestCompositeComplianceXPU.test_backward_var_mean_unbiased_xpu_float32\n20 cases in test_operator, for example: TestCompositeComplianceXPU.test_operator_expand_as_xpu_float32\nRuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.\nTestCompositeComplianceXPU.test_backward_fft_ihfft2_xpu_float32\nTestCompositeComplianceXPU.test_backward_lu_unpack_xpu_float32\nTestCompositeComplianceXPU.test_backward_nn_functional_max_unpool1d_xpu_float32\nTestCompositeComplianceXPU.test_backward_t_xpu_float32\n# 100 cases in test_operator\nTestCompositeComplianceXPU.test_operator_vstack_xpu_float32\nTestCompositeComplianceXPU.test_operator_view_as_xpu_float32c\n... \nRuntimeError: NULL pointer argument in memory copy operation. -30 (PI_ERROR_INVALID_VALUE)\nPYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test_ops_xpu.py -k 'test_backward_t_xpu_float32'\nGot this error:\nRuntimeError: NULL pointer argument in memory copy operation. -30 (PI_ERROR_INVALID_VALUE)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/233. The reporter of the issue is daisyden, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "guangyey", "resolution": "\n", "root_cause": "The reporter of the issue is daisyden, and the assignee is guangyey, and the state of the issue is closed.", "state": "closed"}

### Merged Result:232{"issue_number": 232, "issue_description": "PYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test_ops_xpu.py -k 'test_backward_diagonal_xpu_float32'\nThe reporter of the issue is daisyden, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "daisyden", "assignee": "fengyuan14", "resolution": "\nEnable cases in fine gran cases. There is no segfault there with bugfixing. Close the issue.", "root_cause": "In current XPU implementation, CPU fallback will copy the XPU tensor with an invalid address to CPU, which causes the segfault. The bugfixing is to support these view operators on XPU.", "state": "closed"}

### Merged Result:231{"issue_number": 231, "issue_description": "The test case 'TestAutogradMultipleDispatchXPU::test_autograd_composite_implicit_and_dispatch_registration_xpu' and 'TestAutogradMultipleDispatchXPU::test_autograd_multiple_dispatch_registrations_xpu' are failing with the error 'c10::NotImplementedError: Not implemented for the current device type: xpu'. This error occurs because the current implementation of the test case does not support the xpu device type. The root cause is that the test case is not properly handling the xpu device type and the implementation is not fully compatible with the xpu device type. The resolution is to update the test case to properly handle the xpu device type and ensure that the implementation is fully compatible with the xpu device type.", "reporter": "PenghuiCheng", "assignee": "fengyuan14", "resolution": "Update the test case to properly handle the xpu device type and ensure that the implementation is fully compatible with the xpu device type.", "root_cause": "The test case is not properly handling the xpu device type and the implementation is not fully compatible with the xpu device type.", "state": "closed"}

### Merged Result:230{"issue_number": 230, "issue_description": "segment fault for UT case TestAutogradDeviceTypeXPU::test_resize_version_bump_xpu, the error message is: 'Segmentation fault (core dumped)'", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:229{"issue_number": 229, "issue_description": "RuntimeError: NULL pointer argument in memory copy operation. -30 (PI_ERROR_INVALID_VALUE)\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/229. The reporter of the issue is daisyden, and the assignee is , and the state of the issue is closed.", "reporter": "daisyden", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:228{"issue_number": 228, "issue_description": "NotImplementedError: elapsed_time is not supported by XPUEvent\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/228. The reporter of the issue is etaf, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "etaf", "assignee": "guangyey", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:227{"issue_number": 227, "issue_description": "UT case <test_comprehensive_nn_functional_nll_loss_xpu_float16> fail because of cpu's nll_loss2d backward. We should try this ut when we implement xpu nll_loss2d op.\nThe issue is accuracy issue. The case test_comprehensive_nn_functional_nll_loss_xpu_float16 failed in calling nll_loss2d_backward. xpu result is: 0.04122925, cuda and cpu is: 0.04119873, atol is 1e-7. We have two findings: 1) the atol of nll_loss2d_forward is just 1e-2 as below: (torch.float16, torch.ops.aten.nll_loss2d_forward.default): 1e-2, so we think the atol of backward is too high, maybe we can enlarge the atol value of nll_loss2d_backward like nll_loss2d_forward. 2) If we use compile option O0 instead of O3, xpu and cuda can the same precision, that is 0.04119873. We know that there are some optimization bugs when the xpu compiler processes +-*/ of float16 and bfloat16, I tried to use volatile in nll_loss2d_backward kernel, but it had no effect and I still could not get the same precision as cuda.", "reporter": "chunhuanMeng", "assignee": "huaiyuzh", "resolution": "closed\n", "root_cause": "The issue is caused by a bug in the CPU's nll_loss2d backward function, which is not compatible with the XPU version. The fix is to update the CPU's nll_loss2d backward function to be compatible with the XPU version.", "state": "closed"}

### Merged Result:223{"issue_number": 223, "issue_description": "test/xpu/test_autocast_xpu.py::TestAutocastGPU::test_cache_disabled FAILED [ 20%]\n  File \"/home/gta/penghuic/pytorch_stock/third_party/torch-xpu-ops/test/xpu/test_autocast_xpu.py\", line 90, in test_cache_disabled\n    torch._C._set_cached_tensors_enabled(False)\nAttributeError: module 'torch._C' has no attribute '_set_cached_tensors_enabled'\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/223. The reporter of the issue is PenghuiCheng, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "\nThe issue is closed.", "root_cause": "The issue is closed.", "state": "closed"}

### Merged Result:222{"issue_number": 222, "issue_description": "There are three types of errors in the test_reductions_xpu.py test cases: 1. There is an error in the results: skipped on cuda device, so we also skipped it. 2. RuntimeError: mode only supports CPU AND CUDA device type, got: xpu. 3. largeTensorTest will raise an \nThe reporter of the issue is PenghuiCheng, and the assignee is PenghuiCheng, and the state of the issue is closed.", "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": "\n", "root_cause": "The error of test_ref_extremal_values_mean_xpu_complex64 is due to the different treatment of nan and inf by xpu and cpu. xpu treats nan and inf in the same way as cuda, which is why cuda skips this test case. The error of test_ref_small_input_masked_prod_xpu_float16 is due to the accumulation of errors caused by cumulative multiplication. The accuracy requirement for this sample in cuda's test is lower, so xpu also reduces the accuracy requirement.", "state": "closed"}

### Merged Result:221{"issue_number": 221, "issue_description": "enable mode supports XPU device\n\nRuntimeError: mode only supports CPU AND CUDA device type, got: xpu\n\ncommand:\n\n\u00b7\u00b7\u00b7python\nfrom torch.testing._internal.common_device_type import instantiate_device_type_tests\nfrom torch.testing._internal.common_utils import run_tests\n\ntry:\n    from xpu_test_utils import XPUPatchForImport\nexcept Exception as e:\n    from .xpu_test_utils import XPUPatchForImport\n\nwith XPUPatchForImport(False):\n    from test_reductions import TestReductions\n\ninstantiate_device_type_tests(TestReductions, globals(), only_for=\"xpu\")\n", "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:220{"issue_number": 220, "issue_description": "We added this unit test for XPU device as below code, but the test `TestIndexingXPU` will raise a core dump\n`torch-xpu-ops/src/aten/sycl/Indexing.h:615: operator(): global id: [19,0,0], local id: [19,0,0] Assertion \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/220. The reporter of the issue is yuchengliu1, and the assignee is daisyden, and the state of the issue is closed.", "reporter": "yuchengliu1", "assignee": "daisyden", "resolution": "\nclosed", "root_cause": "The issue is caused by the global id and local id in the XPU device, which is not compatible with the current implementation of the Indexing.h file. The current implementation of the Indexing.h file does not handle the case where the global id and local id are out of bounds.", "state": "closed"}

### Merged Result:213{"issue_number": 213, "issue_description": "Some operators fail in test_ops::TestCommonXPU::test_dtypes, since there is no XPU claimed data type in test_ops infrastructure. We will enhance test infrastructure to add XPU specific claimed data type. The reporter of the issue is fengyuan14, and the assignee is daisyden, and the state of the issue is closed.\nextended UT is added", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "closed\nextended UT is added", "root_cause": "", "state": "closed"}

### Merged Result:211{"issue_number": 211, "issue_description": "The unit test for XPU device added in the code raises an 'unknow device type' error. The test is skipped. The error message is 'test/xpu/test_reductions_xpu.py::TestReductionsXPU::test_reduction_split_xpu SKIPPED (Unknown device type)'.", "reporter": "PenghuiCheng", "assignee": "huaiyuzh", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:210{"issue_number": 210, "issue_description": "IPEX supports ChannelsLast1D. It was a requirement of KPI models before. According to staging goal of upstreaming, give it low priority.\nWon't support it in PyTorch.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed\nWon't support it in PyTorch.", "root_cause": "None", "state": "closed"}

### Merged Result:208{"issue_number": 208, "issue_description": "Some utility functions of ATen operator level are operator semantics related and could be shared among different backends. In existing implementation, some of them are implemented in cpp file, could not be reused. So we will review existing torch-xpu-ops implementation, find them out and submit a PR to stock PyTorch to make them shared among like CPU, CUDA and XPU.\nIt's a long term task. We have got some unification between CUDA and XPU in other components, like runtime and Inductor. We are thinking about how to unified with CUDA implementation from ATen operator perspective.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "", "state": "open"}

### Merged Result:207{"issue_number": 207, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/207. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed. The issue title is Fail test_sort_and_select::test_isin_different_devices_xpu_float32 due to backend specific operator torch.isin., and the issue body is Content of #207 is : ### \ud83d\ude80 The feature, motivation and pitch\nTo support operator specific operator `torch.isin`. CPU fallback cannot cover it.\nSee, https://github.com/intel/torch-xpu-ops/issues/206, ", "reporter": "", "assignee": "", "resolution": "", "root_cause": "", "state": ""}

### Merged Result:206{"issue_number": 206, "issue_description": "Record limitations of CPU fallback during development. These will be the reference/check-list when we get a bug of CPU fallback in future.\n1. View/Tensor meta modification operators cannot be fallback to CPU. Breaking semantics. Requiring change Tensor metas inplace.\n3. Tensor factory operators cannot be fallback to CPU. Backend specific implementation.\n4. RNG operators cannot be fallback to CPU. Backend specific implementation.\n5. Fallback of compound operators don't work. The priority of compound operator dispatch is higher than fallback.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/206. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed\nClose it due to we plan to implement full op coverage", "root_cause": "", "state": "closed"}

### Merged Result:198{"issue_number": 198, "issue_description": "The reporter of the issue is xytintel, the assignee is , and the state of the issue is closed. The issue title is Evaluate aten::concat performance, and the issue body is Content of #198 is : ### \ud83d\ude80 The feature, motivation and pitch\n1. Find the most suitable parameter CAT_ARRAY_BATCH_SIZE.\n2. Compare performance with IPEX implementation\n, the resolution and root cause information is not provided.\nDuplicated. Closed", "reporter": "xytintel", "assignee": "", "resolution": "\nDuplicated", "root_cause": "", "state": "closed"}

### Merged Result:197{"issue_number": 197, "issue_description": "As to staging goal of PyTorch 2.5, we collect 484 operators which are required working with XPU backend. Part of them are required XPU specific implementation.\nWhen we give XPU implementation for an ATen operator, we need register all variants of the operator, like xxx.out, xxx.Tensor, xxx.Scalar, xxx_ and so on.\nFollowing the rule,\n1. We won't take additional efforts to be back for lack of registration in future and complement them. Adding variants at the moment is cheap.\n2. When we align with CUDA registration, in-tree would be seamless.\n\n- [x] random_.to\n- [x] clamp.Tensor_out\n- [x] clamp_min.Tensor_out\n- [x] clamp_max.Tensor_out\n- [x] fmod.Scalar // remove\n- [x] fmod_.Scalar // remove\n- [x] index_add_\n- [x] index_add\n- [x] remainder.Scalar_out // remove\n- [x] remainder.Scalar // remove\n- [x] remainder_.Scalar // remove\n- [x] rsub.Scalar // remove\n- [x] rsub.Scalar_out // remove\n- [x] rsub.Tensor_out // remove\n- [x] sub.Scalar // remove\n- [x] sub_.Scalar // remove\n- [x] sub.Scalar_out // remove\n- [x] sum.out // remove\n- [x] sum.dim_IntList // remove, \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/197. The reporter of the issue is fengyuan14, and the assignee is chunhuanMeng, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "chunhuanMeng", "resolution": "\nwe can close this issue", "root_cause": "", "state": "closed"}

### Merged Result:195{"issue_number": 195, "issue_description": "6 models got failed in huggingface float16 inference accuracy test. Error info: Run failed with return code: -11, Output: None, Error: None. The pass rate for inference accuracy is 97.50%. The pass rate for training accuracy is 100.00%.\nThe issue is related to a crash on fp16 due to a segment fault in the triton backend. The crash happens in the triton backend and is related to the input tensor being `divisible_by_16`. The root cause is that the input tensor is not divisible by 16, which leads to a segment fault. The fix for this issue is to make the Inductor generator hint for triton that the input tensor is `divisible_by_16`, so that triton can avoid the segment fault path. The PR https://github.com/pytorch/pytorch/pull/126261 will make Inductor generator hint for triton that input tensor is `divisible_by_16`, so that triton can avoid the segmentfault path. The fix will be in the next rolling driver and next LTS driver. The PR has been landed and the fix is expected to be verified.", "reporter": "mengfei25", "assignee": "etaf", "resolution": "\nThe fix for the issue is to make the Inductor generator hint for triton that input tensor is `divisible_by_16`, so that triton can avoid the segment fault path. The PR https://github.com/pytorch/pytorch/pull/126261 will make Inductor generator hint for triton that input tensor is `divisible_by_16`, so that triton can avoid the segmentfault path. The fix will be in the next rolling driver and next LTS driver. The PR has been landed and the fix is expected to be verified.", "root_cause": "The input tensor is not divisible by 16, which leads to a segment fault in the triton backend.", "state": "closed"}

### Merged Result:184{"issue_number": 184, "issue_description": "test_compare_cpu tanh complex support has accuracy gap. bfloat16 accuracy gap in rsqrt, sub, rounding, cumsum, add, rsub. float16 cumsum accuracy gap. pow, mul, log, complex64 got nan. index_put , index_add with bool. rounding float36 got inf. XPU reports \"not implemented\" with dtype bool, int*, uint8, while CPU would not report such error message.\nThe reporter of the issue is daisyden, and the assignee is huaiyuzh, and the state of the issue is open.", "reporter": "daisyden", "assignee": "huaiyuzh", "resolution": "\n", "root_cause": "The issue is related to the behavior of the tanh function when the input is -inf+nanj. The output of the tanh function on XPU is not aligned with the output on CUDA. The root cause is that the Sycl compiler and the CUDA compiler have different behaviors for this specific input. The expected behavior is that the tanh function should return -1 for -inf+nanj input.", "state": "open"}

### Merged Result:171{"issue_number": 171, "issue_description": "The Inducor UT passed on CPU and CUDA, but fail on XPU with error: RuntimeError: \"div_true_xpu\" not implemented for 'Long'\nfixed", "reporter": "etaf", "assignee": "fengyuan14", "resolution": "\nfixed", "root_cause": "", "state": "closed"}

### Merged Result:166{"issue_number": 166, "issue_description": "Since we use different ATen dispatch stub code gen script, we have to add checks manually to align with stock CUDA behavior. So far, these checks are not critical,\n1. Common device check. Informative error at runtime.\n2. Device guard. We set device guard on demand at kernel level.\n\n![image](https://github.com/intel/torch-xpu-ops/assets/28250760/a70d9361-e77b-4fce-b013-341ac7e994e1)\n\nAccording to the latest evaluation, we don't need it", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\nAccording to the latest evaluation, we don't need it", "root_cause": "", "state": "closed"}

### Merged Result:165{"issue_number": 165, "issue_description": "Most of e2e tests got failed with stock pytorch + related PR, there are most tests got failed after change pytorch from private to stock + PR.============ Summary for huggingface float32 inference accuracy ============num_total: 46num_passed: 13num_failed: 33pass_rate: 28.26%\nfixed", "reporter": "mengfei25", "assignee": "", "resolution": "\nfixed", "root_cause": "https://github.com/intel/torch-xpu-ops/pull/164/files/50fa410781382d991614281771ebcfcba40b67a7..c755ce75cfc1f2fb53b2643ab3c13e4c064e0d70", "state": "closed"}

### Merged Result:163{"issue_number": 163, "issue_description": "The reporter of the issue is etaf, the assignee is guangyey, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/163. The reporter of the issue is etaf, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "etaf", "assignee": "guangyey", "resolution": "closed\nclosed", "root_cause": "depends on device allocator unification. WIP...", "state": "closed"}

### Merged Result:162{"issue_number": 162, "issue_description": "For some priority gaps, we implement some operators with explicit CPU fallback. We will add XPU implementation according to priority requirements.\n\n- [ ] nonzero  // update xpu pin 2.5\n- [x] tril  // SDP math\n- [x] softmax  // SDP math\n- [x] softmax_backward  // SDP math\n- [ ] fft  // MKL 2.5\n\n### Alternatives\n_No response_\n\n### Additional context\n_No response_,\nThe reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "closed\nHF .compile and HF eager requirements are done. Nonzero and fft will be tracked by following operators development task.", "root_cause": "", "state": "closed"}

### Merged Result:157{"issue_number": 157, "issue_description": "A case fail due to oneDNN matmul implementation, Skip the case temporarily.\ntest_dtypes_nn_functional_multi_head_attention_forward_xpu, test_dtypes_nn_functional_linear_xpu, test_dtypes_pca_lowrank_xpu, test_dtypes_svd_lowrank_xpu, test_noncontiguous_samples_nn_functional_linear_xpu_int64, test_dtypes__refs_nn_functional_pdist_xpu", "reporter": "fengyuan14", "assignee": "PenghuiCheng", "resolution": "\n", "root_cause": "The issue is caused by the oneDNN matmul implementation, which leads to a segfault and failures in the case. The reporter has reported this issue and it is still open.", "state": "open"}

### Merged Result:156{"issue_number": 156, "issue_description": "test_ops.py::TestCommonXPU::test_dtypes_nn_functional_scaled_dot_product_attention_xpu FAILED\n![image](https://github.com/intel/torch-xpu-ops/assets/51150101/a7f7842a-79a0-4f1e-85a5-177d1c9d6ddd)\nSkip this case to WA this issue.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/156. The reporter of the issue is AlienLiang23, and the assignee is ZhiweiYan-96, and the state of the issue is closed.", "reporter": "AlienLiang23", "assignee": "ZhiweiYan-96", "resolution": "\nclosed", "root_cause": "", "state": "closed"}

### Merged Result:155{"issue_number": 155, "issue_description": "The output of torch.index_add on XPU and CPU is not the same when the input is of dtype=bool. The expected output is the same as the CPU version, but the XPU version returns a tensor of dtype=torch.bool instead of torch.uint8. This issue is reproducible with the following code: import torch device = \"xpu\" self_cpu = torch.ones(1, dtype=torch.bool) self_xpu = self_cpu.to(device) src_cpu = torch.zeros(1, dtype=torch.bool) src_xpu = src_cpu.to(device) dim = 0 index_cpu = torch.zeros(1, dtype=torch.int64) index_xpu = index_cpu.to(device) out_xpu = torch.index_add(self_xpu, dim, index_xpu, src_xpu) out_cpu = torch.index_add(self_cpu, dim, index_cpu, src_cpu) print(\"out_xpu:\", out_xpu) print(\"out_cpu:\", out_cpu) The output of the code is: out_xpu: tensor([0], device='xpu', dtype=torch.bool) out_cpu: tensor([0], device='cpu', dtype=torch.bool) The expected output is the same as out_cpu.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/155. The reporter of the issue is etaf, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "etaf", "assignee": "fengyuan14", "resolution": "\nFixed.", "root_cause": "", "state": "closed"}

### Merged Result:151{"issue_number": 151, "issue_description": "The latest nightly test HF FP32 training accuracy test failed on eager_two_runs_differ, there are a lot of models accuracy failed with `eager_two_runs_differ`, the pass rate is 10.87%.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/151. The reporter of the issue is chuanqi129, and the assignee is fengyuan14, and the state of the issue is closed.", "reporter": "chuanqi129", "assignee": "fengyuan14", "resolution": "\nFixing after rebasing RNG kernels.", "root_cause": "Suspected guilty commit: **e7141bd66e30ac9620924168149c2ffc11c0c6d9**", "state": "closed"}

### Merged Result:149{"issue_number": 149, "issue_description": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/149. The reporter of the issue is etaf, and the assignee is guangyey, and the state of the issue is closed.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/149. The reporter of the issue is etaf, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "etaf", "assignee": "guangyey", "resolution": "closed\n", "root_cause": "https://jira.devtools.intel.com/browse/URLZA-203", "state": "closed"}

### Merged Result:148{"issue_number": 148, "issue_description": "The reporter of the issue is etaf, and the assignee is guangyey, and the state of the issue is closed.", "reporter": "etaf", "assignee": "guangyey", "resolution": "closed", "root_cause": "", "state": "closed"}### Result:146 failed to extract

### Merged Result:144{"issue_number": 144, "issue_description": "The operator 'aten::_local_scalar_dense' is not currently implemented for the XPU device. If you want this op to be added in priority during the prototype phase of this feature, please open issue on https://github.com/intel/torch-xpu-ops/issues. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU as a fallback for this op.\nextended cases are added", "reporter": "fengyuan14", "assignee": "daisyden", "resolution": "The operator 'aten::_local_scalar_dense' is not currently implemented for the XPU device. If you want this op to be added in priority during the prototype phase of this feature, please open issue on https://github.com/intel/torch-xpu-ops/issues. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_XPU_FALLBACK=1` to use the CPU as a fallback for this op.\nextended cases are added", "root_cause": "The operator 'aten::_local_scalar_dense' is not currently implemented for the XPU device.", "state": "closed"}

### Merged Result:135{"issue_number": 135, "issue_description": "Evaluate configurations of SYCL global and local range for kernel launch, and issue body Content of #135 is : ### \ud83d\ude80 The feature, motivation and pitch\nWe followed stock CUDA about grid and block configurations. For these configurations, stock CUDA has some NV GPU arch assumption. Even we followed similar configurations, we are not clear about what we can get from Xe arch.\n1. syclMaxWorkItemsPerEU: What we can get from Xe arch, when using it.\n2. syclMaxWorkItemsPerTile: We are using max sub-group size to deduce max number of work items per Tile. It is not accurate. When runtime chooses non-max-sub-group-size kernel (IGC's optimization), we might get insufficient occupancy.\nhttps://github.com/intel/torch-xpu-ops/blob/e914ada988343c0515753360de68812ea42d0ec3/src/aten/sycl/Loops.h#L330\n``` \n  int wg_sz = syclMaxWorkItemsPerEU();\n  int num_wg = ceil_div<int>(N, wg_sz);\n  int hw_max_num_wg = syclMaxWorkItemsPerTile() / wg_sz;\n  num_wg = num_wg > hw_max_num_wg ? hw_max_num_wg : num_wg;\n  sycl_kernel_submit(wg_sz * num_wg, wg_sz, getCurrentSYCLQueue(), ker);\n``` \n### Alternatives\nWe won't regard it as highest priority. We will discuss it when we need contribute SYCL kernels to in-tree, since,\n1. Limited GPU hardware.\n2. No performance exception on IPEX so far.\n### Additional context\n_No response_,\nIt assumes explicit scaling GPU resources when using `syclMaxWorkItemsPerTile`. Or we should consider all resources of a device.", "reporter": "fengyuan14", "assignee": "xytintel", "resolution": "closed\nhttps://github.com/intel/torch-xpu-ops/pull/1418", "root_cause": "When runtime chooses non-max-sub-group-size kernel (IGC's optimization), we might get insufficient occupancy.", "state": "closed"}

### Merged Result:128{"issue_number": 128, "issue_description": "test_noncontiguous_samples_native_dropout_backward_xpu_int64: CUDA fails with same error, IPEX has no such case, will skip. (RuntimeError: \"masked_scale\" not implemented for 'Long')\ntest_non_standard_bool_values_native_dropout_backward_xpu_bool: CUDA fails with same error, IPEX has no such case, will skip. (RuntimeError: \"masked_scale\" not implemented for 'Bool')\ntest_compare_cpu_nn_functional_alpha_dropout_xpu_float32: CUDA xfail, IPEX skips it, will skip.\ntest_dtypes_native_dropout_backward_xpu: Porting difference between IPEX and torch-xpu-ops, will skip (\"masked_scale\" does not claim int support)\ntest_dtypes_nn_functional_linear_xpu: https://github.com/intel/torch-xpu-ops/issues/157\ntest_dtypes_nn_functional_multi_head_attention_forward_xpu: https://github.com/intel/torch-xpu-ops/issues/157\ntest_dtypes_pca_lowrank_xpu: https://github.com/intel/torch-xpu-ops/issues/157\ntest_dtypes_svd_lowrank_xpu: https://github.com/intel/torch-xpu-ops/issues/157\ntest_noncontiguous_samples_nn_functional_linear_xpu_int64: https://github.com/intel/torch-xpu-ops/issues/157\n285 failures due to https://github.com/intel/torch-xpu-ops/issues/157\nmove oneDNN issue to #253", "reporter": "fengyuan14", "assignee": "ZhiweiYan-96", "resolution": "\n", "root_cause": "", "state": "closed"}

### Merged Result:126{"issue_number": 126, "issue_description": "Evaluate non-alignment of kernel implementation between IPEX and stock CUDA,\n\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/126. \n\nThis is the github issue title Evaluate non-alignment of kernel implementation between IPEX and stock CUDA,\n\nContent of #126 is : ### \ud83d\ude80 The feature, motivation and pitch\n\nWe have some kernels not aligned with stock CUDA implementation, since,\n1. Functionality extension was added in stock CUDA implementation. But we have no sustainable rebase.\n2. General memory layout support was added in stock CUDA implementation. But we have no sustainable rebase.\n3. We have specific implementation for performance in some cases. But stock CUDA don't care these cases.\n\n1 is functionality related and 2 and 3 are performance related.\n- For Type-1, we should fix and aligned with stock CUDA during porting from IPEX to torch-xpu-ops.\n- For Type-2, we should align with CUDA implementation with proper priority.\n- For Type-3, we need to trad-off performance and feasibility of in-tree.\n\nHere is the list. We will add items gradually when op is ported.\n- [x] aten::bernoulli_ // Type-2\n- [ ] aten::cumsum // Type-3\n- [ ] atem::cat @xytintel // Type-3\n- [ ] aten::tril/triu @AlienLiang23 // Type-2 CUDA optimization commit: 1462d72904cb81917b9355d6a58916f389e9084c, \n\nExtract the github issue description with error message information from issue tile and issue body, \nif possible also extract the resolution and root cause information. \n\nnPlease generate a json for the information collected in English only. Please provide details and don't generate unrelated informations not addressed in the prompt. If the information is not collected succussfully, just return 0 for integer dtype or \"\" for string dtype as the json value. Please ensure the generated output is a valid json and without repeated information. \nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/126. The reporter of the issue is fengyuan14, and the assignee is fengyuan14, and the state of the issue is open.", "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": "\n", "root_cause": "It is a long term task to remainder us evaluate impacts when we have diff implementation as CUDA.", "state": "open"}

### Merged Result:125{"issue_number": 125, "issue_description": "HostCachingAlloctor provides pin memory for H2D and D2H copy. Some kernels require the alloctor, E.g. copy, cat... Using system memory + synchronization to ensure functionality. But it will hurt performance. _No response_\nWA for Copy.", "reporter": "fengyuan14", "assignee": "", "resolution": "closed\nDone.", "root_cause": "XPU HostCachingAlloctor PR of PyTorch, https://github.com/pytorch/pytorch/pull/123080", "state": "closed"}

### Merged Result:122{"issue_number": 122, "issue_description": "All `hf_clip` accuracy tests crashed with `AttributeError: 'str' object has no attribute 'shape'`.\n\n```\nTraceback (most recent call last):\n  File \"/home/sdp/actions-runner/_work/torch-xpu-ops/pytorch/benchmarks/dynamo/common.py\", line 2197, in validate_model\n    self.model_iter_fn(model, example_inputs)\n  File \"/home/sdp/conda/envs/e2e_ci/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sdp/conda/envs/e2e_ci/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sdp/conda/envs/e2e_ci/lib/python3.8/site-packages/transformers/models/clip/modeling_clip.py\", line 1101, in forward\n    vision_outputs = self.vision_model(\n  File \"/home/sdp/conda/envs/e2e_ci/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sdp/conda/envs/e2e_ci/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sdp/conda/envs/e2e_ci/lib/python3.8/site-packages/transformers/models/clip/modeling_clip.py\", line 841, in forward\n    hidden_states = self.embeddings(pixel_values)\n  File \"/home/sdp/conda/envs/e2e_ci/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/sdp/conda/envs/e2e_ci/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sdp/conda/envs/e2e_ci/lib/python3.8/site-packages/transformers/models/clip/modeling_clip.py\", line 180, in forward\n    batch_size = pixel_values.shape[0]\nAttributeError: 'str' object has no attribute 'shape'\n```\nThese issues also happen on A100 platform, not related to xpu implementation", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClosed", "root_cause": "Align with CUDA", "state": "closed"}

### Merged Result:121{"issue_number": 121, "issue_description": "tacotron2 training accuracy crashed with RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [XPUFloatType [4, 80, 724]], which is output 0 of torch::autograd::CopyBackwards, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the baseline has been refreshed.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "", "state": "closed"}

### Merged Result:120{"issue_number": 120, "issue_description": "Torchbench dlrm training accuracy crashed with NotImplementedError: Could not run 'aten::_sparse_coo_tensor_with_dims_and_tensors' with arguments from the 'SparseXPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_sparse_coo_tensor_with_dims_and_tensors' is only available for these backends: [XPU, Meta, SparseCPU, SparseMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher]\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/120. The reporter of the issue is chuanqi129, and the assignee is , and the state of the issue is closed.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nduplicate with #484", "root_cause": "", "state": "closed"}

### Merged Result:119{"issue_number": 119, "issue_description": "Torchbench functorch_dp_cifar10 training accuracy crashed with RuntimeError: slow_conv2d: grad_weight must be contiguous\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the problem has been resolved by refreshing the baseline.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nRefreshed baseline", "root_cause": "", "state": "closed"}

### Merged Result:118{"issue_number": 118, "issue_description": "Those detectron2 series models accuracy crash with `AssertionError: get_event_storage() has to be called inside a 'with EventStorage(...)' context!`\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the problem has been resolved by refreshing the baseline.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nRefreshed baseline", "root_cause": "", "state": "closed"}

### Merged Result:117{"issue_number": 117, "issue_description": "Those detectron2 series models accuracy crash with RuntimeError: dets should have the same type as scores\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the problem has been resolved by refreshing the baseline.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nRefreshed baseline", "root_cause": "", "state": "closed"}

### Merged Result:116{"issue_number": 116, "issue_description": "Torchbench has 2 models fp16 training crashed with RuntimeError: \"reflection_pad2d\" not implemented for 'Half'\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the problem has been resolved by refreshing the baseline.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nRefreshed baseline", "root_cause": "", "state": "closed"}

### Merged Result:115{"issue_number": 115, "issue_description": "There are some models crashed on RuntimeError: DispatchStub: unsupported device typexpu. Model: demucs, precision: fp32, mode: training. Model: pytorch_CycleGAN_and_pix2pix, precision: fp32 / bf16, mode: training. Model: pytorch_stargan, precision: fp32 / bf16 / fp16, mode: training.\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the problem has been resolved by refreshing the baseline.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nRefreshed baseline", "root_cause": "", "state": "closed"}

### Merged Result:114{"issue_number": 114, "issue_description": "Below models training crashed with RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\nModel | Precision\n-- | --\ncm3leon_generate | fp32/bf16/fp16\nDALLE2_pytorch | fp32/bf16/fp16\nhf_T5_generate | fp32/bf16/fp16\nmaml | fp32/bf16/fp16\npyhpc_equation_of_state | fp32/bf16/fp16\npyhpc_isoneutral_mixing | fp32/bf16/fp16\nsam | fp32/bf16/fp16\nsam_fast | fp32\n\nDALLE2_pytorch and sam float16 cuda has same failure message", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "Not specified", "state": "closed"}

### Merged Result:113{"issue_number": 113, "issue_description": "Below models eager_two_runs_differ\n\n| Model | Precision | Mode |\n| -- | -- | -- |\n| hf_BigBird | fp32 | inference |\n| sam | bf16 | inference |\n| sam | fp16 | inference |\n\n\nThe issue is related to the torch-xpu-ops library, where the reporter chuanqi129 closed the issue as the baseline has been refreshed.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "", "state": "closed"}

### Merged Result:112{"issue_number": 112, "issue_description": "moco crashed with below message, cuda can pass\n\nValueError: Default process group has not been initialized, please make sure to call init_process_group.\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/112. The reporter of the issue is chuanqi129, and the assignee is , and the state of the issue is closed.", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "", "state": "closed"}

### Merged Result:111{"issue_number": 111, "issue_description": "There are some models training crashed on `NotImplementedError:(\nThese issues also happen on A100 platform, not related to xpu implementation", "reporter": "chuanqi129", "assignee": "", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "Not related to xpu implementation", "state": "closed"}

### Merged Result:110{"issue_number": 110, "issue_description": "Torchbench has some models failed on accuracy check, the detail model list can be found as below table.\n\nprecision | mode | model\n-- | -- | --\nbfloat16 | inference | pytorch_stargan\nbfloat16 | inference | hf_Whisper\nbfloat16 | inference | BERT_pytorch\nbfloat16 | inference | hf_distil_whisper\nbfloat16 | inference | squeezenet1_1\nbfloat16 | inference | hf_BigBird\nbfloat16 | inference | mnasnet1_0\nbfloat16 | inference | shufflenet_v2_x1_0\nbfloat16 | inference | hf_Reformer\nbfloat16 | inference | timm_efficientnet\nbfloat16 | inference | timm_nfnet\nbfloat16 | inference | timm_regnet\nbfloat16 | inference | timm_resnest\nbfloat16 | training | BERT_pytorch\nbfloat16 | training | hf_Whisper\nbfloat16 | training | squeezenet1_1\nbfloat16 | training | maml_omniglot\nbfloat16 | training | fastNLP_Bert\nbfloat16 | training | mnasnet1_0\nbfloat16 | training | basic_gnn_gin\nbfloat16 | training | shufflenet_v2_x1_0\nbfloat16 | training | hf_Reformer\nbfloat16 | training | timm_efficientnet\nbfloat16 | training | timm_nfnet\nbfloat16 | training | timm_regnet\nbfloat16 | training | timm_resnest\nfloat16 | inference | pytorch_stargan\nfloat16 | inference | BERT_pytorch\nfloat16 | inference | hf_Whisper\nfloat16 | inference | moondream\nfloat16 | inference | densenet121\nfloat16 | inference | hf_distil_whisper\nfloat16 | inference | squeezenet1_1\nfloat16 | inference | hf_BigBird\nfloat16 | inference | resnet18\nfloat16 | inference | mnasnet1_0\nfloat16 | inference | pyhpc_equation_of_state\nfloat16 | inference | mobilenet_v2\nfloat16 | inference | shufflenet_v2_x1_0\nfloat16 | inference | hf_Reformer\nfloat16 | inference | timm_efficientnet\nfloat16 | inference | timm_nfnet\nfloat16 | inference | timm_regnet\nfloat16 | inference | timm_resnest\nfloat16 | training | hf_Whisper\nfloat16 | training | BERT_pytorch\nfloat16 | training | speech_transformer\nfloat16 | training | moondream\nfloat16 | training | squeezenet1_1\nfloat16 | training | mnasnet1_0\nfloat16 | training | shufflenet_v2_x1_0\nfloat16 | training | hf_Reformer\nfloat16 | training | timm_efficientnet\nfloat16 | training | timm_nfnet\nfloat16 | training | timm_regnet\nfloat16 | training | timm_resnest\nfloat32 | inference | detectron2_maskrcnn_r_50_fpn\nfloat32 | inference | hf_Longformer\nfloat32 | inference | detectron2_maskrcnn\nfloat32 | inference | detectron2_maskrcnn_r_101_c4\nfloat32 | training | resnet50_quantized_qat\nfloat32 | training | mobilenet_v2_quantized_qat\nfloat32 | training | shufflenet_v2_x1_0\nfloat32 | training | timm_resnest\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/110. The reporter of the issue is chuanqi129, and the assignee is etaf, and the state of the issue is closed.", "reporter": "chuanqi129", "assignee": "etaf", "resolution": "\nClose it as we have refreshed baseline", "root_cause": "", "state": "closed"}### Result:109 failed to extract

### Merged Result:88{"issue_number": 88, "issue_description": "The following script will get error \"le_xpu not implemented for 'ComplexFloat'\". \n\n```py\nimport torch\na = torch.tensor([3.+3.j], device=\"xpu\")\nb = torch.tensor([3.+3.j], device=\"xpu\")\nassert torch.isclose(a,b)\n```\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/88. The reporter of the issue is etaf, and the assignee is , and the state of the issue is closed.", "reporter": "etaf", "assignee": "", "resolution": "force fallback aten::abs to CPU by `export PYTORCH_XPU_FALLBACK_OP=abs`\nverified", "root_cause": "abs implementation for complex data type may have bug", "state": "closed"}

### Merged Result:74{"issue_number": 74, "issue_description": "The issue is that the function aten::_fft_c2c is not working as expected. The error message is: 'RuntimeError: expected scalar type Float but found Double' . The reporter of the issue is EikanWang, and the assignee is . The state of the issue is closed.\nThe operator was implemented by CPU fallback. MKL implementation will be tracked by following operator development task. Close the issue.", "reporter": "EikanWang", "assignee": "", "resolution": "\nThe operator was implemented by CPU fallback.", "root_cause": "The operator was implemented by CPU fallback.", "state": "closed"}

### Merged Result:72{"issue_number": 72, "issue_description": "Currently, this repo does not provide any code format logic. We need to port PyTorch linter tools here and enable it in our CI.\n", "reporter": "EikanWang", "assignee": "chuanqi129", "resolution": "\ndone", "root_cause": "", "state": "closed"}### Result:68 failed to extract

### Merged Result:67{"issue_number": 67, "issue_description": "Refactor the source code structure just like ATen, Stock PyTorch places the device kernel implementations under `aten/native/${device_tag}` while the code namespace is `at::native`. We need to refactor the code structure to align with the stock pytorch.", "reporter": "EikanWang", "assignee": "fengyuan14", "resolution": "", "root_cause": "", "state": "closed"}

### Merged Result:66{"issue_number": 66, "issue_description": "Currently, we have developed the test cases to validate the tensor creation operations for XPU. Compared to the stock PyTorch, the coverage of the self-developed test cases is lower than the stock PyTorch. So we need to port the storck PyTorch cases.\nDuplicated. Close.", "reporter": "EikanWang", "assignee": "daisyden", "resolution": "\nDuplicated. Close.", "root_cause": "", "state": "closed"}

### Merged Result:58{"issue_number": 58, "issue_description": "If we fallback `aten::set_.source_Storage` and `aten::set_.source_Storage_storage_offset` to CPU, pytorch cause a runtime error when running huggingface model:\nRuntimeError: Attempted to set the storage of a tensor on device \"cpu\" to a storage on different device \"xpu:0\".  This is no longer allowed; the devices must match.", "reporter": "etaf", "assignee": "", "resolution": "", "root_cause": "This is a github issue link https://github.com/intel/torch-xpu-ops/issues/58. The reporter of the issue is etaf, and the assignee is , and the state of the issue is closed.", "state": "closed"}

### Merged Result:33{"issue_number": 33, "issue_description": "Integer div result with wrong data type(shoule be float but got int).\nThis is a github issue link https://github.com/intel/torch-xpu-ops/issues/33. The reporter of the issue is etaf, and the assignee is , and the state of the issue is closed.", "reporter": "etaf", "assignee": "", "resolution": "\n", "root_cause": "", "state": "closed"}
