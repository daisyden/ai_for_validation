{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b00b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"\"\n",
    "os.environ[\"https_proxy\"] = \"\"\n",
    "\n",
    "DEFAULT_HOST_IP = \"10.112.100.138\"\n",
    "\n",
    "\n",
    "def QnA(request, host_ip=DEFAULT_HOST_IP):\n",
    "    url = f\"http://{host_ip}:8888/v1/chatqna\"\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=request)\n",
    "    return response\n",
    "\n",
    "\n",
    "def upload_rag_file(rag_file, host_ip=DEFAULT_HOST_IP):\n",
    "    url = f\"http://{host_ip}:6007/v1/dataprep/ingest\"\n",
    "\n",
    "    with open(rag_file, \"rb\") as f:\n",
    "        files = {\"files\": f}\n",
    "        response = requests.post(url, files=files)\n",
    "\n",
    "    return response\n",
    "\n",
    "def delete_rag_file(rag_file, host_ip=DEFAULT_HOST_IP):\n",
    "    url = f\"http://{host_ip}:6007/v1/dataprep/delete\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"file_path\": rag_file}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffaa1fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response Body: {\"status\":true}\n"
     ]
    }
   ],
   "source": [
    "response=delete_rag_file(\"results.txt\")\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Body:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d966138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response Body: {\"status\":200,\"message\":\"Data preparation succeeded\"}\n"
     ]
    }
   ],
   "source": [
    "# download pdf file\n",
    "# wget https://raw.githubusercontent.com/opea-project/GenAIComps/v1.1/comps/retrievers/redis/data/nke-10k-2023.pdf\n",
    "\n",
    "# upload pdf file with dataprep\n",
    "rag_file = \"/home/sdp/lifeng/ai_for_validation/RAG_opea/results.txt\"\n",
    "\n",
    "upload_rag_file_response = upload_rag_file(rag_file)\n",
    "\n",
    "print(\"Status Code:\", upload_rag_file_response.status_code)\n",
    "print(\"Response Body:\", upload_rag_file_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d98710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response Body: {\"id\":\"chatcmpl-mmXwFUEzSpPWwDnJ4bq86e\",\"object\":\"chat.completion\",\"created\":1744937735,\"model\":\"chatqna\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Okay, so I'm trying to figure out why the unit test `test_multiheadattention_fastpath_attn_mask_attn_mask_dim_2_key_padding_mask_dim_2_bool_xpu` is failing with a RuntimeError about the output meta disagreeing with the real implementation. Let me go through the information provided step by step.\\n\\nFirst, I see that there are three search results provided. Each is a merged result with details about an issue. I need to check if the failed test is mentioned in any of these.\\n\\nLooking at the first result, issue 1432, the description mentions several failed tests, including the one I'm looking for: `test_multiheadattention_fastpath_attn_mask_attn_mask_dim_2_key_padding_mask_dim_2_bool_xpu`. The root cause is a bug in the scaled_dot_product_attention function due to a specific PyTorch commit, which caused incorrect output shapes and NaN outputs for fully masked rows. The assignee is LuFinch, and the state is open.\\n\\nThe second result, issue 1437, also mentions several test failures, but the test names here seem different. They involve `test_dispatch_meta_outplace_nn_functional_scaled_dot_product_attention_xpu` and others, but not the specific multiheadattention test. Plus, this issue is closed.\\n\\nThe third result, issue 1078, talks about errors in `test_fake_crossref_backward_amp_nn_functional_multilabel_soft_margin_loss_xpu_float32`, which doesn't match the failing test either.\\n\\nSo, the failed test is specifically mentioned in issue 1432. The root cause is a bug in the scaled_dot_product_attention function from a PyTorch commit, leading to incorrect output shapes and NaNs. The assignee is LuFinch, and the issue is still open, meaning it's a known problem that hasn't been resolved yet.\\n</think>\\n\\nThe failed test `test_multiheadattention_fastpath_attn_mask_attn_mask_dim_2_key_padding_mask_dim_2_bool_xpu` is linked to issue #1432. The root cause is a bug in the scaled_dot_product_attention function from commit c21dc11, causing incorrect output shapes and NaNs. The assignee is LuFinch, and the issue is open.\",\"audio\":null},\"finish_reason\":\"stop\",\"metadata\":null}],\"usage\":{\"prompt_tokens\":0,\"total_tokens\":0,\"completion_tokens\":0}}\n",
      "The failed test `test_multiheadattention_fastpath_attn_mask_attn_mask_dim_2_key_padding_mask_dim_2_bool_xpu` is linked to issue #1432. The root cause is a bug in the scaled_dot_product_attention function from commit c21dc11, causing incorrect output shapes and NaNs. The assignee is LuFinch, and the issue is open.\n"
     ]
    }
   ],
   "source": [
    "request = {\n",
    "        \"messages\": \"Unit test test_multiheadattention_fastpath_attn_mask_attn_mask_dim_2_key_padding_mask_dim_2_bool_xpu failed returned RuntimeError: output 1: meta disagrees with real impl \",\n",
    "        \"stream\": False,\n",
    "        \"top_n\": 3,       \n",
    "    }\n",
    "\n",
    "QnA_response=QnA(request)\n",
    "\n",
    "print(\"Status Code:\", QnA_response.status_code)\n",
    "print(\"Response Body:\", QnA_response.text)\n",
    "\n",
    "if QnA_response.status_code==200:\n",
    "    result=QnA_response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    answer = result.split(\"</think>\")[-1].strip()\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QnA_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f128ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I'm trying to figure out why the unit test `test_multiheadattention_fastpath_attn_mask_attn_mask_dim_2_key_padding_mask_dim_2_bool_xpu` is failing with a RuntimeError about the output meta disagreeing with the real implementation. Let me go through the information provided step by step.\n",
      "\n",
      "First, I see that there are three search results provided. Each is a merged result with details about an issue. I need to check if the failed test is mentioned in any of these.\n",
      "\n",
      "Looking at the first result, issue 1432, the description mentions several failed tests, including the one I'm looking for: `test_multiheadattention_fastpath_attn_mask_attn_mask_dim_2_key_padding_mask_dim_2_bool_xpu`. The root cause is a bug in the scaled_dot_product_attention function due to a specific PyTorch commit, which caused incorrect output shapes and NaN outputs for fully masked rows. The assignee is LuFinch, and the state is open.\n",
      "\n",
      "The second result, issue 1437, also mentions several test failures, but the test names here seem different. They involve `test_dispatch_meta_outplace_nn_functional_scaled_dot_product_attention_xpu` and others, but not the specific multiheadattention test. Plus, this issue is closed.\n",
      "\n",
      "The third result, issue 1078, talks about errors in `test_fake_crossref_backward_amp_nn_functional_multilabel_soft_margin_loss_xpu_float32`, which doesn't match the failing test either.\n",
      "\n",
      "So, the failed test is specifically mentioned in issue 1432. The root cause is a bug in the scaled_dot_product_attention function from a PyTorch commit, leading to incorrect output shapes and NaNs. The assignee is LuFinch, and the issue is still open, meaning it's a known problem that hasn't been resolved yet.\n",
      "</think>\n",
      "\n",
      "The failed test `test_multiheadattention_fastpath_attn_mask_attn_mask_dim_2_key_padding_mask_dim_2_bool_xpu` is linked to issue #1432. The root cause is a bug in the scaled_dot_product_attention function from commit c21dc11, causing incorrect output shapes and NaNs. The assignee is LuFinch, and the issue is open.\n"
     ]
    }
   ],
   "source": [
    "result = result.replace(\"\\\\n\\\\n\", \"\\n\\n\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lifeng_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
