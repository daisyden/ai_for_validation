{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b00b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"\"\n",
    "os.environ[\"https_proxy\"] = \"\"\n",
    "\n",
    "DEFAULT_HOST_IP = \"10.112.100.138\"\n",
    "\n",
    "\n",
    "def QnA(request, host_ip=DEFAULT_HOST_IP):\n",
    "    url = f\"http://{host_ip}:8888/v1/chatqna\"\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=request)\n",
    "    return response\n",
    "\n",
    "\n",
    "def upload_rag_file(rag_file, host_ip=DEFAULT_HOST_IP):\n",
    "    url = f\"http://{host_ip}:6007/v1/dataprep/ingest\"\n",
    "\n",
    "    with open(rag_file, \"rb\") as f:\n",
    "        files = {\"files\": f}\n",
    "        response = requests.post(url, files=files)\n",
    "\n",
    "    return response\n",
    "\n",
    "def delete_rag_file(rag_file, host_ip=DEFAULT_HOST_IP):\n",
    "    url = f\"http://{host_ip}:6007/v1/dataprep/delete\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\"file_path\": rag_file}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffaa1fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 404\n",
      "Response Body: {\"detail\":\"File not found in db file-keys. Please check file_path.\"}\n"
     ]
    }
   ],
   "source": [
    "response=delete_rag_file(\"results.txt\")\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response Body:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d966138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response Body: {\"status\":200,\"message\":\"Data preparation succeeded\"}\n"
     ]
    }
   ],
   "source": [
    "# download pdf file\n",
    "# wget https://raw.githubusercontent.com/opea-project/GenAIComps/v1.1/comps/retrievers/redis/data/nke-10k-2023.pdf\n",
    "\n",
    "# upload pdf file with dataprep\n",
    "rag_file = \"/home/sdp/lifeng/ai_for_validation/RAG_opea/results.txt\"\n",
    "\n",
    "upload_rag_file_response = upload_rag_file(rag_file)\n",
    "\n",
    "print(\"Status Code:\", upload_rag_file_response.status_code)\n",
    "print(\"Response Body:\", upload_rag_file_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7d98710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response Body: {\"id\":\"chatcmpl-bVQSj2LLKXQ2nRVUdXLekB\",\"object\":\"chat.completion\",\"created\":1745389100,\"model\":\"chatqna\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Okay, so I'm trying to figure out why the unit test `third_party.torch-xpu-ops.test.xpu.test_linalg_xpu.TestLinalgXPU.test_gemm_bias_offline_tunableop_xpu_bfloat16` is failing with the error message \\\"AssertionError: Torch not compiled with CUDA enabled.\\\" \\n\\nFirst, I'll look at the error message. It says Torch isn't compiled with CUDA enabled. That makes me think that somewhere in the code, it's trying to use CUDA functions but Torch wasn't built with CUDA support. But wait, the test is for XPU, not CUDA. So why is it complaining about CUDA?\\n\\nLooking at the search results, I see two issues that might be related. The first is issue 1521, which mentions that PyTorch's Flex Attention fails on XPU with the same error about Torch not being compiled with CUDA enabled. The root cause there is that FlexAttention isn't enabled on XPU yet, and they're targeting it for version 2.8. \\n\\nThe second issue is 302, which is about various test failures related to XPU. It mentions problems with CUDA functions not having counterparts on XPU, like `torch.cuda.memory_allocated()` and others. It also talks about missing support for XPU in certain parts of the test framework and issues with `torch.xpu.FloatTensor`. The root cause here is that the `torch-xpu-ops` version is outdated, specifically commit `de744d9`, which doesn't enable `nanmean`, causing backward dtype issues.\\n\\nNow, the failing test is in `test_linalg_xpu`, which suggests it's related to linear algebra operations on XPU. The error message is about Torch not having CUDA enabled, but the test is for XPU. So maybe the test is incorrectly trying to use CUDA functions instead of XPU ones, or perhaps there's a missing dependency or incorrect setup in the test environment.\\n\\nLooking at issue 302, it seems like there were multiple issues with XPU support, including missing functions and incorrect handling. The fact that the test is failing with a CUDA-related error might indicate that the test is trying to use CUDA code paths that aren't properly set up for XPU. Alternatively, maybe the test expects certain CUDA functions to be available on XPU, which they aren't.\\n\\nThe root cause in issue 302 was that the `torch-xpu-ops` version was outdated, leading to missing functionality. If the test in question relies on `torch-xpu-ops` for certain operations, an outdated version might not support the required features, causing the test to fail. The solution mentioned was updating `torch-xpu-ops` to a newer version where these issues were fixed.\\n\\nPutting this together, the failing test might be because the test is either incorrectly using CUDA functions or relying on an outdated version of `torch-xpu-ops` that doesn't handle the required operations properly. The solution would be to ensure that the test is correctly using XPU functions and that `torch-xpu-ops` is up to date.\\n\\nI should also consider if there are any other dependencies or environment variables that need to be set correctly for XPU tests to run. Maybe the test environment isn't properly configured to use XPU, causing it to fall back to CUDA or other backends, leading to the error.\\n\\nIn summary, the issue is likely due to either incorrect function usage in the test or an outdated `torch-xpu-ops` version. Updating the dependencies and ensuring the test correctly uses XPU functions should resolve the issue.\\n</think>\\n\\n```json\\n{\\n  \\\"issue_id\\\": 302,\\n  \\\"issue_state\\\": \\\"closed\\\",\\n  \\\"issue_description\\\": \\\"The test failure is due to issues in the torch-xpu-ops version used in the test environment. The outdated version (commit de744d9) lacks necessary support for certain operations, leading to test failures. The error message 'AssertionError: Torch not compiled with CUDA enabled' suggests the test is relying on CUDA functions not properly handled by the current XPU setup.\\\",\\n  \\\"issue_owner\\\": \\\"majing921201\\\",\\n  \\\"root_cause\\\": \\\"The torch-xpu-ops version is outdated, causing missing functionality and test failures.\\\",\\n  \\\"solution\\\": \\\"Update torch-xpu-ops to a newer version where these issues have been resolved.\\\"\\n}\\n```\",\"audio\":null},\"finish_reason\":\"stop\",\"metadata\":null}],\"usage\":{\"prompt_tokens\":0,\"total_tokens\":0,\"completion_tokens\":0}}\n",
      "```json\n",
      "{\n",
      "  \"issue_id\": 302,\n",
      "  \"issue_state\": \"closed\",\n",
      "  \"issue_description\": \"The test failure is due to issues in the torch-xpu-ops version used in the test environment. The outdated version (commit de744d9) lacks necessary support for certain operations, leading to test failures. The error message 'AssertionError: Torch not compiled with CUDA enabled' suggests the test is relying on CUDA functions not properly handled by the current XPU setup.\",\n",
      "  \"issue_owner\": \"majing921201\",\n",
      "  \"root_cause\": \"The torch-xpu-ops version is outdated, causing missing functionality and test failures.\",\n",
      "  \"solution\": \"Update torch-xpu-ops to a newer version where these issues have been resolved.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "request = {\n",
    "        \"messages\": \"Unit test third_party.torch-xpu-ops.test.xpu.test_linalg_xpu.TestLinalgXPU  test_gemm_bias_offline_tunableop_xpu_bfloat16 returned failed returned: AssertionError: Torch not compiled with CUDA enabled.\",\n",
    "        \"stream\": False,\n",
    "        \"top_n\": 3,       \n",
    "    }\n",
    "\n",
    "QnA_response=QnA(request)\n",
    "\n",
    "print(\"Status Code:\", QnA_response.status_code)\n",
    "print(\"Response Body:\", QnA_response.text)\n",
    "\n",
    "if QnA_response.status_code==200:\n",
    "    result=QnA_response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    answer = result.split(\"</think>\")[-1].strip()\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QnA_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22f128ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out if the failed unit test in PyTorch is a known issue. The test that failed is `third_party.torch-xpu-ops.test.xpu.test_linalg_xpu.TestLinalgXPU.test_gemm_bias_offline_tunableop_xpu_bfloat16`, and the error message is `AssertionError: Torch not compiled with CUDA enabled.` \n",
      "\n",
      "First, I'll look at the search results provided. There are two merged results: issue 1521 and issue 302. \n",
      "\n",
      "Starting with issue 1521, the description mentions that PyTorch's Flex Attention fails on XPU with the same error message. The root cause is that FlexAttention isn't enabled on XPU yet, and they're targeting it for enablement in torch-2.8. The issue is open, and the assignee is liangan1. \n",
      "\n",
      "Looking at issue 302, it's a bit more extensive. It lists several issues related to enabling various CUDA functions on XPU. One of the points mentions that `torch-xpu-ops` version is outdated, leading to skipped test cases. The root cause here is that the `torch-xpu-ops` version used doesn't support certain features, causing backward compatibility issues with CUDA.\n",
      "\n",
      "The error message in the failed test is the same as in issue 1521: `AssertionError: Torch not compiled with CUDA enabled.` This suggests that the test is trying to use CUDA functionality, but Torch isn't built with CUDA support. However, since the test is for XPU, it's more likely related to XPU's setup rather than CUDA.\n",
      "\n",
      "In issue 302, point 4 mentions that `test_storage_meta_errors()` requires `torch.TypedStorage.xpu` support. Also, point 5 talks about needing counterparts for CUDA tensor types in XPU. The error might be because the XPU backend doesn't have the necessary support, similar to what's described in issue 1521 where FlexAttention isn't enabled.\n",
      "\n",
      "Additionally, issue 302's root cause points to an outdated `torch-xpu-ops` version, which might mean that certain operations aren't supported or are missing, leading to test failures. The test in question involves `test_gemm_bias_offline_tunableop_xpu_bfloat16`, which likely uses XPU-specific operations that aren't properly implemented or supported in the current version.\n",
      "\n",
      "So, putting this together, the failed test is likely related to missing or outdated XPU support in PyTorch. The error message is a red herring pointing to CUDA, but the underlying issue is with XPU's implementation not being fully enabled or supported yet. This aligns with both issues 1521 and 302, which discuss various missing features and outdated versions affecting XPU tests.\n",
      "\n",
      "Therefore, the failed test is a known issue, specifically related to the state of XPU support in PyTorch, possibly due to either specific operations not being enabled or an outdated `torch-xpu-ops` version.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"issue_id\": 1521,\n",
      "  \"issue_state\": \"open\",\n",
      "  \"issue_description\": \"PyTorch Flex Attention fails on XPU. The error message is AssertionError: Torch not compiled with CUDA enabled. The reporter of the issue is githubsgi, and the assignee is liangan1, and the state of the issue is open. FlexAttention is not enabled on XPU yet. We target to enable it on torch-2.8. The draft pr can be found https://github.com/pytorch/pytorch/pull/143553\",\n",
      "  \"issue_owner\": \"liangan1\",\n",
      "  \"issue_root_cause\": \"FlexAttention is not enabled on XPU yet\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "result = result.replace(\"\\\\n\\\\n\", \"\\n\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### You are a helpful, respectful, and honest assistant tasked with determining whether a failed unit test case is a known issue. \\\n",
    "Please refer to the search results from the local knowledge base. All issues retrieved by RAG, regardless of status, if matched, they should be treated as known issues \\\n",
    "If yes, provide the issue id, issue state, issue description, issue owner, issue root cause and solution information. \\\n",
    "If no, return the issue id, issue state and issue owner as N/A and give some insights in issue description, issue root cause and solution. \\\n",
    "Only include information relevant to the question. Do not provide false information if unsure. \\\n",
    "Please generate a valid json for the information collected in English only. \\\n",
    "Please provide details and don't generate unrelated informations not addressed in the prompt. \\\n",
    "Please ensure the generated output is a valid json and without repeated information. \\n\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lifeng_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
