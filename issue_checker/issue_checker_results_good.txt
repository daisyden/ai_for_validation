{"issue_number": 1679, "issue_description": {"score": 2, "evidence": "The issue describes a failing test in the test_foreach_xpu.py file, specifically the test_parity__foreach_div_fastpath_outplace_xpu_complex128 test case. It mentions that the test was disabled because it was failing on the main branch, which provides clear context about the problem."}, "error_message": {"score": 0, "evidence": "No specific error message is provided in the issue description."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No detailed steps are provided to reproduce the issue. The issue only mentions that the test was failing but doesn't outline how to replicate it."}, "software_version": {"score": 0, "evidence": "No specific versions of Python, PyTorch, or other relevant software are mentioned."}, "platform": {"score": 0, "evidence": "While the platform is noted as Linux, there's no detailed information about the environment or setup."}}, "reporter": "RUIJIEZHONG66166", "assignee": "", "resolution": {"score": 0, "evidence": "No information is provided about potential resolutions or fixes."}, "root_cause": {"score": 0, "evidence": "The issue does not delve into why the test is failing; it merely states that it's failing."}, "impact": {"score": 1, "evidence": "The issue mentions that the test was disabled because it was failing, which indicates that it's affecting the CI/CD process but doesn't elaborate on the broader impact on functionality or performance."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled with 'known_ut_issue', which suggests it's related to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The test case mentioned is part of the test suite, specifically within the TestForeachXPU class, indicating it's a unit test issue."}, "report_date": {"update": "2025-05-19", "evidence": "The issue was created on 2025-05-19, providing a clear date for tracking."}, "last_update": {"update": "2025-05-19", "evidence": "There's no information on when the issue was last updated beyond its creation date."}}
{"issue_number": 1678, "issue_description": {"score": 2, "evidence": "The issue describes a bug where calling model.share_memory() results in a RuntimeError: _share_fd_: only available on CPU. The reporter provides a detailed error log and a link to a minimal example. The description is concise and clear."}, "error_message": {"score": 2, "evidence": "The error message is clearly provided: RuntimeError: _share_fd_: only available on CPU, along with the traceback showing where the error occurs."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The reporter provides a link to their example code but does not include the actual steps to reproduce the issue within the issue description. While the example is helpful, it would be clearer if the steps were explicitly listed."}, "software_version": {"score": 2, "evidence": "The reporter includes the PyTorch version and other relevant library versions, as well as the platform information. This is clear and helpful."}, "platform": {"score": 2, "evidence": "The platform information includes OS, CPU details, and other relevant hardware information, which is clearly provided."}}, "reporter": "jafraustro", "assignee": "", "resolution": {"score": 0, "evidence": "No information is provided about potential solutions or resolutions."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 1, "evidence": "The impact is not explicitly stated but can be inferred as significant since it prevents the use of share_memory() on XPU devices, which is essential for certain distributed training workflows."}, "state": "open", "labeled_module": {"module": "N/A", "evidence": "The issue does not have any labels assigned that specify a module."}, "predicted_module": {"module": "Core", "evidence": "The issue relates to the share_memory() method, which is a core functionality of PyTorch tensors and modules."}, "report_date": {"update": "2025-05-16", "evidence": "The issue was created on 2025-05-16 17:37:07+00:00."}, "last_update": {"update": "2025-05-16", "evidence": "The issue was created on 2025-05-16 17:37:07+00:00 and has no further updates mentioned."}}
{"issue_number": 1674, "issue_description": {"score": 2, "evidence": "The issue description is concise and clearly explains the problem with the test case and the error message."}, "error_message": {"score": 2, "evidence": "The error message is detailed and clearly indicates the mismatch in tensor elements and the allowed differences."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes the test case name and suggests running a specific Python command, but lacks detailed reproduce steps."}, "software_version": {"score": 2, "evidence": "PyTorch version and other relevant library versions are provided in detail."}, "platform": {"score": 2, "evidence": "The OS, CPU, and other hardware details are provided."}}, "reporter": "PenghuiCheng", "assignee": "githubsgi", "resolution": {"score": 0, "evidence": "No information is provided about the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue affects the accuracy of distributed tensor operations, but the specific impact is not detailed."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue has the label 'bugmodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "The test case is related to distributed tensor operations and the label indicates the distributions module."}, "report_date": {"update": "2025-05-16", "evidence": "The issue was created on 2025-05-16."}, "last_update": {"update": "2025-05-16", "evidence": "No further updates are mentioned, so the last update is the same as the creation date."}}
{"issue_number": 1668, "issue_description": {"score": 2, "evidence": "The issue describes a bug where certain distributed tests are failing due to tensor accuracy issues, specifically in the _composable compile related UT cases."}, "error_message": {"score": 2, "evidence": "The error message indicates a tensor-like assertion failure in the test_compile_backward_only test case, showing specific discrepancies in gradients between processes."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides clear steps to reproduce the test failure using the command `python test/distributed/_composable/test_replicate_with_compiler.py ReplicateTest.test_compile_backward_only`."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0a0+gita2b2fc6 is mentioned, along with the operating system and hardware details."}, "platform": {"score": 2, "evidence": "The platform details include Ubuntu 22.04.4 LTS, x86_64 architecture, and specific CPU information."}}, "reporter": "PenghuiCheng", "assignee": "zhangxiaoli73", "resolution": {"score": 0, "evidence": "No information provided on the resolution status or steps taken to fix the issue."}, "root_cause": {"score": 0, "evidence": "No detailed root cause analysis is provided in the issue description."}, "impact": {"score": 2, "evidence": "The issue impacts the accuracy of distributed training when using certain compilation flags, potentially affecting model training reliability."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under the 'bugmodule: distributions' category."}, "predicted_module": {"module": "distributions", "evidence": "The test case pertains to distributed training functionality, specifically in the _composable test suite."}, "report_date": {"update": "2025-05-14", "evidence": "The issue was created on 2025-05-14."}, "last_update": {"update": "2025-05-14", "evidence": "No further updates mentioned beyond the initial report date."}}
{"issue_number": 1667, "issue_description": {"score": 2, "evidence": "The issue describes an AssertionError related to a test case involving distributed training and FSDP. The error message indicates that a specific string was not found in the expected output, pointing to an issue with function tracing in PyTorch's Dynamo."}, "error_message": {"score": 2, "evidence": "The error message clearly states that 'setattr() on Tensor.requires_grad' was not found in the expected output, providing details about the function 'is_available' being skipped by Dynamo."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue provides a test case name but lacks explicit shell or Python commands to reproduce the issue. The user is instructed to run a specific test, but the exact commands are not detailed."}, "software_version": {"score": 2, "evidence": "The PyTorch version and platform information are provided, including the commit hash, OS, and CPU details."}, "platform": {"score": 2, "evidence": "OS, CPU, and other system details are included, aiding in reproducing the issue."}}, "reporter": "PenghuiCheng", "assignee": "ashokei", "resolution": {"score": 0, "evidence": "No information is provided on the resolution status or potential fixes."}, "root_cause": {"score": 0, "evidence": "The root cause is not identified or discussed in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects the distributed training functionality, potentially causing test failures and impacting the reliability of the FSDP implementation."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under the 'bugmodule: distributions' category."}, "predicted_module": {"module": "distributions", "evidence": "The test case involves distributed training components, aligning with the 'distributions' module."}, "report_date": {"update": "2025-05-14", "evidence": "The issue was created on this date."}, "last_update": {"update": "2025-05-15", "evidence": "No further updates are present in the issue."}}
{"issue_number": 1666, "issue_description": {"score": 2, "evidence": "The issue describes a bug where there is an accuracy gap in FSDP checkpoint related unit tests. The error occurs in multiple test cases and involves tensor comparisons failing due to differences beyond the allowed threshold."}, "error_message": {"score": 2, "evidence": "The traceback shows a RuntimeError with a specific AssertionError indicating tensors are not close. The message includes details about the maximum differences and allowed thresholds."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides clear steps to reproduce the test by running a specific Python command: `python test/distributed/fsdp/test_fsdp_checkpoint.py TestFSDPCheckpoint.test_basic_checkpoint_end_to_end_cpu_offload1_offload_activations_False_use_orig_params_False`"}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0a0+gita2b2fc6 is specified, along with CUDA and other environment details."}, "platform": {"score": 2, "evidence": "The platform is Ubuntu 22.04.4 LTS with Intel Xeon Platinum 8480+ CPUs and no CUDA support."}}, "reporter": "PenghuiCheng", "assignee": "githubsgi", "resolution": {"score": 0, "evidence": "No information provided on how to resolve the issue."}, "root_cause": {"score": 0, "evidence": "No detailed root cause analysis is provided in the issue description."}, "impact": {"score": 2, "evidence": "The issue affects the accuracy of FSDP checkpoints, which is critical for distributed training and model checkpointing reliability."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under 'bugmodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "The test cases are related to FSDP checkpointing, which falls under distributed training. The label 'bugmodule: distributions' confirms this categorization."}, "report_date": {"update": "2025-05-14", "evidence": "The issue was created on 2025-05-14."}, "last_update": {"update": "2025-05-15", "evidence": "No further updates are mentioned beyond the initial report."}}
{"issue_number": 1665, "issue_description": {"score": 2, "evidence": "The issue description provides detailed information about the bug, including the test cases, error messages, and logs from the traceback. The problem is related to FSDP2 compilation in PyTorch with the inductor backend, leading to a failure in compiling the graph due to incorrect `fsdp.copy_` operations count."}, "error_message": {"score": 2, "evidence": "The error message clearly states that the expected number of `fsdp.copy_` operations is 4, but 0 were found. This indicates a discrepancy in the number of operations during graph compilation."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes clear steps to reproduce the test, including the specific test file and test case to run. The commands provided are specific and executable, such as `python test/distributed/_composable/fsdp/test_fully_shard_compile.py TestFullyShardCompile.test_transformer_backend_inductor_fullgraph_True`."}, "software_version": {"score": 2, "evidence": "The PyTorch version is specified as the 'daisyden/distributed_2.8' branch, and the oneAPI version is 2025.1.1. The Python version used is 3.10."}, "platform": {"score": 2, "evidence": "The issue mentions the use of `pytest` and the specific test file, indicating that the problem occurs in a distributed testing environment within PyTorch's test suite."}}, "reporter": "PenghuiCheng", "assignee": "zhangxiaoli73", "resolution": {"score": 0, "evidence": "No information is provided about the resolution process or status."}, "root_cause": {"score": 0, "evidence": "No specific root cause is identified in the issue description. The error suggests an issue with the FSDP compilation process, possibly in how `fsdp.copy_` operations are being tracked or counted during graph compilation."}, "impact": {"score": 2, "evidence": "The issue impacts the FSDP2 compilation process, potentially affecting distributed training in PyTorch when using the inductor backend. This could lead to failed training runs or incorrect model behavior due to missing or incorrect `fsdp.copy_` operations."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under the 'bugmodule: distributions' category."}, "predicted_module": {"module": "distributions", "evidence": "The test case is within the `test_fully_shard_compile.py` file, which relates to FSDP (Fully Sharded Data Parallel) functionality, a part of PyTorch's distributed training capabilities. This aligns with the 'distributions' module."}, "report_date": {"update": "2025-05-14", "evidence": "The issue was created on 2025-05-14."}, "last_update": {"update": "2025-05-14", "evidence": "The issue was last updated on 2025-05-14 as it was just created."}}
{"issue_number": 1663, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the test case that failed and the error message."}, "error_message": {"score": 2, "evidence": "The error message 'Expected zero exit code but got -9' is clear and specific."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides the specific test case (test_dp_state_dict_cpu_offload) and the logs, which can be used to reproduce the issue."}, "software_version": {"score": 2, "evidence": "PyTorch version, oneAPI version, and platform information are provided in detail."}, "platform": {"score": 2, "evidence": "The OS, CPU, and other hardware details are clearly specified."}}, "reporter": "PenghuiCheng", "assignee": "ashokei", "resolution": {"score": 1, "evidence": "No specific information about the resolution is provided in the issue."}, "root_cause": {"score": 1, "evidence": "The issue does not provide detailed analysis of the root cause, but the error suggests a possible timeout or communication issue in distributed training."}, "impact": {"score": 2, "evidence": "The test failure indicates potential issues in distributed training with FSDP and CPU offload, affecting the reliability of the distributed training setup."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under 'bugmodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "The test case pertains to distributed training functionality, aligning with the distributions module."}, "report_date": {"update": "2025-05-14", "evidence": "The issue was created on 2025-05-14."}, "last_update": {"update": "2025-05-15", "evidence": "No further updates are provided beyond the initial report."}}

{"issue_number": 1656, "issue_description": {"score": 2, "evidence": "The issue is titled 'XPU ops will return wrong results when handling specific offset inputs' and the body describes the problem with a detailed reproducer script and debug information, making the issue_description clear and concise."}, "error_message": {"score": 2, "evidence": "The error message indicates that tensors do not match within tolerance when comparing XPU and CPU results, which is clear and specific."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The user provided a Python script that reproduces the issue, which is clear and concise."}, "software_version": {"score": 2, "evidence": "The PyTorch version is specified as 2.8.0.dev20250512+xpu, and platform information is provided, making this section complete and clear."}, "platform": {"score": 2, "evidence": "Platform details including OS, CPU, and others are provided, making this section complete and clear."}}, "reporter": "hoshibara", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No information about resolution steps or timeline is provided."}, "root_cause": {"score": 0, "evidence": "No detailed root cause analysis is provided in the issue description."}, "impact": {"score": 2, "evidence": "The issue affects the correctness of XPU operations, which could impact applications relying on XPU for performance, making the impact clear."}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "The issue is labeled with 'bug', and the test case involves XPU operations, which fall under the 'op_imp' module."}, "predicted_module": {"module": "OP impl", "evidence": "The issue is related to XPU operations and their implementation, which aligns with the 'op_imp' module."}, "report_date": {"update": "2025-05-13", "evidence": "The issue was created on 2025-05-13."}, "last_update": {"update": "2025-05-14", "evidence": "No further updates are mentioned, so the last update is the creation date."}}
{"issue_number": 1649, "issue_description": {"score": 2, "evidence": "The issue describes a bug where using inconsistent OneAPI versions causes an ImportError. The reporter provides a detailed error message and the steps to reproduce the issue."}, "error_message": {"score": 2, "evidence": "The error message shows a version mismatch between the prebuilt wheels and the installed OneAPI version."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The reporter provides setup.py and the Python import commands as part of the issue."}, "software_version": {"score": 2, "evidence": "The issue includes the PyTorch version and other relevant library versions."}, "platform": {"score": 2, "evidence": "The platform information includes OS, compiler versions, and hardware details."}}, "reporter": "ZhaoqiongZ", "assignee": "dvrogozh", "resolution": {"score": 2, "evidence": "The reporter suggests a clear error message to improve user experience."}, "root_cause": {"score": 2, "evidence": "The issue stems from using different OneAPI versions for building wheels and runtime environment."}, "impact": {"score": 2, "evidence": "The error prevents the successful import of PyTorch modules when OneAPI versions are inconsistent."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue relates to dependency management between PyTorch and OneAPI libraries."}, "predicted_module": {"module": "dependency bug", "evidence": "The problem arises from version incompatibility between PyTorch and OneAPI, a dependency issue."}, "report_date": {"update": "2025-05-09", "evidence": "The issue was created on 2025-05-09."}, "last_update": {"update": "2025-05-09", "evidence": "No further updates mentioned in the issue."}}
{"issue_number": 1645, "issue_description": {"score": 0, "evidence": "Missing detailed issue description"}, "error_message": {"score": 0, "evidence": "No error message provided"}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps provided"}, "software_version": {"score": 0, "evidence": "No version information provided"}, "platform": {"score": 0, "evidence": "No platform information provided"}}, "reporter": "mengfei25", "assignee": "mengfei25", "resolution": {"score": 0, "evidence": "No resolution provided"}, "root_cause": {"score": 0, "evidence": "No root cause provided"}, "impact": {"score": 0, "evidence": "No impact information provided"}, "state": "open", "labeled_module": {"module": "N/A", "evidence": "Labels are N/A"}, "predicted_module": {"module": "UT", "evidence": "Issue relates to test cases and comparisons"}, "report_date": {"update": "2025-05-08", "evidence": "Issue created at 2025-05-08 02:53:07+00:00"}, "last_update": {"update": "2025-05-08", "evidence": "Issue created at 2025-05-08 02:53:07+00:00"}}
{"issue_number": 1641, "issue_description": {"score": 0, "evidence": "The issue description is missing as the body only mentions the title and no specific details about the issue, error, or impact."}, "error_message": {"score": 0, "evidence": "No error message is provided in the issue."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps are provided in the issue."}, "software_version": {"score": 0, "evidence": "No software version is specified in the issue."}, "platform": {"score": 0, "evidence": "No platform information is provided in the issue."}}, "reporter": "xytintel", "assignee": "chunhuanMeng", "resolution": {"score": 0, "evidence": "No resolution information is provided in the issue."}, "root_cause": {"score": 0, "evidence": "No root cause is mentioned in the issue."}, "impact": {"score": 0, "evidence": "No impact of the issue is described in the issue."}, "state": "open", "labeled_module": {"module": "N/A", "evidence": "The issue has no labels assigned, so the module cannot be determined from the labels."}, "predicted_module": {"module": "N/A", "evidence": "The issue description does not provide enough information to predict a module."}, "report_date": {"update": "2025-05-07", "evidence": "The issue was created on 2025-05-07 07:00:40+00:00."}, "last_update": {"update": "2025-05-15", "evidence": "No information about the last update is provided beyond the creation date."}}
{"issue_number": 1636, "issue_description": {"score": 2, "evidence": "The issue describes three FSDP test cases failing with specific assertion errors, including 'Tensor-likes are not close!' and 'Scalars are not equal!'."}, "error_message": {"score": 2, "evidence": "The error messages include 'AssertionError: Tensor-likes are not close!' and 'AssertionError: Scalars are not equal!'."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Reproduce command provided: pytest -vs test/distributed/_composable/fsdp/test_fully_shard_compile.py -k case_name(like:test_transformer_backend_inductor_fullgraph_True)[rank1]"}, "software_version": {"score": 2, "evidence": "PyTorch version from the path: 2025_ww17, and Python version 3.10."}, "platform": {"score": 2, "evidence": "conda environment and specific file paths indicating a Linux-based system."}}, "reporter": "daisyden", "assignee": "zhangxiaoli73", "resolution": {"score": 0, "evidence": "No information provided on resolution."}, "root_cause": {"score": 0, "evidence": "No information provided on root cause."}, "impact": {"score": 2, "evidence": "Impact is significant as it affects FSDP functionality, which is critical for distributed training."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "Issue labeled as 'bugmodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "Based on the test cases related to FSDP and the error messages, the issue likely pertains to the distributions module."}, "report_date": {"update": "2025-05-06", "evidence": "Issue created at 2025-05-06 09:03:42+00:00."}, "last_update": {"update": "2025-05-06", "evidence": "No further updates mentioned."}}
{"issue_number": 1634, "issue_description": {"score": 2, "evidence": "The issue is about a failed test in the distributed tensor parallelism module, specifically the test_linear_row_wise_parallel test case. The error indicates that the weights are not equal between the distributed and non-distributed versions, which is a clear and concise description of the problem."}, "error_message": {"score": 2, "evidence": "The error message states: 'AssertionError: Tensor-likes are not close! ... weight not equal between dist and non-dist'. This is specific and provides a clear indication of where the failure occurred."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes a note on how to execute the test, but it does not provide a detailed step-by-step guide. The user is instructed to run a specific command, which is somewhat helpful but not comprehensive."}, "software_version": {"score": 0, "evidence": "The issue does not specify the version of PyTorch or any other relevant software being used."}, "platform": {"score": 0, "evidence": "No platform information is provided, such as operating system or hardware details."}}, "reporter": "daisyden", "assignee": "syedshahbaaz", "resolution": {"score": 0, "evidence": "No information is provided about any attempts to resolve the issue or potential solutions."}, "root_cause": {"score": 0, "evidence": "The issue does not delve into possible causes of the failure, leaving the root cause unclear."}, "impact": {"score": 1, "evidence": "The issue affects the distributed tensor parallelism functionality, which could impact the performance and correctness of parallelized models. However, the description is somewhat vague on the extent of the impact."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under 'bugmodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "The test case is related to distributed tensor parallelism, which falls under the distributions module."}, "report_date": {"update": "2025-05-06", "evidence": "The issue was created on 2025-05-06."}, "last_update": {"update": "2025-05-07", "evidence": "The issue was created on 2025-05-06 and there is no indication of any subsequent updates."}}
{"issue_number": 1626, "issue_description": {"score": 2, "evidence": "The issue is about a failed opcheck when using a C++ extension with XPU devices. The error message indicates that autograd registration is not implemented for devices other than CPU/CUDA, specifically XPU."}, "error_message": {"score": 2, "evidence": "The error message is: NotImplementedError: autograd_registration_check: NYI devices other than CPU/CUDA, got {'xpu'}"}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides detailed steps to reproduce the problem, including setting up a conda environment, installing nightly wheels, cloning the repository, building, and running the test. The steps are clear and executable via shell commands."}, "software_version": {"score": 2, "evidence": "The versions include PyTorch 2.8.0.dev20250427+xpu, Ubuntu 24.10, Python 3.10.17, and other relevant libraries."}, "platform": {"score": 2, "evidence": "The platform is Ubuntu 24.10 on an Intel Core i5-9600K CPU with oneAPI support."}}, "reporter": "ZhaoqiongZ", "assignee": "dvrogozh", "resolution": {"score": 0, "evidence": "No information is provided about the resolution."}, "root_cause": {"score": 0, "evidence": "No specific root cause is identified, but it appears to be related to missing support for XPU in autograd registration checks."}, "impact": {"score": 0, "evidence": "The issue blocks the successful registration of autograd for XPU devices, preventing proper testing and validation of the custom C++ extension."}, "state": "open", "labeled_module": {"module": "N/A", "evidence": "The issue does not have any labels assigned."}, "predicted_module": {"module": "UT", "evidence": "The issue relates to test cases (opcheck) failing during the testing phase, which falls under the Unit Testing (UT) category."}, "report_date": {"update": "2025-04-29", "evidence": "The issue was created on 2025-04-29."}, "last_update": {"update": "2025-05-07", "evidence": "No further updates are mentioned beyond the initial report."}}
{"issue_number": 1624, "issue_description": {"score": 0, "evidence": "Missing information about the issue description."}, "error_message": {"score": 0, "evidence": "Missing information about error messages."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "Missing reproduce steps."}, "software_version": {"score": 0, "evidence": "Missing software version information."}, "platform": {"score": 0, "evidence": "Missing platform information."}}, "reporter": "RUIJIEZHONG66166", "assignee": "", "resolution": {"score": 0, "evidence": "Missing resolution information."}, "root_cause": {"score": 0, "evidence": "Missing root cause information."}, "impact": {"score": 0, "evidence": "Missing impact information."}, "state": "open", "labeled_module": {"module": "N/A", "evidence": "Labels are N/A."}, "predicted_module": {"module": "distributions", "evidence": "The issue involves test cases related to distributed training and FSDP (Fully Sharded Data Parallel)."}, "report_date": {"update": "2025-04-29", "evidence": "The issue was created on 2025-04-29."}, "last_update": {"update": "2025-04-29", "evidence": "No further updates mentioned."}}
{"issue_number": 1623, "issue_description": {"score": 1, "evidence": "The issue describes low scaling efficiency in distributed training scenarios but lacks a clear, concise problem statement."}, "error_message": {"score": 0, "evidence": "No specific error message is provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No clear reproduce steps are provided, though test commands are linked."}, "software_version": {"score": 1, "evidence": "PyTorch version 2.8.0a0+gite558eaa is specified along with the OS and hardware details."}, "platform": {"score": 1, "evidence": "Platform details include OS, CPU, and system configuration."}}, "reporter": "zxd1997066", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified or discussed."}, "impact": {"score": 0, "evidence": "No specific impact is described beyond the performance metrics provided."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "Issue label is 'bugmodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "Based on the label and test case classifications involving distributed training."}, "report_date": {"update": "2025-04-28", "evidence": "Issue created on 2025-04-28."}, "last_update": {"update": "2025-04-28", "evidence": "No subsequent updates mentioned."}}
{"issue_number": 1618, "issue_description": {"score": 2, "evidence": "The issue description provides a detailed bug report including the error message, test cases, and stack trace."}, "error_message": {"score": 2, "evidence": "The error message clearly states the problem: 'torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped' and provides the stack trace."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "Reproduce steps are mentioned but not clearly detailed. The user is instructed to run a specific test but not given explicit commands."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0a0+gite558eaa is provided, along with the platform information."}, "platform": {"score": 2, "evidence": "OS, CPU, and other hardware details are provided."}}, "reporter": "zxd1997066", "assignee": "guangyey", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided."}, "impact": {"score": 1, "evidence": "The issue impacts distributed training tests, particularly in FSDP, but the full impact is not detailed."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The label 'bugmodule: distributions' is assigned."}, "predicted_module": {"module": "distributions", "evidence": "The test cases are related to distributed training under the 'distributions' category."}, "report_date": {"update": "2025-04-28", "evidence": "The issue was created on 2025-04-28."}, "last_update": {"update": "2025-05-19", "evidence": "The issue was created on 2025-04-28 and there's no mention of any updates."}}


{"issue_number": 1614, "issue_description": {"score": 2, "evidence": "The issue describes a performance drop in models when using oneDNN v3.8-rc compared to v3.7.1, specifically mentioning BartForCausalLM with a 70% drop."}, "error_message": {"score": 2, "evidence": "No specific error message is mentioned in the comments."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides Python commands to reproduce the issue, including using specific parameters with `benchmarks/dynamo/huggingface.py`."}, "software_version": {"score": 2, "evidence": "Mentions PyTorch version is not explicitly stated, but the backend used is `inductor` and the issue is related to oneDNN v3.8-rc."}, "platform": {"score": 2, "evidence": "The issue is observed on XPU hardware."}}, "reporter": "mengfei25", "assignee": "LuFinch", "resolution": {"score": 1, "evidence": "The comment mentions that upgrading the driver to `25.05.32567` could fix the issue, but it does not provide a detailed resolution plan or root cause analysis."}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 2, "evidence": "The performance drop is significant (70%) and affects specific models like BartForCausalLM, impacting the efficiency of machine learning tasks."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue relates to model performance in Hugging Face's transformers, specifically BartForCausalLM."}, "predicted_module": {"module": "transformers", "evidence": "The test case involves a transformer model, BartForCausalLM, indicating the issue is likely in the transformers module."}, "report_date": {"update": "2025-04-25", "evidence": "The issue was created on 2025-04-25."}, "last_update": {"update": "2025-04-29", "evidence": "No further updates mentioned beyond the initial report."}}
{"issue_number": 1612, "issue_description": {"score": 2, "evidence": "The issue title and body provide detailed information about the bug, including the error message and the test case."}, "error_message": {"score": 2, "evidence": "RuntimeError: could not create a primitive descriptor for the matmul primitive with v3.8-rc"}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue provides the commands used to run the test, but the steps are not clearly outlined in a concise manner."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned."}, "platform": {"score": 2, "evidence": "The platform is identified as a Linux system with specific Python and pytest versions."}}, "reporter": "mengfei25", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects matrix multiplication operations in PyTorch using oneDNN, potentially causing runtime errors in models relying on these operations."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled under hw: PVCmodule: UTregression, indicating it relates to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The issue involves a test case (test_compare_cpu_addmm_xpu_float32) which suggests it is related to unit tests."}, "report_date": {"update": "2025-04-25", "evidence": "The issue was created on 2025-04-25 08:21:19+00:00."}, "last_update": {"update": "2025-05-08", "evidence": "The issue was created on 2025-04-25 08:21:19+00:00 and has no further updates mentioned."}}
{"issue_number": 1606, "issue_description": {"score": 2, "evidence": "The issue describes a performance regression when using oneDNN v3.8 compared to v3.7.1 in PyTorch with XPU devices. The reporter provides benchmark results showing a significant drop in performance, which is clear and concise."}, "error_message": {"score": 0, "evidence": "No specific error message is provided, only benchmark performance data."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes a Python command to reproduce the issue, which is clear and concise. `python benchmarks/dynamo/huggingface.py --performance --amp --amp-dtype bfloat16 -d xpu -n10 --inference  --only GPT2ForSequenceClassification --backend=inductor`"}, "software_version": {"score": 2, "evidence": "PyTorch version is specified as release/2.7."}, "platform": {"score": 2, "evidence": "The issue mentions XPU devices and the use of rolling drivers, which provides platform-specific information."}}, "reporter": "mengfei25", "assignee": "LuFinch", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided."}, "impact": {"score": 2, "evidence": "The performance drop is significant, affecting the efficiency of models running on XPU devices, which impacts the overall performance of PyTorch for users relying on XPU hardware."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under E2E performance hw: PVC regression, which suggests it relates to hardware performance, likely in the distributions module."}, "predicted_module": {"module": "distributions", "evidence": "The issue involves performance regression with oneDNN and XPU hardware, which falls under the distributions module as it relates to hardware-specific optimizations and performance."}, "report_date": {"update": "2025-04-24", "evidence": "The issue was created on 2025-04-24."}, "last_update": {"update": "2025-04-24", "evidence": "No further updates or comments are provided beyond the initial report."}}
{"issue_number": 1605, "issue_description": {"score": 2, "evidence": "The issue is about an assertion error in the test_fully_shard_memory test, which is part of the distributed testing in PyTorch. The error message indicates that the memory usage is higher than expected, which could be related to issues in how memory is managed in fully sharded data parallel training."}, "error_message": {"score": 2, "evidence": "The error is an AssertionError stating that 177 is not less than or equal to 163.904256, which suggests a memory allocation issue during training."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides a command to reproduce the test: `python test/distributed/_composable/fsdp/test_fully_shard_memory.py TestFullyShardMemory.test_fully_shard_training_memory`."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0a0+git624be3a is specified, along with the OS and other relevant versions."}, "platform": {"score": 2, "evidence": "The OS is SUSE Linux Enterprise Server 15 SP4 (x86_64), and the CPU information is detailed, which is essential for reproduction."}}, "reporter": "ratnampa", "assignee": "ratnampa", "resolution": {"score": 0, "evidence": "No information is provided about the resolution."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause."}, "impact": {"score": 2, "evidence": "The issue affects the memory management in distributed training, potentially causing failures in training jobs due to unexpected memory usage."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under 'bugmodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "The test case is related to fully sharded memory, which falls under the distributed training module."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was created on 2025-04-23 23:35:02+00:00."}, "last_update": {"update": "2025-05-16", "evidence": "The issue was created on 2025-04-23 and has no further updates mentioned."}}
{"issue_number": 1604, "issue_description": {"score": 2, "evidence": "The issue describes a problem with the test case test_bernoulli hanging due to batch_isend_irecv() async P2P ops."}, "error_message": {"score": 1, "evidence": "The comments do not mention any error messages related to the issue."}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 2, "evidence": "PyTorch version: 2.8.0a0+git624be3a, CUDA used to build PyTorch: None, OS: SUSE Linux Enterprise Server 15 SP4 (x86_64), Python version: 3.10.16"}, "platform": {"score": 2, "evidence": "x86_64, SUSE Linux, Intel Xeon CPU"}}, "reporter": "ratnampa", "assignee": "ratnampa", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "labels of the issue are bugmodule: distributions"}, "predicted_module": {"module": "distributions", "evidence": "test case is in test.distributed.tensor.test_experimental_ops.DistOtherOpsTest.test_bernoulli which relates to distributed operations and the issue mentions batch_isend_irecv() which is part of the distributed communication."}, "report_date": {"update": "2025-04-23", "evidence": "created at 2025-04-23 23:21:08+00:00"}, "last_update": {"update": "2025-05-16", "evidence": "created at 2025-04-23 23:21:08+00:00"}}
{"issue_number": 1597, "issue_description": {"score": 2, "evidence": "The issue is titled 'Implement aten::_linalg_solve_ex.result on xpu' and the body provides context about the operation falling back to CPU and its use in comfyUI text to video generation."}, "error_message": {"score": 0, "evidence": "No error message is provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps are provided."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "jianyizh", "assignee": "", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is mentioned."}, "impact": {"score": 2, "evidence": "The issue mentions that the operation is used in comfyUI text to video generation, indicating its importance."}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "The issue is labeled with 'module: OP Impl.'"}, "predicted_module": {"module": "OP impl", "evidence": "The issue focuses on implementing a specific PyTorch operation on XPU, which falls under OP implementation."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was created on 2025-04-23 03:17:19+00:00."}, "last_update": {"update": "2025-04-23", "evidence": "The issue was created on 2025-04-23 03:17:19+00:00 and there is no mention of any updates."}}

{"issue_number": 1592, "issue_description": {"score": 2, "evidence": "The issue description mentions an AssertionError related to tensor-like objects not being close, providing details about the test case and the versions of PyTorch and other dependencies."}, "error_message": {"score": 2, "evidence": "The error message is clear: 'AssertionError:Tensor-likes are not close!' with details about the mismatched elements and their indices."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue does not provide explicit steps to reproduce but mentions the test case and environment variables that affect the outcome."}, "software_version": {"score": 2, "evidence": "PyTorch version, CUDA, and other relevant versions are provided in detail."}, "platform": {"score": 2, "evidence": "OS, CPU, and other hardware details are included."}}, "reporter": "PenghuiCheng", "assignee": "zxd1997066", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue."}, "impact": {"score": 1, "evidence": "The issue affects a regression in the test case, implying it could impact distributed training functionality."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The label is explicitly provided as 'bugmodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "The test case relates to distributed operations and the error occurs in a distributed math operation test."}, "report_date": {"update": "2025-04-21", "evidence": "The issue was created on this date."}, "last_update": {"update": "2025-04-22", "evidence": "No further updates are mentioned beyond the creation date."}}
{"issue_number": 1587, "issue_description": {"score": 2, "evidence": "The issue title is 'Keep track on the latest CUDA op impl', and the body provides specific PR links related to CUDA optimizations and gather operations, which is concise and clear."}, "error_message": {"score": 0, "evidence": "No error message is provided in the issue."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps are provided."}, "software_version": {"score": 0, "evidence": "No pytorch version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "xytintel", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is mentioned."}, "impact": {"score": 0, "evidence": "No specific impact is described."}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "The label is kernel_optimization, which relates to operation implementations."}, "predicted_module": {"module": "OP impl", "evidence": "The issue focuses on CUDA optimizations and gather operations, which are part of operation implementations."}, "report_date": {"update": "2025-04-21", "evidence": "The issue was created at 2025-04-21 02:17:08+00:00."}, "last_update": {"update": "2025-04-21", "evidence": "No further updates are mentioned."}}

{"issue_number": 1574, "issue_description": {"score": 2, "evidence": "The operator 'aten::_grouped_mm' is not currently implemented for the XPU device."}, "error_message": {"score": 2, "evidence": "NotImplementedError: The operator 'aten::_grouped_mm' is not currently implemented for the XPU device."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No steps provided to reproduce the issue."}, "software_version": {"score": 2, "evidence": "pytorch master and xpu-ops master"}, "platform": {"score": 2, "evidence": "XPU device."}}, "reporter": "githubsgi", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No information provided on how the issue can be resolved."}, "root_cause": {"score": 0, "evidence": "No information provided on the root cause of the issue."}, "impact": {"score": 0, "evidence": "No information on the impact of the issue provided."}, "state": "open", "labeled_module": {"module": "N/A", "evidence": "Labels are marked as N/A."}, "predicted_module": {"module": "N/A", "evidence": "No module could be predicted based on the description."}, "report_date": {"update": "2025-04-11", "evidence": "The issue was created at 2025-04-11 23:52:13+00:00."}, "last_update": {"update": "2025-04-17", "evidence": "The issue was created at 2025-04-11 23:52:13+00:00."}}

{"issue_number": 1556, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the error message and test case details."}, "error_message": {"score": 2, "evidence": "The error message 'NotImplementedError: Operator aten._scaled_dot_product_fused_attention_overrideable.default does not have a sharding strategy registered.' is specific and provides insight into the issue."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions the test case but does not provide explicit reproduce steps using Python, pytest, or shell commands."}, "software_version": {"score": 0, "evidence": "The pytorch version and platform information are not specified."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "PenghuiCheng", "assignee": "githubsgi", "resolution": {"score": 1, "evidence": "There is no information provided about the resolution process or timeline, so it's unclear how soon this issue might be addressed."}, "root_cause": {"score": 1, "evidence": "The issue mentions that a specific operator lacks a sharding strategy, which suggests that the root cause is related to the missing implementation for distributed tensor operations. However, the exact root cause is not explicitly detailed."}, "impact": {"score": 2, "evidence": "The issue affects the distributed tensor parallelism functionality, potentially breaking transformer models during training or inference."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under 'featuremodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "The issue relates to distributed tensor parallelism, which falls under the distributions module."}, "report_date": {"update": "2025-04-08", "evidence": "The issue was created on 2025-04-08."}, "last_update": {"update": "2025-05-14", "evidence": "No further updates are mentioned in the issue."}}
{"issue_number": 1555, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the error message and the test cases that failed."}, "error_message": {"score": 2, "evidence": "The error message 'RuntimeError: aten.add.Tensor: got mixed torch.Tensor and DTensor, need to convert all torch.Tensor to DTensor before calling distributed operators!' is clear and specific."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps are provided to reproduce the issue."}, "software_version": {"score": 0, "evidence": "No information about PyTorch version or platform details is provided."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "PenghuiCheng", "assignee": "githubsgi", "resolution": {"score": 0, "evidence": "No information about resolution steps is provided."}, "root_cause": {"score": 1, "evidence": "The root cause is identified as the mask tensor not being converted to a DTensor, but this is mentioned briefly in a comment."}, "impact": {"score": 2, "evidence": "The issue affects distributed tensor operations, potentially breaking models that use mixed tensor types in distributed settings."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled with 'featuremodule: distributions.'"}, "predicted_module": {"module": "distributions", "evidence": "The issue is related to distributed tensor operations, which fall under the distributions module."}, "report_date": {"update": "2025-04-08", "evidence": "The issue was created on 2025-04-08."}, "last_update": {"update": "2025-04-24", "evidence": "No further updates or comments are provided in the issue."}}
{"issue_number": 1554, "issue_description": {"score": 2, "evidence": "The issue describes encountering a PermissionError during multi-threaded compilation when multiple threads attempt to open the same file simultaneously."}, "error_message": {"score": 2, "evidence": "PermissionError: [Errno 13] Permission denied during multi-threaded compilation."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "Build pytorch with xpu pin b1c5462be0274cfc234ddb47d68166be28a47414"}, "software_version": {"score": 1, "evidence": "CMake version: 3.31.6, Python version: 3.10.16, Platform: Windows-10-10.0.20348-SP0"}, "platform": {"score": 2, "evidence": "Platform is Windows, as indicated by the label and the Python platform information."}}, "reporter": "chunhuanMeng", "assignee": "chunhuanMeng", "resolution": {"score": 1, "evidence": "The comments indicate that the regression code change has been reverted and that refactoring and relanding are needed, but the root cause is not clearly explained."}, "root_cause": {"score": 0, "evidence": "No root cause information provided."}, "impact": {"score": 0, "evidence": "No impact information provided."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under the 'os: Windows' label, which suggests it's related to the operating system dependency."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue involves a PermissionError during file operations, which is likely related to operating system-level file handling and concurrency."}, "report_date": {"update": "2025-04-08", "evidence": "The issue was created on 2025-04-08."}, "last_update": {"update": "2025-04-23", "evidence": "No further updates mentioned in the issue."}}
{"issue_number": 1551, "issue_description": {"score": 2, "evidence": "The issue is titled '[distributed] NotImplementedError: The operator 'symm_mem::fused_scaled_matmul_reduce_scatter' is not currently implemented for the XPU device.' and the body describes the error and the failing test cases."}, "error_message": {"score": 2, "evidence": "The error message is 'NotImplementedError: The operator 'symm_mem::fused_scaled_matmul_reduce_scatter' is not currently implemented for the XPU device.'"}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes the test cases that are failing, which can be used to reproduce the issue by running the specific test cases mentioned."}, "software_version": {"score": 2, "evidence": "The issue mentions 'test/distributed/tensor/parallel/test_micro_pipeline_tp.py', indicating it's part of the PyTorch codebase, but the specific PyTorch version is not explicitly mentioned."}, "platform": {"score": 2, "evidence": "The platform is Linux, as indicated by the 'platform linux' in the test logs."}}, "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": {"score": 1, "evidence": "Issue is open and needs further investigation."}, "root_cause": {"score": 1, "evidence": "The operator is not implemented for XPU device."}, "impact": {"score": 2, "evidence": "The issue affects multiple test cases related to distributed tensor parallelism, indicating it impacts the distributed training functionality."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue has the label 'featuremodule: distributions.'"}, "predicted_module": {"module": "distributions", "evidence": "The test cases involve distributed tensor operations, which fall under the distributions module."}, "report_date": {"update": "2025-04-07", "evidence": "The issue was created on 2025-04-07 08:26:40+00:00."}, "last_update": {"update": "2025-04-07", "evidence": "The issue was created on 2025-04-07 08:26:40+00:00 and there is no indication of further updates."}}
{"issue_number": 1550, "issue_description": {"score": 2, "evidence": "The issue describes a NotImplementedError for the operator 'aten::_scaled_mm.out' on the XPU device, with relevant test failures and logs provided."}, "error_message": {"score": 2, "evidence": "NotImplementedError: The operator 'aten::_scaled_mm.out' is not currently implemented for the XPU device."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes test failures from pytest, but specific Python, pytest, or shell commands are not explicitly provided."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version or platform information is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "PenghuiCheng", "assignee": "xytintel", "resolution": {"score": 1, "evidence": "The issue is open, so resolution steps are not yet available."}, "root_cause": {"score": 1, "evidence": "The problem is identified as a missing implementation, but the root cause is not detailed."}, "impact": {"score": 1, "evidence": "The issue affects distributed tensor parallelism tests, potentially impacting the functionality of distributed training in PyTorch."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue has the label 'bugmodule: distributions.'"}, "predicted_module": {"module": "distributions", "evidence": "The issue involves distributed tensor parallelism tests, which falls under the distributions module."}, "report_date": {"update": "2025-04-07", "evidence": "The issue was created on 2025-04-07."}, "last_update": {"update": "2025-05-07", "evidence": "No further updates or comments are present in the issue."}}
{"issue_number": 1549, "issue_description": {"score": 2, "evidence": "The issue title and body provide clear information about the problem with test failures related to 'fused_all_gather_scaled_matmul'."}, "error_message": {"score": 2, "evidence": "AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\\"}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps provided for reproduction."}, "software_version": {"score": 0, "evidence": "No information about PyTorch version or platform details."}, "platform": {"score": 0, "evidence": "No platform information provided."}}, "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": {"score": 0, "evidence": "No resolution information provided."}, "root_cause": {"score": 0, "evidence": "No root cause analysis provided."}, "impact": {"score": 1, "evidence": "The issue affects test cases related to distributed tensor operations, potentially impacting the functionality of distributed training."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "Label explicitly states 'featuremodule: distributions.'"}, "predicted_module": {"module": "distributions", "evidence": "Test cases are within the distributed tensor parallelism framework."}, "report_date": {"update": "2025-04-07", "evidence": "Issue creation date is provided."}, "last_update": {"update": "2025-04-07", "evidence": "No information on updates after the initial creation date."}}
{"issue_number": 1548, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the error message and test cases affected."}, "error_message": {"score": 2, "evidence": "The error message 'AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\\'2_inference\\']\\"}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No detailed steps provided to reproduce the issue."}, "software_version": {"score": 0, "evidence": "No information about the PyTorch version or platform provided."}, "platform": {"score": 0, "evidence": "No platform information provided."}}, "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": {"score": 0, "evidence": "No resolution information provided."}, "root_cause": {"score": 0, "evidence": "No root cause information provided."}, "impact": {"score": 1, "evidence": "The issue affects multiple test cases related to distributed tensor parallelism, indicating potential impact on distributed training and model parallelism features."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue has the label 'featuremodule: distributions.'"}, "predicted_module": {"module": "distributions", "evidence": "The issue relates to distributed tensor operations, which fall under the distributions module."}, "report_date": {"update": "2025-04-07", "evidence": "The issue was created on 2025-04-07 08:05:24+00:00."}, "last_update": {"update": "2025-04-07", "evidence": "The issue was created on 2025-04-07 08:05:24+00:00 and is still open."}}
{"issue_number": 1547, "issue_description": {"score": 2, "evidence": "The issue title and body provide clear and concise information about the problem, including the error message and test cases that failed."}, "error_message": {"score": 2, "evidence": "The error message is specific: NotImplementedError: The operator 'symm_mem::fused_matmul_reduce_scatter' is not currently implemented for the XPU device."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes specific test cases that can be used to reproduce the issue, such as test_fuse_matmul_reduce_scatter_A_dims_2_scatter_dim_0."}, "software_version": {"score": 0, "evidence": "No specific version of PyTorch or other software is mentioned."}, "platform": {"score": 0, "evidence": "No specific platform or hardware information is provided."}}, "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": {"score": 0, "evidence": "No information is provided about potential solutions or resolutions."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue affects specific test cases, but the broader impact on the project is unclear."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under the 'featuremodule: distributions' category."}, "predicted_module": {"module": "distributions", "evidence": "The issue involves distributed tensor operations, which aligns with the 'distributions' module."}, "report_date": {"update": "2025-04-07", "evidence": "The issue was created on 2025-04-07."}, "last_update": {"update": "2025-04-07", "evidence": "The issue was created and has not been updated since."}}
{"issue_number": 1543, "issue_description": {"score": 2, "evidence": "The issue describes a memory allocation discrepancy between XPU and CUDA during fine-tuning, with XPU reserving 8GB more VRAM. The reporter provided detailed steps to reproduce, including cloning a repository, running a specific command, and modifying code to track memory usage. Logs and code changes are included to illustrate the problem, and the impact is clear as it affects model training efficiency and resource usage."}, "error_message": {"score": 2, "evidence": "The comments do not mention any error messages or specific issues encountered during the investigation."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides clear reproduce steps: cloning a specific branch, running a command with parameters, and code modifications. These steps are well-documented and executable."}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "airMeng", "assignee": "guangyey", "resolution": {"score": 2, "evidence": "Guangyey mentioned that a refactoring is in progress to align the XPU allocator with CUDA, with an expected completion before Q3. This indicates a clear resolution plan."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as differences between the XPU and CUDA allocators."}, "impact": {"score": 2, "evidence": "The issue impacts training efficiency and resource usage, as 8GB of extra VRAM reserved on XPU could lead to underutilization of GPU resources and increased costs. It could also cause training instability or failures on systems with limited VRAM."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The labels include 'Core' which relates to the core functionality of the XPU operations."}, "predicted_module": {"module": "Core", "evidence": "The issue relates to memory management in XPU operations, which falls under the core functionality of the library."}, "report_date": {"update": "2025-04-03", "evidence": "The issue was created on 2025-04-03."}, "last_update": {"update": "2025-05-07", "evidence": "The issue was created on 2025-04-03 and has not been updated since."}}
{"issue_number": 1536, "issue_description": {"score": 2, "evidence": "The issue describes a bug where the test_distributed_checkpoint.py test fails randomly with a status of atl_status: FAILURE. It includes details about the environment, build configuration, and error logs."}, "error_message": {"score": 2, "evidence": "The error message indicates a timeout and failure in CCL communication with specific warnings and exceptions related to FSDP and CCL."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions running pytest with specific test file and marks, but lacks detailed and clear reproduce steps."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0a0+git124ff16, build with 2025.0, and other environment details are provided."}, "platform": {"score": 2, "evidence": "Ubuntu 22.04.5 LTS, x86_64, Intel Xeon Platinum 8480+."}}, "reporter": "daisyden", "assignee": "ratnampa", "resolution": {"score": 0, "evidence": "No resolution information provided."}, "root_cause": {"score": 0, "evidence": "No root cause information provided."}, "impact": {"score": 1, "evidence": "The issue affects the distributed checkpointing functionality, which is critical for distributed training. Random failures could impact reliability and reproducibility of distributed training tasks."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under bugmodule: distributions."}, "predicted_module": {"module": "distributions", "evidence": "The test case is related to distributed checkpointing, which falls under the distributions module."}, "report_date": {"update": "2025-04-02", "evidence": "The issue was created on 2025-04-02."}, "last_update": {"update": "2025-05-14", "evidence": "No information about updates after creation date."}}
{"issue_number": 1535, "issue_description": {"score": 2, "evidence": "The issue describes a RuntimeError where process 0 terminates or times out during distributed testing."}, "error_message": {"score": 2, "evidence": "RuntimeError: Process 0 terminated or timed out after 300.09047198295593 seconds"}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions test cases but does not provide explicit steps to reproduce the issue."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0a0+git20d074c is provided along with CUDA and other environment details."}, "platform": {"score": 2, "evidence": "OS: Ubuntu 22.04.4 LTS (x86_64), CPU details, etc."}}, "reporter": "PenghuiCheng", "assignee": "ratnampa", "resolution": {"score": 2, "evidence": "The resolution involved updating to the latest oneCCL master branch, which resolved the failing test cases. The Gather OP issue was specific to the gold release branch and not present in the master branch."}, "root_cause": {"score": 2, "evidence": "The root cause was identified as differences in behavior between the oneCCL gold release branch and the latest master branch, particularly affecting the Gather OP functionality in distributed tests."}, "impact": {"score": 1, "evidence": "The issue affects distributed testing with timeouts, potentially impacting the reliability of distributed operations."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under the distributions module."}, "predicted_module": {"module": "distributions", "evidence": "The test cases are related to distributed object collectives."}, "report_date": {"update": "2025-04-02", "evidence": "Issue created at 2025-04-02 05:42:39+00:00."}, "last_update": {"update": "2025-05-13", "evidence": "No further updates mentioned."}}
{"issue_number": 1525, "issue_description": {"score": 2, "evidence": "The issue has a clear description of the bug, including the error message and the test cases that failed."}, "error_message": {"score": 2, "evidence": "The error message 'ValueError: trying to initialize the default process group twice!' is provided."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions the test cases that failed but does not provide explicit reproduce steps using Python, pytest, or shell commands."}, "software_version": {"score": 2, "evidence": "The PyTorch version and other relevant library versions are provided."}, "platform": {"score": 2, "evidence": "The OS, CPU, and other hardware information are included."}}, "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": {"score": 0, "evidence": "No information about the resolution is provided."}, "root_cause": {"score": 0, "evidence": "No information about the root cause is provided."}, "impact": {"score": 1, "evidence": "The issue affects multiple test cases in the distributed module but does not clearly state the broader impact."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled as 'bugmodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "The test cases are related to distributed functions, indicating the issue is within the distributed module."}, "report_date": {"update": "2025-04-01", "evidence": "The issue was created on 2025-04-01."}, "last_update": {"update": "2025-05-13", "evidence": "The issue was created and has no further updates mentioned."}}

{"issue_number": 1519, "issue_description": {"score": 2, "evidence": "The issue description provides clear information about the problem, including the test cases that fail and the steps to reproduce the issue."}, "error_message": {"score": 2, "evidence": "The error messages are specific and reproducible, such as the test cases failing and passing on different tiles."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Reproducible steps are provided, such as the specific command to run the tests."}, "software_version": {"score": 2, "evidence": "Python, PyTorch, and driver versions are specified."}, "platform": {"score": 2, "evidence": "Hardware and platform details are provided, including the specific GPU model and operating system."}}, "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No resolution information is provided in the issue."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue."}, "impact": {"score": 2, "evidence": "The issue affects specific test cases in the PyTorch and IPEX 2.7 versions, impacting the reliability of these tests."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled as a bug and involves test cases in test_pooling_xpu.py and test_ops_xpu.py, which are part of the unit tests."}, "predicted_module": {"module": "UT", "evidence": "The test cases mentioned are part of the unit tests for the pooling and common operations in PyTorch-xpu-ops."}, "report_date": {"update": "2025-03-27", "evidence": "The issue was created on 2025-03-27, providing clear date information."}, "last_update": {"update": "2025-05-14", "evidence": "No further updates are mentioned, so the last update is the same as the creation date."}}
{"issue_number": 1513, "issue_description": {"score": 2, "evidence": "The issue describes that the Inductor UT test cannot exit normally after completion, causing problems when adding cases to nightly tests."}, "error_message": {"score": 1, "evidence": "The comments do not provide enough information to determine the root cause or resolution steps for the issue. The discussion is limited and lacks clarity."}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "RUIJIEZHONG66166", "assignee": "RUIJIEZHONG66166", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 1, "evidence": "The issue impacts the addition of Inductor UT test cases to nightly tests, potentially delaying CI/CD processes."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled under component: UT."}, "predicted_module": {"module": "UT", "evidence": "The issue involves Inductor UT test failures, which directly relates to the UT module."}, "report_date": {"update": "2025-03-27", "evidence": "The issue was created on 2025-03-27."}, "last_update": {"update": "2025-04-17", "evidence": "No information about last update is provided beyond the creation date."}}
{"issue_number": 1512, "issue_description": {"score": 2, "evidence": "The issue describes the problem where the first run takes a long time on Windows for ARC/BMG, with specific timing details for different PyTorch versions and devices."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Steps include creating a clean conda environment, installing specific PyTorch wheels, and running a sample script. The script uses `import torch; a = torch.randn(10,1).to('xpu'); print(a)`."}, "software_version": {"score": 2, "evidence": "Mentions PyTorch versions v2.6, v2.7.0_0312, v2.7_0326, and platform Windows."}, "platform": {"score": 2, "evidence": "Issue is specific to Windows."}}, "reporter": "ZhaoqiongZ", "assignee": "LuFinch", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 2, "evidence": "The issue affects the performance of the first run on Windows for specific PyTorch versions, causing delays which could hinder user experience and productivity."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue relates to performance on specific devices (ARC/BMG) under Windows, which could be linked to the distributions module handling different hardware or OS configurations."}, "predicted_module": {"module": "distributions", "evidence": "The problem seems to stem from the way distributions are handled during the first run on Windows, affecting initialization or setup times."}, "report_date": {"update": "2025-03-26", "evidence": "The issue was created on 2025-03-26."}, "last_update": {"update": "2025-04-21", "evidence": "No further updates mentioned beyond the initial report."}}
{"issue_number": 1510, "issue_description": {"score": 2, "evidence": "Issue title: Some test cases will be hang on BMG Ubuntu. Body describes test failures in test/xpu with specific test cases hanging, including test_linspace_xpu_complex128. Logs show multiple tests failing after one failure, indicating a systemic issue. Reproducibility steps are not explicitly provided, but the issue is reproducible on BMG Ubuntu with the given setup."}, "error_message": {"score": 2, "evidence": "Test logs show failed tests such as test_dtypes__refs_nn_functional_prelu_xpu with no specific error message but indicate that tests hang or fail after the first failure."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "Issue does not explicitly provide steps, but the environment and setup can be inferred from the provided PyTorch version and platform info. Reproduction might involve running tests on BMG Ubuntu with specific configurations."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.7.0+xpu is specified, along with environment details including Ubuntu 24.10 and specific Python version."}, "platform": {"score": 2, "evidence": "OS: Ubuntu 24.10, Device: BMGOS, Driver version: 24.52.32224."}}, "reporter": "mengfei25", "assignee": "Stonepia", "resolution": {"score": 2, "evidence": "The comments include concise information about the resolution and root cause."}, "root_cause": {"score": 2, "evidence": "The root cause is clearly identified and explained concisely in the comments."}, "impact": {"score": 1, "evidence": "Multiple test cases fail, affecting the reliability of PyTorch XPU operations on BMG Ubuntu, potentially impacting the functionality of neural network operations and other computations."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "Labels include 'UT' which likely stands for Unit Tests."}, "predicted_module": {"module": "UT", "evidence": "Issue relates to test cases hanging, pointing towards unit testing framework or related modules."}, "report_date": {"update": "2025-03-26", "evidence": "Issue created at 2025-03-26 03:53:44+00:00."}, "last_update": {"update": "2025-04-23", "evidence": "No further updates mentioned in the issue."}}
{"issue_number": 1509, "issue_description": {"score": 2, "evidence": "The issue describes a bug where x.sum().backward() results in a RuntimeError: Data corruption detected. The description is concise and clear."}, "error_message": {"score": 2, "evidence": "RuntimeError: Data corruption detected is provided as the error message, which is clear and specific."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The reproduce command is provided as pytest -v test/distributed/test_multi_threaded_pg.py, which is concise and clear."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0a0+gitfd4037d and other relevant library versions are clearly specified."}, "platform": {"score": 2, "evidence": "OS: Ubuntu 22.04.5 LTS (x86_64) and CPU information are provided in detail."}}, "reporter": "PenghuiCheng", "assignee": "syedshahbaaz", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 1, "evidence": "The issue is linked to multi-threading and oneCCL support but lacks a detailed root cause explanation."}, "impact": {"score": 2, "evidence": "The issue affects distributed operations, which is critical for parallel and distributed computing tasks."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under bugmodule: distributions."}, "predicted_module": {"module": "distributions", "evidence": "The test case involves distributed operations, as indicated by the test_multi_threaded_pg.py file and the error occurring during backward pass in a distributed context."}, "report_date": {"update": "2025-03-25", "evidence": "The issue was created on 2025-03-25 14:28:04+00:00."}, "last_update": {"update": "2025-04-23", "evidence": "No further updates are mentioned, so the last update is considered the creation date."}}

{"issue_number": 1506, "issue_description": {"score": 2, "evidence": "The issue describes that several E2E models are failing accuracy tests on BMG/LNL. The reporter provided specific commands and versions, making the description clear and concise."}, "error_message": {"score": 0, "evidence": "No specific error messages are provided in the issue."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes detailed Python commands for running the benchmarks, which can be used to reproduce the issue."}, "software_version": {"score": 2, "evidence": "PyTorch version and torch-xpu-ops commit are specified, along with driver and Conda information."}, "platform": {"score": 2, "evidence": "The issue mentions Windows OS, driver version, and Python version."}}, "reporter": "libohao1201", "assignee": "Stonepia", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified or discussed in the issue."}, "impact": {"score": 0, "evidence": "The impact of the issue is not explicitly mentioned, though failing models suggest potential performance or functionality issues."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue involves Hugging Face models and transformers, which relate to the transformers module."}, "predicted_module": {"module": "transformers", "evidence": "The issue includes tests for DebertaV2ForQuestionAnswering and other transformer-based models, indicating the transformers module is relevant."}, "report_date": {"update": "2025-03-25", "evidence": "The issue was created on 2025-03-25."}, "last_update": {"update": "2025-03-25", "evidence": "No further updates are mentioned beyond the initial report."}}
{"issue_number": 1505, "issue_description": {"score": 2, "evidence": "The issue describes that 14 Timm models failed accuracy tests on ARC-WSL using torch-xpu-ops with specific configurations. The description provides details about the test commands and the models involved, making it clear and concise."}, "error_message": {"score": 0, "evidence": "No specific error messages are provided in the issue description."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes the exact bash commands used to run the tests, which can be used to reproduce the issue. The steps are clear and concise."}, "software_version": {"score": 2, "evidence": "The versions of PyTorch, torch-xpu-ops, and other relevant software are provided in the issue, making it clear how to reproduce the issue."}, "platform": {"score": 2, "evidence": "The platform details, including the machine name, OS, and driver versions, are provided, which are essential for reproduction."}}, "reporter": "libohao1201", "assignee": "", "resolution": {"score": 0, "evidence": "No information is provided about the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 2, "evidence": "The impact is significant as 14 models failing accuracy tests could indicate a regression in performance or functionality, affecting the reliability of the framework."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue involves testing with bfloat16 precision, which is related to quantization."}, "predicted_module": {"module": "quant", "evidence": "The use of bfloat16 in the test commands suggests that the issue is related to quantization."}, "report_date": {"update": "2025-03-25", "evidence": "The issue was created on 2025-03-25, which is noted in the issue details."}, "last_update": {"update": "2025-03-25", "evidence": "No further updates are provided beyond the initial report."}}
{"issue_number": 1504, "issue_description": {"score": 2, "evidence": "The issue describes test failures in FSDP with accuracy gaps on XELink. It includes specific test cases and error messages."}, "error_message": {"score": 2, "evidence": "Multiple test failures with assertion errors and detailed numerical discrepancies."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Commands to run the tests are provided, such as `python test_fsdp_fine_tune.py -k test_hooks_multi_traversal_xpu`."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned."}, "platform": {"score": 2, "evidence": "Tests were run on PVC 1550 and 1100 machines with specific ZE_AFFINITY_MASK settings."}}, "reporter": "daisyden", "assignee": "zhangxiaoli73", "resolution": {"score": 2, "evidence": "The issue has been identified as a regression from oneCCL, and a corresponding issue has been created, indicating a clear resolution path."}, "root_cause": {"score": 2, "evidence": "The issue is traced back to oneCCL, showing a clear understanding of the root cause."}, "impact": {"score": 2, "evidence": "The issue affects multiple test cases in FSDP, causing test failures in distributed training scenarios."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under 'bugmodule: distributions'."}, "predicted_module": {"module": "distributions", "evidence": "The test cases are under FSDP, which relates to distributed training."}, "report_date": {"update": "2025-03-25", "evidence": "The issue was created on 2025-03-25."}, "last_update": {"update": "2025-03-27", "evidence": "No further updates mentioned beyond the initial report."}}
{"issue_number": 1502, "issue_description": {"score": 2, "evidence": "The issue describes that WSL crashes when running torchbench. The bug is clearly stated with the test command provided."}, "error_message": {"score": 0, "evidence": "No specific error message is provided."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The user provided detailed steps including changing iterations in a script and running a specific command."}, "software_version": {"score": 2, "evidence": "PyTorch version and torch-xpu-ops commit are specified."}, "platform": {"score": 2, "evidence": "Windows 11 machine, WSL driver details, and Python version are provided."}}, "reporter": "libohao1201", "assignee": "", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is discussed."}, "impact": {"score": 1, "evidence": "The issue affects WSL environments when running torchbench, but the full impact is not detailed."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue relates to WSL crashes when using torchbench, which may involve dependencies or interactions between different components."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue involves a crash in the WSL environment when running a PyTorch benchmark, suggesting potential dependency issues or compatibility problems between the components."}, "report_date": {"update": "2025-03-25", "evidence": "The issue was created on 2025-03-25."}, "last_update": {"update": "2025-03-25", "evidence": "No further updates are mentioned."}}
{"issue_number": 1500, "issue_description": {"score": 2, "evidence": "The issue describes a problem with the operator 'aten::_slow_conv2d_forward' not being implemented for the XPU device, with a provided code snippet and environment details."}, "error_message": {"score": 2, "evidence": "The error message indicates that the specific convolution operation is not implemented for XPU."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The code snippet provided includes all necessary steps to reproduce the issue, including importing libraries, loading the model, and processing an image."}, "software_version": {"score": 2, "evidence": "The PyTorch version and other relevant libraries' versions are clearly listed."}, "platform": {"score": 2, "evidence": "Detailed platform information, including OS, CPU, and other system details, is provided."}}, "reporter": "kaixuanliu", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No information provided about the resolution."}, "root_cause": {"score": 0, "evidence": "No information provided about the root cause."}, "impact": {"score": 2, "evidence": "The issue impacts the functionality of the model when using XPU, potentially affecting performance and usability."}, "state": "open", "labeled_module": {"module": "N/A", "evidence": "No specific labels indicating a module were provided."}, "predicted_module": {"module": "transformers", "evidence": "The issue involves using a transformers model, specifically AutoModelForImageClassification, which is part of the transformers library."}, "report_date": {"update": "2025-03-24", "evidence": "The issue was created on 2025-03-24."}, "last_update": {"update": "2025-03-24", "evidence": "No further updates mentioned beyond the initial report."}}
{"issue_number": 1498, "issue_description": {"score": 2, "evidence": "The issue describes that extended UTs failed with a specific RuntimeError: Native API failed. Native API returns: 29 (UR_RESULT_ERROR_INVALID_KERNEL_NAME). The error message clearly indicates an issue with the Native API returning an invalid kernel name, which is concise and clear."}, "error_message": {"score": 2, "evidence": "The error message is 'RuntimeError: Native API failed. Native API returns: 29 (UR_RESULT_ERROR_INVALID_KERNEL_NAME)'. This is a specific and clear error message indicating a problem with the Native API and the kernel name."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides detailed steps to reproduce, including setting environment variables and running pytest commands. These steps are clear and reproducible, making it easy to identify the problem."}, "software_version": {"score": 2, "evidence": "The versions section provides detailed information about PyTorch, torch-xpu-ops, driver version, and Conda. This information is comprehensive and clear."}, "platform": {"score": 2, "evidence": "The issue mentions the machine is running on ARC - Win11, and the platform details are provided, including WSL information and driver versions."}}, "reporter": "libohao1201", "assignee": "gaopengff", "resolution": {"score": 0, "evidence": "No information is provided about the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue affects the extended UTs, which are part of the testing suite. While it's clear that the tests are failing, the broader impact on the project or users is not explicitly detailed, but it can be inferred that it affects the reliability of the codebase."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue has the label 'module: UT', which clearly indicates that the problem is related to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The issue is about failed unit tests, specifically extended UTs, which directly relates to the UT module."}, "report_date": {"update": "2025-03-24", "evidence": "The issue was created on 2025-03-24 03:51:05+00:00, which is clearly stated in the issue details."}, "last_update": {"update": "2025-03-24", "evidence": "As of the information provided, the issue was created on 2025-03-24, and there's no indication of any subsequent updates or changes."}}
{"issue_number": 1496, "issue_description": {"score": 2, "evidence": "The issue describes an application error when running E2E inductor on LNL, with a specific memory access problem."}, "error_message": {"score": 2, "evidence": "The error message indicates a memory read issue at a specific memory address."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "No specific steps are provided, but the issue mentions running E2E inductor on LNL."}, "software_version": {"score": 2, "evidence": "Includes specific versions of PyTorch, torch-xpu-ops, and the driver."}, "platform": {"score": 2, "evidence": "The issue specifies Windows, LNL, and Inductor."}}, "reporter": "libohao1201", "assignee": "", "resolution": {"score": 2, "evidence": "The comment suggests a possible resolution by switching back to re-test after a fix is available."}, "root_cause": {"score": 2, "evidence": "The comment identifies a potential root cause related to the driver enabling the overcommit feature making the writing invalid."}, "impact": {"score": 2, "evidence": "The error causes the application to crash, affecting E2E inductor functionality."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The label explicitly mentions dependency bug."}, "predicted_module": {"module": "dependency bug", "evidence": "Based on the labels and error context, the issue likely relates to driver or dependency issues."}, "report_date": {"update": "2025-03-21", "evidence": "The issue was created on this date."}, "last_update": {"update": "2025-03-21", "evidence": "No further updates mentioned."}}
{"issue_number": 1483, "issue_description": {"score": 2, "evidence": "The issue is titled 'Sam model got Segmentation fault on Rolling but passed on LTS' and the body provides a detailed description including error messages and version information."}, "error_message": {"score": 2, "evidence": "The issue includes a detailed error message showing a segmentation fault during GPU testing."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides a bash command to reproduce the issue: `python benchmarks/dynamo/torchbench.py --performance --float16 -d xpu -n10 --inference --only sam --backend=inductor --cold-start-latency`"}, "software_version": {"score": 2, "evidence": "The versions section specifies PyTorch version as release/2.7 RC2 and driver versions for Rolling and LTS."}, "platform": {"score": 2, "evidence": "The platform information includes the OS as Ubuntu and hardware as PVC."}}, "reporter": "mengfei25", "assignee": "jianyizh", "resolution": {"score": 2, "evidence": "The issue was addressed by removing the function call _sfdp_init(), and fixes were implemented in specific versions of oneDNN. The resolution is clear and actionable."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as being related to the _sfdp_init() function call, which was causing NaN issues. The comments also reference specific tickets and versions where the root cause was addressed."}, "impact": {"score": 2, "evidence": "The issue affects the Sam model's functionality on Rolling but works on LTS, indicating a regression."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue involves the Sam model, which is part of the transformers module."}, "predicted_module": {"module": "transformers", "evidence": "The test case involves the Sam model, which is a transformer-based model."}, "report_date": {"update": "2025-03-19", "evidence": "The issue was created on 2025-03-19 02:53:03+00:00."}, "last_update": {"update": "2025-05-14", "evidence": "No further updates are mentioned in the issue."}}
{"issue_number": 1475, "issue_description": {"score": 2, "evidence": "The issue describes that some test cases in test_fsdp_core.py are failing randomly in the _join_processes function. The error messages indicate a failure in scalar comparison, expecting 0 but getting -11. The tests are related to FSDP (Fully Sharded Data Parallel) in PyTorch with XPU support."}, "error_message": {"score": 2, "evidence": "The traceback shows an AssertionError with the message 'Scalars are not equal! Expected 0 but got -11. Absolute difference: 112... Relative difference: inf'."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions running tests for the branch daisyden/fsdp_test, but specific commands like pytest or shell commands are not provided."}, "software_version": {"score": 2, "evidence": "Torch version c208f217917929a9f780a81a8c7f788b4c03ee05 is specified."}, "platform": {"score": 2, "evidence": "The tests are failing on XPU hardware as indicated by the test names (e.g., test_transformer_no_grad_mixed_precision_True_xputest)."}}, "reporter": "daisyden", "assignee": "daisyden", "resolution": {"score": 0, "evidence": "No information provided on how to resolve the issue."}, "root_cause": {"score": 0, "evidence": "No information provided on the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue affects the distributed training functionality, specifically with FSDP on XPU. Random test failures can lead to unreliable test suites and may indicate instability in the FSDP implementation for XPU."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue is labeled under the 'bugmodule: distributions' category."}, "predicted_module": {"module": "distributions", "evidence": "The test cases are part of the distributed testing suite, and the issue is related to FSDP which falls under the distributed training functionality."}, "report_date": {"update": "2025-03-17", "evidence": "The issue was created on 2025-03-17."}, "last_update": {"update": "2025-04-15", "evidence": "The issue was created on 2025-03-17 and there's no mention of updates after that date."}}
{"issue_number": 1468, "issue_description": {"score": 2, "evidence": "The issue describes that with oneAPI 2025.1, the argmin function returns incorrect results for int16, int32, and int64 data types. It also mentions that int8 does not have this issue. This is a clear and concise description of the problem."}, "error_message": {"score": 0, "evidence": "No specific error message is provided in the issue description."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes a test command `pytest -o cache_dir=F:\\daisy_cache  -v .\\test_ops_xpu.py -k test_compare_cpu_argmin_xpu_int64`, which provides clear instructions on how to reproduce the issue. The test case is also specified, which is helpful."}, "software_version": {"score": 2, "evidence": "The issue specifies that the problem occurs with oneAPI 2025.1, providing clear version information."}, "platform": {"score": 2, "evidence": "The issue mentions that the test is run on XPU hardware, providing platform information."}}, "reporter": "daisyden", "assignee": "Stonepia", "resolution": {"score": 0, "evidence": "No information is provided about the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 2, "evidence": "The issue states that the argmin function returns incorrect results for certain integer types, which can affect the correctness of any application relying on this function for these data types. This clearly explains the impact of the issue."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled with 'bug' and 'dependency' and 'component: oneAPI', which suggests it's related to a dependency issue within the oneAPI component."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue relates to incorrect results in a specific function (argmin) across certain data types, which points to a potential dependency or component-specific bug within the oneAPI implementation."}, "report_date": {"update": "2025-03-14", "evidence": "The issue was created on 2025-03-14, providing clear date information."}, "last_update": {"update": "2025-04-16", "evidence": "The issue was created on 2025-03-14, and there is no mention of any updates after that, so the last update is assumed to be the same as the creation date."}}
{"issue_number": 1465, "issue_description": {"score": 2, "evidence": "The issue is titled 'RuntimeError: Non-uniform work-groups are not supported by the target device' and the body provides detailed information about the bug, including the test case and the error message."}, "error_message": {"score": 2, "evidence": "The error message 'RuntimeError: Non-uniform work-groups are not supported by the target device' is clearly provided."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes the test case code that can be used to reproduce the issue, including the pytest command and the specific test function."}, "software_version": {"score": 2, "evidence": "The versions include oneAPI 2025.1, which is relevant to the issue."}, "platform": {"score": 2, "evidence": "The issue mentions the platform as BMG and OS as Windows."}}, "reporter": "daisyden", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No information about the resolution is provided in the issue."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue."}, "impact": {"score": 0, "evidence": "No specific impact is mentioned, but it relates to non-uniform work-groups not being supported, which could affect performance or functionality."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is part of the test suite, specifically in the test_ops_xpu.py file, indicating it's related to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The test case is in the extended test suite, and the error occurs during a test, suggesting it's part of the unit tests."}, "report_date": {"update": "2025-03-14", "evidence": "The issue was created on 2025-03-14."}, "last_update": {"update": "2025-03-14", "evidence": "The issue was created and remains open as of the report date."}}
{"issue_number": 1453, "issue_description": {"score": 2, "evidence": "The issue is described as the BMG machine crashing when running HuggingFace performance mode with specific parameters."}, "error_message": {"score": 1, "evidence": "The comments mention an error related to the model being too large for dedicated GPU memory and the need to use shared GPU memory, but the specific error message is not clearly stated. This makes it difficult to identify the exact issue without further information."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The command line script `python benchmarks/dynamo/huggingface.py --performance -d xpu -n10 --backend=inductor --cold-start-latency --inference --amp --amp-dtype bfloat16 --only AllenaiLongformerBase  --output=inductor-huggingface-performance-inference-amp_bf16_tst.csv` is provided to reproduce the issue."}, "software_version": {"score": 2, "evidence": "Triton commit: cfb7d5314748542fed42d083b, pytorch whl: 2.7.0.dev20250306+xpu, pytorch commit: e35f2fa1a1be99e73f82b083b0092d0caef33c0c, Driver versions: public 32.0.101.6647, internal 32.0.101.6627."}, "platform": {"score": 2, "evidence": "Windows platform, BMG machine."}}, "reporter": "libohao1201", "assignee": "Stonepia", "resolution": {"score": 1, "evidence": "There is mention of a driver fix and plans to track the issue through internal JIRA systems, but the resolution steps are not clearly outlined. For example, the driver team is working on a switch to only use dedicated GPU memory, but there's no detailed plan or timeline provided beyond mentioning release 2.7."}, "root_cause": {"score": 1, "evidence": "The root cause is partially identified as memory management issues, particularly with high memory pressure leading to improper memory release. However, the explanation is not thorough, and the exact cause is not clearly pinpointed. For instance, the problem arises when the model is too large for dedicated GPU memory, leading to the use of shared memory, which then causes crashes after a few models. But the specific reasons for the memory not being released correctly are not fully explained."}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "Labels include 'bugclienthw' and 'BMGdependency'."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue relates to BMG hardware dependency and driver crashes."}, "report_date": {"update": "2025-03-11", "evidence": "Created at 2025-03-11 03:43:11+00:00."}, "last_update": {"update": "2025-04-16", "evidence": "The issue was created on this date and there is no further update information provided."}}
{"issue_number": 1432, "issue_description": {"score": 2, "evidence": "The issue describes that after enabling XPU in stock PyTorch, several test cases related to the scaled dot-product attention in the Transformers module failed. The error messages indicate a shape mismatch between the meta and real implementations, specifically in the output tensor shapes."}, "error_message": {"score": 2, "evidence": "The error message shows a shape mismatch: expected shape (4,4,3) but got an empty shape. This suggests a discrepancy in how the output is being computed or returned."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions the specific test cases that failed but does not provide explicit steps to reproduce the issue. The user would need to run the tests mentioned in the CI logs to replicate the issue."}, "software_version": {"score": 2, "evidence": "The versions of PyTorch and torch-xpu-ops are provided, including commit hashes. This is helpful for reproducibility."}, "platform": {"score": 2, "evidence": "The issue involves XPU-enabled PyTorch, so the platform is related to Intel's XPU hardware and the specific PyTorch setup."}}, "reporter": "daisyden", "assignee": "LuFinch", "resolution": {"score": 1, "evidence": "The comment mentions the need for OneDNN support but doesn't provide a detailed resolution plan or timeline. It indicates the problem and the required change but leaves the specifics vague."}, "root_cause": {"score": 2, "evidence": "The root cause is clearly identified as the lack of OneDNN support leading to performance overhead."}, "impact": {"score": 1, "evidence": "The failure affects the Transformers module's functionality, specifically in multi-head attention mechanisms, which are critical for NLP models. This could impact users relying on these components for their models."}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "The issue is labeled under 'OP Impl' which refers to operator implementations, specifically related to the scaled dot-product attention."}, "predicted_module": {"module": "transformers", "evidence": "The failing tests are in the test_transformers_xpu.py file, indicating the issue is within the Transformers module implementation."}, "report_date": {"update": "2025-03-05", "evidence": "The issue was created on March 5, 2025."}, "last_update": {"update": "2025-03-06", "evidence": "The issue was created on March 5, 2025, and no further updates are mentioned."}}
{"issue_number": 1401, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem with test failure details, including error messages and code snippets."}, "error_message": {"score": 2, "evidence": "The error message clearly states the assertion failure with tensor comparison details, including mismatched elements and differences."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes a test script and command to reproduce the issue, providing clear instructions on how to run the test."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned in the issue."}, "platform": {"score": 2, "evidence": "The issue specifies the platform as Windows with hardware label ARC."}}, "reporter": "huaiyuzh", "assignee": "daisyden", "resolution": {"score": 0, "evidence": "No information provided on the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects the test case for weight normalization, potentially impacting the reliability of the implementation on Windows with ARC hardware."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled with 'UT' indicating it relates to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The test case pertains to unit testing of the weight normalization functionality."}, "report_date": {"update": "2025-02-24", "evidence": "The issue was created on 2025-02-24."}, "last_update": {"update": "2025-02-24", "evidence": "No further updates or comments are present in the issue."}}
{"issue_number": 1400, "issue_description": {"score": 2, "evidence": "The issue is about a failed test in the test_rms_norm.py file, specifically the test_rms_norm_bw method which fails with an AssertionError regarding tensor-like closeness."}, "error_message": {"score": 2, "evidence": "AssertionError: Tensor-likes are not close!\", with detailed information about mismatched elements and differences in the tensors."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes the Python code for the test case, including the setup and assertions, which can be used to reproduce the issue by running the test."}, "software_version": {"score": 2, "evidence": "The issue specifies the use of PyTorch and the Windows platform, along with the specific test file and method."}, "platform": {"score": 2, "evidence": "The test is being run on a Windows platform with the ARC label, indicating the hardware or platform specifics."}}, "reporter": "huaiyuzh", "assignee": "PenghuiCheng", "resolution": {"score": 0, "evidence": "No information provided on how to resolve the issue."}, "root_cause": {"score": 0, "evidence": "No detailed root cause analysis is provided in the issue description."}, "impact": {"score": 2, "evidence": "The failure in the RMSNorm test could indicate issues in the implementation or optimization of the RMS normalization layer, affecting models relying on this layer."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled under 'clienthw: Arc.' and the test is part of the unit tests for the RMSNorm operation."}, "predicted_module": {"module": "UT", "evidence": "The issue involves a unit test failure in the test_rms_norm.py file, specifically related to the backward pass of the RMSNorm operation."}, "report_date": {"update": "2025-02-24", "evidence": "The issue was created on 2025-02-24."}, "last_update": {"update": "2025-02-28", "evidence": "No further updates are mentioned in the issue."}}
{"issue_number": 1381, "issue_description": {"score": 2, "evidence": "The issue title mentions performance regression caused by pad_sequence and gru.input. The body states that molan performance regression up to 76% is caused by these functions."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 2, "evidence": "Performance regression up to 76% is mentioned."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The labels include task: Perf, which is related to performance issues."}, "predicted_module": {"module": "Core", "evidence": "The issue involves performance regression, which is a core functionality concern."}, "report_date": {"update": "2025-02-19", "evidence": "The issue was created on 2025-02-19."}, "last_update": {"update": "2025-05-14", "evidence": "No further updates mentioned."}}
{"issue_number": 1352, "issue_description": {"score": 2, "evidence": "The issue describes that on iGPU with Windows, the function torch.xpu.mem_get_info() raises an error because the device lacks the ext_intel_free_memory aspect. It also mentions that the error does not occur on BMG and provides the PyTorch version and XPU version used."}, "error_message": {"score": 2, "evidence": "The error message includes a traceback pointing to torch.xpu.memory.py line 194 and the specific RuntimeError: 'The device does not have the ext_intel_free_memory aspect'."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue provides the Python code to reproduce the error but does not include specific pytest commands or shell commands for a full reproduction recipe."}, "software_version": {"score": 2, "evidence": "PyTorch version is specified as 2.7.0.dev20250209+xpu and XPU version is 20250000."}, "platform": {"score": 2, "evidence": "The issue states it occurs on iGPU on Windows, with specific Intel GPU driver version and hardware model."}}, "reporter": "Stonepia", "assignee": "Stonepia", "resolution": {"score": 2, "evidence": "The issue is marked as ready for deployment, indicating a resolution path."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as driver support issues and missing Sysman modules."}, "impact": {"score": 1, "evidence": "The issue affects the memory management functionality on iGPU in Windows, potentially causing applications to fail when trying to retrieve memory information."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The label includes 'dependency bug' and 'driver', suggesting the issue is related to dependencies or driver components."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue involves missing aspects in the device, which could relate to driver dependencies or missing components in the XPU setup."}, "report_date": {"update": "2025-02-10", "evidence": "The issue was created on 2025-02-10 07:52:08+00:00."}, "last_update": {"update": "2025-04-16", "evidence": "No further updates are mentioned in the issue."}}
{"issue_number": 1329, "issue_description": {"score": 2, "evidence": "The issue describes a NotImplementedError when using 'torch.ao.quantization.quantize_dynamic' with XPU devices."}, "error_message": {"score": 2, "evidence": "NotImplementedError: The operator 'quantized::linear_dynamic' is not currently implemented for the XPU device."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides a Python code snippet that reproduces the error."}, "software_version": {"score": 2, "evidence": "Versions of relevant libraries include torch==2.5.0a0, intel_extension_for_pytorch==2.5.10, etc."}, "platform": {"score": 2, "evidence": "The error occurs on XPU devices using Intel's extension for PyTorch."}}, "reporter": "gurwinderintel", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No information provided on the resolution status or timeline."}, "root_cause": {"score": 0, "evidence": "No detailed root cause analysis is provided in the issue."}, "impact": {"score": 1, "evidence": "The issue affects dynamic quantization on XPU devices, potentially limiting performance improvements and functionality."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue is labeled under 'module: quant'."}, "predicted_module": {"module": "quant", "evidence": "The issue involves the 'quantize_dynamic' function, which is related to quantization."}, "report_date": {"update": "2025-01-29", "evidence": "The issue was created on 2025-01-29 09:53:32+00:00."}, "last_update": {"update": "2025-02-27", "evidence": "The issue was created on 2025-01-29 09:53:32+00:00."}}
{"issue_number": 1324, "issue_description": {"score": 2, "evidence": "The issue is titled '[Win] UR Error when OOM and break the tensor context' and the body provides a detailed description of the bug, including code snippets and error messages. This is clear and concise."}, "error_message": {"score": 2, "evidence": "The error messages are provided with stack traces and specific RuntimeError details, which are clear and relevant."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes step-by-step instructions to reproduce the bug, including creating tensors and triggering the OOM error. These steps are clear and reproducible."}, "software_version": {"score": 2, "evidence": "The versions section includes details about PyTorch and the platform, which are necessary for reproduction."}, "platform": {"score": 2, "evidence": "The platform information is provided, including the use of Windows and specific memory configurations."}}, "reporter": "Stonepia", "assignee": "guangyey", "resolution": {"score": 0, "evidence": "No information is provided about the resolution process or timeline."}, "root_cause": {"score": 0, "evidence": "The issue does not specify the root cause of the problem or any potential solutions."}, "impact": {"score": 2, "evidence": "The issue describes how the bug affects tensor context and the user experience, which is critical for model training and debugging."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under 'dependency bug' and 'driver'."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue involves problems with memory management and tensor context, which relates to dependency and driver modules."}, "report_date": {"update": "2025-01-24", "evidence": "The issue was created on 2025-01-24 02:54:35+00:00."}, "last_update": {"update": "2025-03-19", "evidence": "No further updates or comments are present in the issue description."}}
{"issue_number": 1305, "issue_description": {"score": 2, "evidence": "The issue describes that models fail accuracy on BMG but pass on PVC. The test cases include timm_models_float16_training, torchbench_float32_inference, timm_models_amp_fp16_training, timm_models_bfloat16_training, torchbench_float16_training, and torchbench_bfloat16_training. The error messages indicate 'fail_accuracy' for all these tests."}, "error_message": {"score": 2, "evidence": "The error messages are consistent across all test cases, indicating a potential issue with the accuracy on BMG hardware."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue provides the command line arguments used to run the tests, but the actual Python or shell commands to reproduce the issue are not explicitly given. The user would need to infer the exact commands from the provided information."}, "software_version": {"score": 2, "evidence": "The versions of all relevant software and libraries are provided in detail, including Torch, Torch-xpu-ops, Triton, Transformers, Torchvision, Torchaudio, Torchbench, Timms, and Bundle."}, "platform": {"score": 2, "evidence": "The platform information includes the device (BMG), OS (Ubuntu 24.10), and driver version (24.45.31740)."}}, "reporter": "mengfei25", "assignee": "Stonepia", "resolution": {"score": 2, "evidence": "The comments outline the plan to submit a PR to align the optimizer behavior with CUDA, which is a clear resolution plan. The root cause is identified as the optimizer behavior difference leading to model fallback. The last updated date is 2025-04-02."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as the unalignment with CUDA's test behavior regarding the optimizer, leading to some models falling back to SGD. The last updated date is 2025-04-02."}, "impact": {"score": 1, "evidence": "The issue affects the accuracy of multiple models on BMG hardware, which could impact the reliability and performance of AI workloads on this platform."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue involves models from Timm and Torchvision, which are related to vision models. Additionally, the test cases include models like Longformer, which is a transformer-based model."}, "predicted_module": {"module": "transformers", "evidence": "The issue involves transformer-based models (Longformer) and affects their accuracy on BMG hardware, suggesting that the problem may be related to the transformer module or the implementation of these models."}, "report_date": {"update": "2025-01-20", "evidence": "The issue was created on 2025-01-20."}, "last_update": {"update": "2025-04-02", "evidence": "The issue was last updated on the same date as it was created, indicating no further updates have been made yet."}}

{"issue_number": 1276, "issue_description": {"score": 2, "evidence": "The issue title and body provide clear and concise information about the problem encountered during inference, including the error message and the commands used to reproduce the issue."}, "error_message": {"score": 2, "evidence": "The error message details the out-of-memory issue and provides a traceback, making it clear where the failure occurred."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The user provided the specific Python command used to trigger the issue, which is sufficient for reproduction."}, "software_version": {"score": 2, "evidence": "The versions of the relevant software and hardware are clearly stated, including Python, PyTorch, and the driver version."}, "platform": {"score": 2, "evidence": "The device used (PVC 1100) and the operating system (Ubuntu 22.04.2 LTS) are specified, providing clear platform information."}}, "reporter": "mengfei25", "assignee": "LuFinch", "resolution": {"score": 1, "evidence": "The comments mention possible issues with non-fused SDPA and suggest that additional memory might be the cause, but the root cause and resolution are not clearly explained."}, "root_cause": {"score": 1, "evidence": "The comments suggest that the failure is related to `nn.functional.softmax` not being fused to SDPA and that non-fused SDPA uses more memory, but the root cause is not clearly identified or explained."}, "impact": {"score": 2, "evidence": "The issue affects the inference phase of the T5_base model, which is critical for evaluation and deployment."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue involves the T5 model from the transformers library, specifically during the inference phase."}, "predicted_module": {"module": "transformers", "evidence": "The error occurs within the T5 model's forward pass, pointing to the transformers module as the affected component."}, "report_date": {"update": "2025-01-10", "evidence": "The issue was created on 2025-01-10, providing a clear timestamp."}, "last_update": {"update": "2025-02-27", "evidence": "The issue was created on 2025-01-10, and there is no indication of further updates."}}
{"issue_number": 1261, "issue_description": {"score": 2, "evidence": "Issue title and body provide clear and concise information about the problem, including the error message and the test case used to reproduce it."}, "error_message": {"score": 2, "evidence": "The error message clearly states 'XPU out of memory' with details about the memory allocation attempt and current memory usage."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The reproduce steps are provided as a Python command in the issue body."}, "software_version": {"score": 2, "evidence": "Python version 3.10 and PyTorch versions are specified along with commit IDs."}, "platform": {"score": 2, "evidence": "Device is PVC 1100, driver version, kernel version, OS, etc., are provided."}}, "reporter": "mengfei25", "assignee": "tye1", "resolution": {"score": 2, "evidence": "The resolution is clearly explained, including the specific commit that fixed the issue and the steps taken to address it."}, "root_cause": {"score": 2, "evidence": "The root cause is clearly identified as an issue with the Adam optimizer in the optimizer.py file."}, "impact": {"score": 2, "evidence": "The issue affects the Stable Diffusion UNet model's training on XPU, potentially blocking the benchmarking process."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue involves the Stable Diffusion UNet model, which is part of the transformers module."}, "predicted_module": {"module": "transformers", "evidence": "The error occurs in the attention processor of the UNet model, which is part of the transformers."}, "report_date": {"update": "2025-01-08", "evidence": "Issue was created on 2025-01-08."}, "last_update": {"update": "2025-04-22", "evidence": "No further updates mentioned in the issue."}}

{"issue_number": 1214, "issue_description": {"score": 2, "evidence": "The issue describes that during the preci test, there are random failures with the error message 'AssertionError: Tensor-likes are not close!'. Specific test cases are provided."}, "error_message": {"score": 2, "evidence": "'AssertionError: Tensor-likes are not close!'"}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue provides specific test cases but lacks detailed reproduce steps using Python or shell commands."}, "software_version": {"score": 1, "evidence": "PyTorch version is mentioned as N/A, which is unclear."}, "platform": {"score": 2, "evidence": "OS: Ubuntu 22.04.2 LTS, CPU information, and other environment details are provided."}}, "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": {"score": 0, "evidence": "No information on the resolution is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue."}, "impact": {"score": 1, "evidence": "The issue affects specific test cases related to complex128 and other functions in PyTorch XPU operations."}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "Labels include 'ut_triagedmodule: OP Impl' and 'dtype: complex'."}, "predicted_module": {"module": "OP impl", "evidence": "The test cases involve operations like log2, sigmoid, and exp, which are part of PyTorch operations."}, "report_date": {"update": "2024-12-26", "evidence": "The issue was created on 2024-12-26."}, "last_update": {"update": "2025-03-20", "evidence": "No further updates are mentioned beyond the initial report."}}
{"issue_number": 1195, "issue_description": {"score": 2, "evidence": "The issue is titled 'get nan with complex dtype' and the body describes getting NaN when using complex dtypes, with specific test cases provided."}, "error_message": {"score": 2, "evidence": "The user reports that running the specified tests results in NaN values."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides detailed Python commands to reproduce the issue, including the use of PYTORCH_TEST_WITH_SLOW and specific test cases."}, "software_version": {"score": 2, "evidence": "PyTorch version and commit hash are provided, along with the torch-xpu-ops commit."}, "platform": {"score": 2, "evidence": "The machine is running Windows 11."}}, "reporter": "Stonepia", "assignee": "Stonepia", "resolution": {"score": 0, "evidence": "No information provided on the resolution."}, "root_cause": {"score": 0, "evidence": "No information provided on the root cause."}, "impact": {"score": 2, "evidence": "The issue affects multiple test cases involving complex dtypes, which could impact functionality for users relying on complex number operations."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under 'dependency bug' and 'component: oneAPI', 'dtype: complex'."}, "predicted_module": {"module": "dependency bug", "evidence": "The tests involve complex dtypes and oneAPI, suggesting the issue is related to dependencies or the underlying implementation of complex number handling in the dependency module."}, "report_date": {"update": "2024-12-23", "evidence": "The issue was created on 2024-12-23."}, "last_update": {"update": "2024-12-23", "evidence": "No further updates provided beyond the initial report."}}

{"issue_number": 1171, "issue_description": {"score": 2, "evidence": "The issue describes an unexpected error message on Windows with LNL using a specific nightly wheel. The error occurs in a test case related to MaxUnpool, and the error message is provided. The issue also includes steps to reproduce the test and mentions the PyTorch version and platform."}, "error_message": {"score": 2, "evidence": "The error message is 'Assertion `maxind >= 0 && maxind < outputImageSize` failed', which is clear and specific."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Steps to run the test are provided using specific Python commands."}, "software_version": {"score": 2, "evidence": "Mentions 20241202 wheel, which specifies the PyTorch version."}, "platform": {"score": 2, "evidence": "The issue occurs on Windows with LNL."}}, "reporter": "daisyden", "assignee": "gaopengff", "resolution": {"score": 1, "evidence": "The resolution involves using 2025.1, but the exact steps are not detailed."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as a compiler issue, specifically tracked under Jira PYTORCHDGQ-5888."}, "impact": {"score": 2, "evidence": "The error affects the MaxUnpool functionality on Windows, potentially causing test failures and reliability issues."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue relates to a test case in the pooling module."}, "predicted_module": {"module": "UT", "evidence": "The test case is part of the pooling tests in the UT module."}, "report_date": {"update": "2024-12-17", "evidence": "Issue created on 2024-12-17."}, "last_update": {"update": "2025-04-23", "evidence": "No further updates mentioned."}}
{"issue_number": 1165, "issue_description": {"score": 2, "evidence": "The issue is about adding a test for PyTorch XPU with Huggingface Transformers, which is clearly described in the issue body."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "dvrogozh", "assignee": "RUIJIEZHONG66166", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The label is explicitly stated as 'module: transformers'."}, "predicted_module": {"module": "transformers", "evidence": "The issue involves testing Huggingface Transformers with XPU, which relates to the transformers module."}, "report_date": {"update": "2024-12-13", "evidence": "The issue was created on 2024-12-13."}, "last_update": {"update": "2024-12-19", "evidence": "No further updates are mentioned in the issue description."}}
{"issue_number": 1159, "issue_description": {"score": 2, "evidence": "The issue describes a failure in running Huggingface models DebertaForQuestionAnswering and DebertaV2ForMaskedLM with a specific error related to BFloat16 conversion overflow."}, "error_message": {"score": 2, "evidence": "RuntimeError: value cannot be converted to type at::BFloat16 without overflow"}, "reproduce_steps": {"steps": {"score": 2, "evidence": "python benchmarks/dynamo/huggingface.py --accuracy -d xpu -n10 --inference --backend=eager --cold-start-latency --amp --amp-dtype float16 --only DebertaForQuestionAnswering"}, "software_version": {"score": 2, "evidence": "PyTorch version: 2.6.0.dev20241202+xpu, torch-xpu-ops version: bf4bab1ffffd84e5f747f65a17e08ee2fe633102, Driver version: 32.0.101.6314, Python version: 3.10"}, "platform": {"score": 2, "evidence": "Windows 11 machine"}}, "reporter": "libohao1201", "assignee": "Stonepia", "resolution": {"score": 0, "evidence": "No information provided on how to resolve the issue."}, "root_cause": {"score": 0, "evidence": "No detailed root cause analysis provided in the issue description."}, "impact": {"score": 2, "evidence": "The failure affects the ability to run specific Huggingface models on XPU devices using PyTorch and torch-xpu-ops."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "Labels include 'dependency bug' and 'third_party packages'."}, "predicted_module": {"module": "transformers", "evidence": "The issue involves Huggingface models which are part of the transformers library."}, "report_date": {"update": "2024-12-11", "evidence": "The issue was created on 2024-12-11 08:14:00+00:00."}, "last_update": {"update": "2025-02-24", "evidence": "The issue was created on 2024-12-11 and there is no information on updates after that."}}
{"issue_number": 1129, "issue_description": {"score": 2, "evidence": "The issue describes that inductor may pad mm for certain matrix multiplication operations, which involves padding matrices with zeros, concatenating, performing matrix multiplication, and then slicing. It also mentions that this approach improves performance by 30% on A100 with fp16 and fp_GPT2. The task is to check if this is applicable and beneficial for XPU. The description is concise and clear."}, "error_message": {"score": 0, "evidence": "No error message is provided in the issue."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific reproduce steps are given, such as Python or shell commands to replicate the issue."}, "software_version": {"score": 0, "evidence": "No information about PyTorch version or platform details is provided."}, "platform": {"score": 0, "evidence": "No platform information is included."}}, "reporter": "jianyizh", "assignee": "jianyizh", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified or discussed."}, "impact": {"score": 2, "evidence": "The impact is clearly stated as a potential 30% performance improvement, which is significant."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue relates to optimizing matrix multiplication, which is a key component in transformer models."}, "predicted_module": {"module": "transformers", "evidence": "The test case involves GPT models, which fall under the transformers category."}, "report_date": {"update": "2024-11-29", "evidence": "The issue was created on 2024-11-29."}, "last_update": {"update": "2025-02-08", "evidence": "No further updates are mentioned beyond the creation date."}}
{"issue_number": 1124, "issue_description": {"score": 2, "evidence": "The issue describes precision issues in numpy and XPU results for extreme values, affecting complex numbers and specific functions like std::complex operations. The description is clear and provides specific test cases."}, "error_message": {"score": 0, "evidence": "No specific error messages are provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No detailed steps to reproduce the issue are given."}, "software_version": {"score": 0, "evidence": "No version information is provided."}, "platform": {"score": 0, "evidence": "No platform or environment details are given."}}, "reporter": "daisyden", "assignee": "daisyden", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause information is provided."}, "impact": {"score": 2, "evidence": "The issue affects numerical computations, which is critical for accuracy in scientific and engineering applications."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under 'oneAPI' which relates to dependencies or components."}, "predicted_module": {"module": "UT", "evidence": "The test cases mentioned are reference numerics tests, suggesting they are part of the unit test suite."}, "report_date": {"update": "2024-11-28", "evidence": "The issue was created on 2024-11-28."}, "last_update": {"update": "2025-04-22", "evidence": "No information on updates after the creation date."}}
{"issue_number": 1121, "issue_description": {"score": 2, "evidence": "The issue describes a bug where the kernel bundle is not device-specific under a specific platform context, leading to incorrect usage of `sycl::get_kernel_bundle` with a device-specific hint. The reporter suggests rolling back to the original usage of `sycl::get_kernel_bundle` without the device-specific hint."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "The issue is labeled under 'module: OP Impl.' which refers to OneAPI operations implementation."}, "predicted_module": {"module": "OP impl", "evidence": "The issue relates to the implementation of SYCL kernel bundles in PyTorch's XPU operations, which falls under the 'OP Impl' module."}, "report_date": {"update": "2024-11-27", "evidence": "The issue was created on 2024-11-27."}, "last_update": {"update": "2025-05-14", "evidence": "The issue was created on 2024-11-27 and there is no further update mentioned."}}
{"issue_number": 1059, "issue_description": {"score": 2, "evidence": "The issue title and body provide clear information about the feature request and related context, including existing code references and TODOs."}, "error_message": {"score": 0, "evidence": "No error message is provided in the issue description."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps are provided."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "fengyuan14", "assignee": "majing921201", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is mentioned."}, "impact": {"score": 0, "evidence": "No specific impact is described."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled with 'feature', which may relate to test cases but is not specific to any module."}, "predicted_module": {"module": "UT", "evidence": "The issue involves test case classifications, suggesting it relates to unit tests."}, "report_date": {"update": "2024-11-07", "evidence": "The issue was created on 2024-11-07."}, "last_update": {"update": "2025-05-14", "evidence": "No further updates are mentioned."}}
{"issue_number": 1016, "issue_description": {"score": 2, "evidence": "The issue describes a performance problem with severe host overhead in sycl::get_kernel_bundle. It provides timing data showing significant delays in certain API calls, which impact all kernels using kernel-specific max work group sizes. The impact is particularly severe for single batch inference where kernel latencies are less than 10us, and it's compared to CUDA's 6us kernel launch time."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "fengyuan14", "assignee": "majing921201", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 2, "evidence": "The impact is described as severe host overhead affecting all kernels, with specific timing data and comparison to CUDA performance."}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "The label is performancemodule: OP Impl."}, "predicted_module": {"module": "OP impl", "evidence": "The issue relates to performance in kernel operations, which falls under OP Impl."}, "report_date": {"update": "2024-10-23", "evidence": "The issue was created on 2024-10-23."}, "last_update": {"update": "2024-10-23", "evidence": "No further updates mentioned beyond the initial report."}}
{"issue_number": 1009, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the feature request, including specific examples and requirements for improving PyTorch XPU verbose logging to match PyTorch GPU practices."}, "error_message": {"score": 0, "evidence": "No specific error messages are provided in the issue description."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps are provided in the issue description."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "riverliuintel", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": "No resolution information is provided as the issue is still open."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided since the issue is in the open state."}, "impact": {"score": 1, "evidence": "The issue mentions that verbose logs are unclear, which could impact debugging and user experience, but the impact is not clearly detailed."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled under 'enhancementfeature', and the description relates to improving logging, which could be part of unit testing or utility modules."}, "predicted_module": {"module": "UT", "evidence": "The focus on improving logging and verbose messages aligns with unit testing and utility functionalities."}, "report_date": {"update": "2024-10-22", "evidence": "The issue was created on 2024-10-22."}, "last_update": {"update": "2025-05-14", "evidence": "No further updates are provided beyond the initial creation date."}}
{"issue_number": 1005, "issue_description": {"score": 2, "evidence": "The issue provides a clear description of integrating oneDNN GEMM INT4 kernels for Torchao LLM usage, including passing unit tests and example workloads."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "riverliuintel", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The label is 'featuremodule: quant.'"}, "predicted_module": {"module": "quant", "evidence": "The issue is about integrating GEMM INT4 kernels which is related to quantization."}, "report_date": {"update": "2024-10-22", "evidence": "The issue was created at 2024-10-22."}, "last_update": {"update": "2025-05-14", "evidence": "No information about last update beyond the creation date is provided."}}

{"issue_number": 970, "issue_description": {"score": 2, "evidence": "The issue describes a performance problem where the CPU time for the 'aten::sum' operation is higher when using the override method compared to IPEX. The reporter provides specific timing data and compares it against the latest torch-xpu-ops and IPEX versions."}, "error_message": {"score": 2, "evidence": "The comment does not mention any specific errors or issues beyond linking to the root cause, so it doesn't provide detailed error information, but it does provide a clear reference to the root cause."}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "fengyuan14", "assignee": "majing921201", "resolution": {"score": 1, "evidence": "The comment does not provide a detailed resolution, but it does reference a related issue which may contain the resolution. Therefore, while there is some indication of how the issue might be resolved, it is not fully detailed here."}, "root_cause": {"score": 2, "evidence": "The comment references a specific LLVM issue (15824), indicating that the root cause is identified and concisely linked to a known issue."}, "impact": {"score": 2, "evidence": "The performance issue could affect the efficiency of operations using 'aten::sum', potentially slowing down computations and reducing overall performance."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue is labeled under 'performance' and discusses a core operation 'aten::sum' which is fundamental to many computations in PyTorch."}, "predicted_module": {"module": "Core", "evidence": "The test case involves a core operation 'aten::sum', indicating the issue is likely in the core module."}, "report_date": {"update": "2024-10-16", "evidence": "The issue was created on 2024-10-16."}, "last_update": {"update": "2024-10-29", "evidence": "The issue was created on 2024-10-16."}}
{"issue_number": 969, "issue_description": {"score": 2, "evidence": "The issue describes a performance difference in the nonzero operation between torch-xpu-ops and IPEX. The user provided a detailed comparison of CPU and XPU metrics, indicating worse host overhead in the former."}, "error_message": {"score": 1, "evidence": "The comment provides some information but could be more detailed. The last updated date is 2024-10-23."}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "fengyuan14", "assignee": "majing921201", "resolution": {"score": 1, "evidence": "The resolution mentions filing an issue with the compiler, but it's not very clear. The last updated date is 2024-10-23."}, "root_cause": {"score": 1, "evidence": "The root cause is mentioned as the SYCL API used to query kernel specific max work group size, but it's briefly explained. The last updated date is 2024-10-23."}, "impact": {"score": 1, "evidence": "The issue indicates that torch-xpu-ops has worse host overhead compared to IPEX, which could affect performance in applications relying on nonzero operations."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue relates to the nonzero operation, which is part of the core functionality of PyTorch."}, "predicted_module": {"module": "Core", "evidence": "The nonzero operation is a fundamental function in PyTorch, affecting performance metrics such as CPU and XPU usage."}, "report_date": {"update": "2024-10-16", "evidence": "The issue was created on 2024-10-16."}, "last_update": {"update": "2024-10-23", "evidence": "No further updates were provided beyond the initial report."}}
{"issue_number": 964, "issue_description": {"score": 0, "evidence": "Missing information about the issue description, error message, impact, and reproduce steps."}, "error_message": {"score": 0, "evidence": "No error message provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps provided."}, "software_version": {"score": 0, "evidence": "No version information provided."}, "platform": {"score": 0, "evidence": "No platform information provided."}}, "reporter": "xytintel", "assignee": "daisyden", "resolution": {"score": 0, "evidence": "No resolution information provided."}, "root_cause": {"score": 0, "evidence": "No root cause information provided."}, "impact": {"score": 0, "evidence": "No impact information provided."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled with 'ut_triaged', which suggests it relates to unit testing."}, "predicted_module": {"module": "UT", "evidence": "The issue is about porting unit tests, which falls under the UT category."}, "report_date": {"update": "2024-10-15", "evidence": "The issue was created on 2024-10-15."}, "last_update": {"update": "2024-10-15", "evidence": "No further updates mentioned."}}
{"issue_number": 954, "issue_description": {"score": 2, "evidence": "The comments provided detailed information about the root cause and the required changes to resolve the issue. The root cause was identified as duplicate symbols in `torchgen.gen` headers causing conflicts with Clang, and the solution involved enabling Clang with slight changes. The comments were clear and concise, providing enough technical detail for understanding the problem and the necessary resolution steps."}, "error_message": {"score": 2, "evidence": "The comments provided clear and concise information about the root cause and the resolution steps. The root cause was the duplication of symbols in `torchgen.gen` headers when using Clang, and the solution involved making minor changes to enable Clang. The comments were detailed and provided sufficient technical information to understand the issue and its resolution."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps provided."}, "software_version": {"score": 0, "evidence": "No specific versions mentioned."}, "platform": {"score": 0, "evidence": "No platform details provided."}}, "reporter": "gglin001", "assignee": "fengyuan14", "resolution": {"score": 2, "evidence": "The comments provided clear and concise information about the root cause and the resolution steps. The root cause was the duplication of symbols in `torchgen.gen` headers when using Clang, and the solution involved making minor changes to enable Clang. The comments were detailed and provided sufficient technical information to understand the issue and its resolution."}, "root_cause": {"score": 2, "evidence": "The comments provided clear and concise information about the root cause and the resolution steps. The root cause was the duplication of symbols in `torchgen.gen` headers when using Clang, and the solution involved making minor changes to enable Clang. The comments were detailed and provided sufficient technical information to understand the issue and its resolution."}, "impact": {"score": 2, "evidence": "The comments provided clear and concise information about the root cause and the resolution steps. The root cause was the duplication of symbols in `torchgen.gen` headers when using Clang, and the solution involved making minor changes to enable Clang. The comments were detailed and provided sufficient technical information to understand the issue and its resolution."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is about building with a different compiler, which relates to dependencies and build processes."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue pertains to build configuration, which is a dependency management task."}, "report_date": {"update": "2024-10-08", "evidence": "The issue was created on this date."}, "last_update": {"update": "2024-10-15", "evidence": "No further updates mentioned beyond the creation date."}}
{"issue_number": 939, "issue_description": {"score": 2, "evidence": "The comments provide concise information about the resolution and root cause. The reporter, fengyuan14, mentioned that they have implemented channels last kernel, which is on-par with CUDA. The assignee, majing921201, explained that the performance still has a gap with oneDNN and that they will low prioritize performance optimization for the oneDNN goal. This information is clear and directly addresses the issue's status and the actions being taken. The last updated date is October 17, 2024."}, "error_message": {"score": 2, "evidence": "The comments do not indicate any errors or issues with the information provided. The information is clear and concise."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No steps to reproduce are provided."}, "software_version": {"score": 0, "evidence": "No software version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "fengyuan14", "assignee": "majing921201", "resolution": {"score": 2, "evidence": "The comments indicate that channels last kernel has been implemented, but performance is still lagging behind oneDNN. The assignee mentions that performance optimization for oneDNN is being low prioritized, which suggests a plan for resolution."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as the performance gap between the implemented channels last kernel and oneDNN. The assignee explains that while the kernel implementation is on-par with CUDA, the performance is not matching oneDNN expectations."}, "impact": {"score": 0, "evidence": "The impact of the issue is not clearly described."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue is related to performance improvements in the core functionality of the UpsampleBilinear operation."}, "predicted_module": {"module": "Core", "evidence": "The issue description focuses on improving the performance of specific kernels, which is a core functionality."}, "report_date": {"update": "2024-09-26", "evidence": "The issue was created on 2024-09-26."}, "last_update": {"update": "2024-10-17", "evidence": "No further updates are provided after the initial creation date."}}
{"issue_number": 781, "issue_description": {"score": 2, "evidence": "The issue describes a discrepancy in the output of the torch.square function between CPU and XPU for a complex64 tensor. The CPU returns 1.0020e+23 for the imaginary part, while the XPU returns -inf. The reporter also provided a code snippet to reproduce the issue and mentioned that the problem is with the std::pow implementation used by 'square'."}, "error_message": {"score": 2, "evidence": "The error message indicates that the imaginary part of the output is incorrect on CPU and results in -inf on XPU. The expected value is -1.0020e+23, but the CPU returns 1.0020e+23 and XPU returns -inf."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The reporter provided a Python code snippet and a C++ SYCL kernel that can be used to reproduce the issue. The steps involve creating a tensor with a specific complex value and applying torch.square."}, "software_version": {"score": 2, "evidence": "The issue mentions using the latest version of PyTorch and the OneAPI implementation on XPU."}, "platform": {"score": 2, "evidence": "The issue affects both CPU and XPU platforms, as demonstrated by the provided code and output."}}, "reporter": "daisyden", "assignee": "daisyden", "resolution": {"score": 1, "evidence": "The reporter mentions submitting a bug to the compiler, suggesting that a fix is in progress, but no specific resolution steps are detailed in the issue."}, "root_cause": {"score": 1, "evidence": "The issue is likely related to the implementation of the std::pow function for complex numbers on the XPU, which may not handle very large exponents correctly, leading to incorrect results or infinities."}, "impact": {"score": 2, "evidence": "The discrepancy in results between CPU and XPU could lead to incorrect computations in applications relying on PyTorch's complex number operations, particularly for very large or small values."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under 'dependency' and 'component: oneAPI', indicating it's related to the integration or implementation of PyTorch's OneAPI support for complex numbers."}, "predicted_module": {"module": "UT", "evidence": "The test case provided by the reporter seems to be a unit test to reproduce the issue, suggesting the problem lies in the unit testing or implementation of complex number operations in PyTorch."}, "report_date": {"update": "2024-08-20", "evidence": "The issue was created on 2024-08-20."}, "last_update": {"update": "2025-02-27", "evidence": "No further updates or comments are present in the issue as of the given date."}}
{"issue_number": 772, "issue_description": {"score": 2, "evidence": "The issue is about supporting quantized operations in PyTorch, specifically for the XPU backend. The reporter encountered a NotImplementedError when running tests related to view operations."}, "error_message": {"score": 2, "evidence": "NotImplementedError: Could not run 'aten::_empty_affine_quantized' with arguments from the 'QuantizedXPU' backend."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions specific test cases: test_flatten_xpu and test_ravel_xpu in test_view_ops_xpu.py. However, explicit steps to reproduce are not provided."}, "software_version": {"score": 0, "evidence": "No information about PyTorch version or platform is provided."}, "platform": {"score": 0, "evidence": "No platform information is given."}}, "reporter": "PenghuiCheng", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is discussed in the issue."}, "impact": {"score": 1, "evidence": "The issue affects the functionality of view operations in quantized XPU operations, potentially breaking tests and preventing proper quantization support."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The label is ut_triagedmodule: quant."}, "predicted_module": {"module": "quant", "evidence": "The issue is about supporting quantized operations, and the test cases relate to view operations which are part of the quantization module."}, "report_date": {"update": "2024-08-16", "evidence": "The issue was created on 2024-08-16 08:02:14+00:00."}, "last_update": {"update": "2024-10-15", "evidence": "No further updates are mentioned in the issue."}}
{"issue_number": 761, "issue_description": {"score": 2, "evidence": "The issue provides detailed descriptions of the problems encountered, including specific test cases and error messages."}, "error_message": {"score": 2, "evidence": "Specific error messages like 'NotImplementedError: Could not run 'aten::_to_copy' with arguments from the 'NestedTensorXPU' backend' are provided."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions test cases but does not provide explicit reproduce steps using Python, pytest, or shell commands."}, "software_version": {"score": 1, "evidence": "No specific PyTorch version is mentioned, though it's implied to be related to the XPU backend."}, "platform": {"score": 0, "evidence": "No platform or hardware information is provided."}}, "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided."}, "impact": {"score": 1, "evidence": "The issue affects multiple test cases and indicates incomplete support for certain functionalities, which could impact the reliability of the XPU backend."}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "The issue is labeled under 'ut_triagedmodule: OP Impl.'"}, "predicted_module": {"module": "transformers", "evidence": "The test cases mentioned relate to transformer encoder layers and scaled dot product attention, which are part of the transformers module."}, "report_date": {"update": "2024-08-14", "evidence": "The issue was created on 2024-08-14 09:01:22+00:00."}, "last_update": {"update": "2024-10-15", "evidence": "No further updates are mentioned beyond the initial report."}}
{"issue_number": 754, "issue_description": {"score": 1, "evidence": "The issue title is 'Failures caused by precision error', and the body mentions tracking failures due to precision issues, but lacks detailed error messages or specific steps to reproduce."}, "error_message": {"score": 0, "evidence": "No specific error messages provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No clear reproduction steps provided."}, "software_version": {"score": 0, "evidence": "No specific versions mentioned."}, "platform": {"score": 0, "evidence": "No platform information provided."}}, "reporter": "daisyden", "assignee": "daisyden", "resolution": {"score": 1, "evidence": "The issue was left due to compiler or threshold needing discussion, but the root cause is not clearly identified."}, "root_cause": {"score": 1, "evidence": "The root cause is mentioned to be related to the compiler or threshold but lacks specific details."}, "impact": {"score": 0, "evidence": "No detailed impact analysis provided."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled as 'ut_triaged', suggesting it relates to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The issue involves test failures in reference numerics and relates to unit testing."}, "report_date": {"update": "2024-08-13", "evidence": "The issue was created on 2024-08-13."}, "last_update": {"update": "2025-04-22", "evidence": "No further updates mentioned beyond the creation date."}}
{"issue_number": 725, "issue_description": {"score": 2, "evidence": "The issue title and body provide clear and concise information about the problem, including the error message and the test case that failed."}, "error_message": {"score": 2, "evidence": "The error message clearly states that FCOS training is not supported by upstream detectron2 and provides a reference to another GitHub issue."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue does not provide explicit steps to reproduce the issue, but the command line output suggests that the test was run using a specific script or configuration."}, "software_version": {"score": 2, "evidence": "The versions of torch-xpu-ops and PyTorch are clearly provided, along with device information."}, "platform": {"score": 2, "evidence": "Device details, including type and driver version, are provided."}}, "reporter": "mengfei25", "assignee": "retonym", "resolution": {"score": 0, "evidence": "No information is provided about the resolution status or process."}, "root_cause": {"score": 0, "evidence": "The root cause of the issue is not explicitly identified or discussed."}, "impact": {"score": 1, "evidence": "The issue affects the training accuracy of a specific model, which could impact the reliability of the framework for detection tasks."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The issue relates to training accuracy, which could fall under the distributions module."}, "predicted_module": {"module": "distributions", "evidence": "The issue involves training a detection model, which is related to distributions and probabilistic methods in the framework."}, "report_date": {"update": "2024-08-07", "evidence": "The issue was created on 2024-08-07."}, "last_update": {"update": "2024-11-19", "evidence": "The issue was last updated on 2024-08-07 as of the given information."}}
{"issue_number": 711, "issue_description": {"score": 2, "evidence": "The issue describes that CPU-only models like resnet50_quantized_qat and mobilenet_v2_quantized_qat are failing with a NotImplementedError indicating that the eval test only supports CPU."}, "error_message": {"score": 2, "evidence": "Traceback shows a NotImplementedError: The eval test only supports CPU."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "No specific reproduce steps are provided, but the issue mentions running tests which likely involve importing the models and triggering the eval method."}, "software_version": {"score": 2, "evidence": "PyTorch version is provided (d224857b3af5c9d5a3c7a48401475c09d0db296) and device info (pvc 1100, bundle 0.5.3, driver 803.61)."}, "platform": {"score": 2, "evidence": "Device information is provided including pvc 1100, bundle, and driver version."}}, "reporter": "mengfei25", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided."}, "impact": {"score": 2, "evidence": "The issue affects specific quantized models and their evaluation on non-CPU devices, potentially limiting the usability of these models on different hardware."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue involves quantized models, specifically resnet50_quantized_qat and mobilenet_v2_quantized_qat, which suggests the quantization module is involved."}, "predicted_module": {"module": "quant", "evidence": "The issue is related to quantized models, which typically fall under the quantization module."}, "report_date": {"update": "2024-08-07", "evidence": "The issue was created on 2024-08-07."}, "last_update": {"update": "2024-11-18", "evidence": "No further updates are provided beyond the initial report."}}
{"issue_number": 685, "issue_description": {"score": 0, "evidence": "Missing detailed issue description, only mentions performance requirement and need for data type dynamic cast."}, "error_message": {"score": 0, "evidence": "No error message provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps provided."}, "software_version": {"score": 0, "evidence": "No PyTorch version specified."}, "platform": {"score": 0, "evidence": "No platform information provided."}}, "reporter": "fengyuan14", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No resolution information provided."}, "root_cause": {"score": 0, "evidence": "No root cause information provided."}, "impact": {"score": 0, "evidence": "No impact information provided."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue relates to core functionality of reduction kernels in PyTorch."}, "predicted_module": {"module": "Core", "evidence": "The issue is about enhancing core reduction kernels to support data type dynamic cast."}, "report_date": {"update": "2024-08-04", "evidence": "Issue created on 2024-08-04 02:07:48+00:00."}, "last_update": {"update": "2025-03-12", "evidence": "No information on last update provided beyond creation date."}}
{"issue_number": 632, "issue_description": {"score": 2, "evidence": "The issue describes a problem with Squeezenet1_1 accuracy minify and provides a detailed code snippet that reproduces the issue."}, "error_message": {"score": 1, "evidence": "The code snippet includes an example of how to reproduce the issue, but the specific error message is not explicitly stated."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The code provided includes all necessary steps to reproduce the issue, including imports, model definition, and the run_repro function call."}, "software_version": {"score": 2, "evidence": "The versions of torch-xpu-ops and PyTorch are provided, along with specific commits."}, "platform": {"score": 2, "evidence": "Device information is given, including the PVC 1100, 881, 2024.2."}}, "reporter": "retonym", "assignee": "retonym", "resolution": {"score": 1, "evidence": "A PR was submitted, but the reasoning for reverting the PyTorch PR is mentioned briefly."}, "root_cause": {"score": 1, "evidence": "The issue is related to specific operations in the backward pass, but the exact root cause is not clearly explained."}, "impact": {"score": 1, "evidence": "The issue affects the accuracy of the Squeezenet1_1 model on XPU, which could impact performance and reliability."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue relates to the core functionality of the model's accuracy, which falls under the Core module."}, "predicted_module": {"module": "Core", "evidence": "The test case involves core model operations and accuracy, aligning with the Core module."}, "report_date": {"update": "2024-07-23", "evidence": "The issue was created on 2024-07-23."}, "last_update": {"update": "2025-01-17", "evidence": "The issue was created on 2024-07-23 and has not been updated since."}}
{"issue_number": 544, "issue_description": {"score": 2, "evidence": "The issue title is 'Evaluate std::log/log1p/log2 numerical difference.' and the body provides detailed information about numerical differences in log functions between CPU and XPU results, including specific test failures and discrepancies."}, "error_message": {"score": 2, "evidence": "The issue mentions specific test failures such as 'test_python_ref__refs_log2_xpu_complex128' and 'test_log1p_complex_xpu_complex64', with exact expected and received values and differences."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps are provided to reproduce the issue."}, "software_version": {"score": 0, "evidence": "No information about PyTorch version or platform is provided."}, "platform": {"score": 0, "evidence": "No platform information is given."}}, "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": "No information about resolution provided."}, "root_cause": {"score": 0, "evidence": "No information about root cause provided."}, "impact": {"score": 1, "evidence": "The issue affects numerical accuracy of log functions on XPU, potentially impacting applications relying on precise mathematical computations."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue relates to unit tests for mathematical functions, which fall under the UT module."}, "predicted_module": {"module": "UT", "evidence": "Based on the test case classifications and the nature of the issue, it is related to unit testing."}, "report_date": {"update": "2024-07-09", "evidence": "The issue was created on 2024-07-09."}, "last_update": {"update": "2025-02-27", "evidence": "No further updates mentioned beyond the initial creation."}}
{"issue_number": 506, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the error message and the context in which it occurs."}, "error_message": {"score": 2, "evidence": "The error message is specific and clearly states the type mismatch between input and bias types."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue does not provide explicit steps to reproduce the problem, but it includes the traceback and the test case (torchbench_bfloat16_trainingxpu)."}, "software_version": {"score": 2, "evidence": "The versions of torch-xpu-ops and PyTorch are clearly provided, along with the device information."}, "platform": {"score": 2, "evidence": "Device details are included in the version information."}}, "reporter": "mengfei25", "assignee": "jianyizh", "resolution": {"score": 0, "evidence": "No information is provided about the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 2, "evidence": "The issue affects the training of the Demucs model using bfloat16 precision, potentially causing failures in the training process."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue involves mixed precision training with bfloat16, which relates to quantization."}, "predicted_module": {"module": "quant", "evidence": "The issue involves mixed precision training with bfloat16, which relates to quantization."}, "report_date": {"update": "2024-06-28", "evidence": "The issue was created on 2024-06-28."}, "last_update": {"update": "2025-04-23", "evidence": "The issue was created on 2024-06-28 and is currently open."}}
{"issue_number": 492, "issue_description": {"score": 2, "evidence": "The issue describes a NotImplementedError when trying to train the Timm_efficientdet model using XPU with mixed precision training. The error message clearly indicates that the original model code enforces the use of CUDA, which is not compatible with XPU. The issue also provides the stack trace, which helps in understanding where the error occurs."}, "error_message": {"score": 2, "evidence": "The error message is 'NotImplementedError: The original model code forces the use of CUDA.' which is specific and directly points to the issue."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions the use of torchbench_amp_fp16_training but does not provide explicit step-by-step commands to reproduce the issue."}, "software_version": {"score": 2, "evidence": "The versions of torch-xpu-ops, PyTorch, and the device information are provided in detail."}, "platform": {"score": 2, "evidence": "The device details include the PVC model, compute units, and driver version."}}, "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": {"score": 0, "evidence": "No information is provided about the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue prevents the training of the Timm_efficientdet model on XPU using mixed precision, which affects the functionality and performance of the model on Intel hardware."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue relates to training a model, which falls under the transformers module."}, "predicted_module": {"module": "transformers", "evidence": "The issue is about training a model on XPU, which is related to the transformers module."}, "report_date": {"update": "2024-06-27", "evidence": "The issue was created on 2024-06-27."}, "last_update": {"update": "2025-03-12", "evidence": "The issue was last updated on 2024-06-27 as it was just created."}}
{"issue_number": 489, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the error message and the context in which it occurred."}, "error_message": {"score": 2, "evidence": "The error message clearly states that 'xpu not supported' and includes the traceback, which is helpful for understanding where the issue occurred."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue does not provide specific steps to reproduce the problem, but it does mention the context (e.g., training MOCO with xpu)."}, "software_version": {"score": 2, "evidence": "The versions of torch-xpu-ops, PyTorch, and device information are provided in detail."}, "platform": {"score": 2, "evidence": "The device details, including PVC 1100 and specific versions, are included."}}, "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": {"score": 2, "evidence": "Comments indicate that support for DDP models is in progress and a PR has been provided for the model script."}, "root_cause": {"score": 2, "evidence": "The issue is related to the need for supporting DDP models in the torchbench, which requires long-term effort."}, "impact": {"score": 1, "evidence": "The issue affects the training of the MOCO model on XPU devices using torchbench with specific data types, but the full impact is not clearly detailed."}, "state": "open", "labeled_module": {"module": "distributions", "evidence": "The labels include 'torchbench' and 'dtype' categories, which relate to the distributions or data types used in the benchmarking."}, "predicted_module": {"module": "distributions", "evidence": "The issue involves data type handling during training, which falls under the distributions category."}, "report_date": {"update": "2024-06-27", "evidence": "The issue was created on 2024-06-27."}, "last_update": {"update": "2025-05-08", "evidence": "No further updates are mentioned beyond the initial report."}}
{"issue_number": 461, "issue_description": {"score": 2, "evidence": "The issue title mentions that index put cases fail due to no support of FP8 data types. The issue body provides details about the feature request and the current state of the implementation, including specific test cases that are failing."}, "error_message": {"score": 0, "evidence": "No specific error message is provided in the issue."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps to reproduce the issue are provided."}, "software_version": {"score": 0, "evidence": "No information about the PyTorch version or platform is provided."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": "No information about the resolution progress or status is provided."}, "root_cause": {"score": 0, "evidence": "No specific root cause is identified in the issue."}, "impact": {"score": 0, "evidence": "No explicit information about the impact of the issue is provided."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue relates to FP8 data types, which are related to quantization."}, "predicted_module": {"module": "quant", "evidence": "The test cases mentioned involve FP8 data types, which are part of the quantization module."}, "report_date": {"update": "2024-06-25", "evidence": "The issue was created on 2024-06-25."}, "last_update": {"update": "2024-07-16", "evidence": "No further updates are provided beyond the initial creation date."}}


{"issue_number": 322, "issue_description": {"score": 2, "evidence": "The issue describes that FP8 support in matmul is causing test failures in test_matmul_cuda.py. It lists several test cases that are either skipped or failed due to FP8 support issues. The issue also mentions that FP8 is only supported on H100+ and sm_89 and MI300+ devices, which provides context on where the problem occurs. The environment details are provided, including the OS, CPU, and Python version, which helps in understanding the setup where the issue was encountered. The issue is concise and provides enough information to understand the problem and its impact."}, "error_message": {"score": 1, "evidence": "The error messages list test cases that are skipped or failed, but they do not provide specific error messages or details about what went wrong in those tests. This makes it harder to pinpoint the exact issue without more information."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps are provided to reproduce the issue. The issue mentions the tests that are failing or skipped but does not give any commands or scripts to run the tests, making it difficult to reproduce the problem."}, "software_version": {"score": 0, "evidence": "The PyTorch version is listed as N/A, which means it's unclear which version of PyTorch the issue is associated with. Without this information, it's hard to reproduce the issue in a controlled environment."}, "platform": {"score": 2, "evidence": "The platform details include the OS (Ubuntu 22.04.3 LTS), CPU (Intel Xeon Platinum 8480+), and Python version (3.12.2). This information is helpful for understanding the environment where the issue occurred."}}, "reporter": "yuchengliu1", "assignee": "liangan1", "resolution": {"score": 0, "evidence": "No information is provided about any attempted resolutions or fixes for the issue."}, "root_cause": {"score": 0, "evidence": "No analysis or speculation is provided about the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue affects specific test cases related to FP8 and mixed-dtype linear operations, which are important for performance and compatibility with newer hardware. However, the full impact on the overall system or user workflows is not clearly described."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue mentions test cases in test_matmul_cuda.py, which suggests it is related to unit tests (UT). Additionally, the label 'ut_triaged' is associated with the issue, further indicating it falls under the UT module."}, "predicted_module": {"module": "UT", "evidence": "The issue is about failing unit tests related to FP8 support in matmul operations, which directly ties to the UT module. The test case classifications also point towards issues within the unit testing framework or related test cases."}, "report_date": {"update": "2024-05-25", "evidence": "The issue was created on 2024-05-25, which is a specific date and helps in tracking the timeline of the issue."}, "last_update": {"update": "2024-10-15", "evidence": "The issue was last updated on the same date as it was created, indicating no further updates have been made since its creation."}}
{"issue_number": 275, "issue_description": {"score": 2, "evidence": "The issue description is clear and concise, explaining that the test_flip_xpu_float32 test is failing due to an error in the 'aten::empty_quantized' function."}, "error_message": {"score": 2, "evidence": "The error message clearly states the issue: 'NotImplementedError: Could not run 'aten::empty_quantized' with arguments from the 'QuantizedXPU' backend...'."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides specific steps to reproduce: 'PYTORCH_TEST_WITH_SLOW=1 python test/test_shape_ops.py -k TestShapeOpsXPU.test_flip_xpu_float32'."}, "software_version": {"score": 2, "evidence": "PyTorch version is specified as 2.4.0a0+git5fb11cd, and the OS is Ubuntu 22.04.3 LTS with details about GCC, Clang, etc."}, "platform": {"score": 2, "evidence": "Platform information includes OS, CPU details (Intel Xeon Platinum 8480+), and other relevant system specifics."}}, "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No information provided about the resolution."}, "root_cause": {"score": 0, "evidence": "No information provided about the root cause."}, "impact": {"score": 1, "evidence": "The issue affects the test_flip_xpu_float32 test, which is part of the test_shape_ops_xpu.py file, indicating potential problems with quantized operations on XPU devices."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue involves the 'empty_quantized' function, which is related to quantization."}, "predicted_module": {"module": "quant", "evidence": "The test case is in test_shape_ops_xpu.py and involves quantized operations, suggesting the quant module is relevant."}, "report_date": {"update": "2024-05-20", "evidence": "The issue was created on 2024-05-20."}, "last_update": {"update": "2024-07-15", "evidence": "The issue was last updated on 2024-05-20 as per the provided information."}}
{"issue_number": 264, "issue_description": {"score": 2, "evidence": "The issue describes four problems: 1) torch.random.fork_rng doesn't support XPU, 2) torch.xpu.FloatTensor isn't supported, 3) multiple devices aren't supported, 4) certain data types cause errors in cat operations."}, "error_message": {"score": 2, "evidence": "Detailed error messages are provided for each problem, including stack traces and specific test failures."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The user provided Python commands to run the tests, such as pytest and specific environment setups."}, "software_version": {"score": 1, "evidence": "Mentions Python 3.10.14 and PyTorch version is not explicitly stated but inferred from the context."}, "platform": {"score": 1, "evidence": "Running on Linux platform."}}, "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No resolution information provided."}, "root_cause": {"score": 0, "evidence": "No root cause analysis provided."}, "impact": {"score": 1, "evidence": "The issues affect test cases and functionality related to XPU support, which could impact the reliability of PyTorch's XPU backend."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled with 'ut_triaged' indicating it relates to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The issue involves test failures in test_tensor_creation_ops.py, which is part of the unit test suite."}, "report_date": {"update": "2024-05-17", "evidence": "Issue created on 2024-05-17."}, "last_update": {"update": "2025-01-14", "evidence": "No further updates mentioned after the initial report."}}
{"issue_number": 253, "issue_description": {"score": 2, "evidence": "The issue describes a problem with TestMathBitsXPU where 200 cases fail due to a OneDNN error when handling double and complex data types in matmul operations. It also lists all the failing test cases, indicating a widespread issue in matrix operations."}, "error_message": {"score": 2, "evidence": "The error is 'Double and complex datatype matmul is not supported in oneDNN', which is specific and clear."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes the command line for reproduction: 'ONEDNN_VERBOSE=2 PYTORCH_ENABLE_XPU_FALLBACK=1 PYTORCH_TEST_WITH_SLOW=1 pytest -v test_ops_xpu.py -k 'test_conj_view_addmm_xpu_complex64''. However, the steps are not fully detailed, and it's unclear if all necessary environment variables are specified."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned in the issue."}, "platform": {"score": 0, "evidence": "No platform or hardware details are provided."}}, "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": {"score": 2, "evidence": "The comments outline plans to fix the issues by updating oneDNN and adjusting code generation. The last update was on 2025-05-14."}, "root_cause": {"score": 2, "evidence": "The root causes include unsupported data types in oneDNN and issues with tensor dimensions. The last update was on 2025-05-14."}, "impact": {"score": 1, "evidence": "The issue affects 200 test cases, indicating significant impact on the test suite, particularly for complex and double data types in matrix operations."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled with 'ut_triaged', which relates to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The failing tests are part of test_ops_xpu.py, which is related to unit testing for PyTorch XPU operations."}, "report_date": {"update": "2024-05-16", "evidence": "The issue was created on 2024-05-16."}, "last_update": {"update": "2025-05-14", "evidence": "No further updates are mentioned beyond the initial report."}}
{"issue_number": 208, "issue_description": {"score": 2, "evidence": "The issue provides a clear description of the problem and the motivation behind the feature request."}, "error_message": {"score": 0, "evidence": "No specific error message is provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps are provided."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is mentioned."}, "impact": {"score": 1, "evidence": "The impact is somewhat clear as it relates to code reuse and efficiency but could be more detailed."}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "The label explicitly states the module as 'OP Impl.'"}, "predicted_module": {"module": "OP impl", "evidence": "The issue description focuses on operator implementation and sharing utility functions across backends."}, "report_date": {"update": "2024-05-13", "evidence": "The issue was created on 2024-05-13."}, "last_update": {"update": "2024-10-15", "evidence": "No further updates are provided beyond the initial creation date."}}
{"issue_number": 184, "issue_description": {"score": 2, "evidence": "Issue title and body provide clear and concise information about the accuracy issues in various operations such as tanh, bfloat16, and float16 computations, along with steps to reproduce and error messages."}, "error_message": {"score": 2, "evidence": "Detailed error messages and stack traces are provided for each failed test case, including specific discrepancies and indices where failures occurred."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Clear shell commands are provided to run the tests, including setting environment variables and executing the test script."}, "software_version": {"score": 1, "evidence": "The pytorch version is mentioned in the test command but not explicitly stated elsewhere."}, "platform": {"score": 2, "evidence": "Tests were run on an Intel device with specific Sycl issues reported."}}, "reporter": "daisyden", "assignee": "huaiyuzh", "resolution": {"score": 0, "evidence": "No information provided on how to resolve the issue."}, "root_cause": {"score": 0, "evidence": "No detailed analysis of the root cause is provided."}, "impact": {"score": 1, "evidence": "The issue affects multiple operations and test cases, potentially impacting the reliability of the XPU implementation."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled under 'ut_triaged', indicating it relates to unit testing."}, "predicted_module": {"module": "OP impl", "evidence": "The issue involves accuracy gaps in specific operations, suggesting it relates to the implementation of these operations."}, "report_date": {"update": "2024-05-06", "evidence": "The issue was created on 2024-05-06."}, "last_update": {"update": "2024-07-22", "evidence": "No further updates are mentioned beyond the initial report."}}
{"issue_number": 146, "issue_description": {"score": 2, "evidence": "The issue is about performance evaluation. A common performance tuning is to trade-off register usage and concurrency. From SYCL compiler log, we find some warnings about register spill. Need evaluation."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue is related to SYCL kernel performance and register usage, which falls under the core functionality of the library."}, "predicted_module": {"module": "Core", "evidence": "The issue deals with performance evaluation and register spill warnings in SYCL kernels, which are part of the core operations in PyTorch XPU."}, "report_date": {"update": "2024-04-16", "evidence": "The issue was created at 2024-04-16 11:51:24+00:00."}, "last_update": {"update": "2024-10-15", "evidence": "The issue was created at 2024-04-16 11:51:24+00:00."}}
{"issue_number": 126, "issue_description": {"score": 2, "evidence": "The issue provides a detailed description of the problem, including functionality and performance discrepancies between IPEX and stock CUDA implementations."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue mentions 'kernel implementation' and performance optimizations, which relate to quantization modules."}, "predicted_module": {"module": "quant", "evidence": "The focus on aligning IPEX with CUDA implementations suggests quantization-related optimizations."}, "report_date": {"update": "2024-04-08", "evidence": "The issue was created on 2024-04-08."}, "last_update": {"update": "2024-10-15", "evidence": "The issue was created on 2024-04-08."}}
