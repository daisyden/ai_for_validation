{"issue_number": 1693, "issue_description": {"score": 0, "evidence": "The issue description is missing detailed information about the problem, error messages, and reproduction steps."}, "error_message": {"score": 0, "evidence": "No error message is provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps are provided."}, "software_version": {"score": 0, "evidence": "No software version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "xytintel", "assignee": "yucai-intel", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is mentioned."}, "impact": {"score": 0, "evidence": "No information about the impact of the issue is provided."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue is related to implementing a fused RMSNorm, which is commonly used in transformer layers."}, "predicted_module": {"module": "transformers", "evidence": "The issue involves implementing a fused RMSNorm, which is typically part of transformer modules in PyTorch."}, "report_date": {"update": "2025-05-23", "evidence": "The issue was last updated on 2025-05-23."}, "last_update": {"update": "2025-05-23", "evidence": "The issue was last updated on 2025-05-23."}}
{"issue_number": 1684, "issue_description": {"score": 2, "evidence": "The issue description is clear and provides detailed steps to reproduce the problem, including code snippets and expected results."}, "error_message": {"score": 2, "evidence": "The error message clearly states that the exported model retains FP32 parameters alongside INT8, failing to reduce memory usage."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Detailed reproduce steps are provided, including Python scripts and commands to run the example."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0.dev20250515+xpu is specified."}, "platform": {"score": 2, "evidence": "The platform is Ubuntu 22.04.5 LTS with specific Python and other environment details provided."}}, "reporter": "ZhaoqiongZ", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No information about the resolution is provided."}, "root_cause": {"score": 0, "evidence": "No root cause information is provided."}, "impact": {"score": 2, "evidence": "The impact is significant as it prevents memory footprint reduction, leading to increased memory usage when loading the quantized model."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The label indicates the issue is related to the 'quant' module."}, "predicted_module": {"module": "quant", "evidence": "The issue description and test case focus on quantization, specifically with INT8 parameters and the use of PyTorch's quantization tools."}, "report_date": {"update": "2025-05-22", "evidence": "The issue was last updated on 2025-05-22, and created on 2025-05-20."}, "last_update": {"update": "2025-05-22", "evidence": "The last update was on 2025-05-22."}}
{"issue_number": 1682, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the test case that failed and the error message."}, "error_message": {"score": 2, "evidence": "The error message clearly states the assertion failure with details about the tensor mismatch."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides specific commands to reproduce the test, such as running a Python script with a specific test method."}, "software_version": {"score": 2, "evidence": "PyTorch version and other relevant library versions are included in the environment information."}, "platform": {"score": 2, "evidence": "Detailed system information, including OS, CPU, and other hardware details, is provided."}}, "reporter": "PenghuiCheng", "assignee": "ashokei", "resolution": {"score": 0, "evidence": "No information provided on the resolution."}, "root_cause": {"score": 0, "evidence": "No information provided on the root cause."}, "impact": {"score": 2, "evidence": "The issue affects the accuracy of distributed pipelining tests, which could impact the reliability of distributed training."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The label explicitly mentions 'bug.module: distributed'."}, "predicted_module": {"module": "distributed", "evidence": "The test case is in the distributed pipelining folder, indicating the issue is related to distributed training."}, "report_date": {"update": "2025-05-20", "evidence": "The issue was last updated on this date, indicating recent activity."}, "last_update": {"update": "2025-05-20", "evidence": "The issue was last updated on this date, indicating recent activity."}}
{"issue_number": 1681, "issue_description": {"score": 2, "evidence": "The issue describes the lack of support for the torch.histc operation with half precision on XPU, which is needed for the PT2E calibration process. The reporter provides example code that reproduces the issue, making the description clear and concise."}, "error_message": {"score": 2, "evidence": "The example code demonstrates the error when using torch.histc with half tensors on XPU."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes a detailed script that demonstrates the problem, including the setup, data loading, model training, and the specific function where the issue occurs."}, "software_version": {"score": 2, "evidence": "The PyTorch version is specified as 2.8.0.dev20250515+xpu, and the platform is Ubuntu 22.04.5 LTS with an Intel Xeon CPU."}, "platform": {"score": 2, "evidence": "The platform details include the OS, CPU architecture, and hardware specifications."}}, "reporter": "ZhaoqiongZ", "assignee": "xytintel", "resolution": {"score": 1, "evidence": "The second comment provides a link to a pull request, which suggests a resolution but does not explain it in detail."}, "root_cause": {"score": 1, "evidence": "The first comment questions the necessity of supporting the dtype since CUDA doesn't, which touches on the root cause but is brief."}, "impact": {"score": 2, "evidence": "The issue impacts the PT2E quantization process, which is essential for optimizing models using half precision on XPU devices."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue is labeled under 'module: op impl' and relates to the implementation of PyTorch operations on XPU, specifically affecting quantization processes."}, "predicted_module": {"module": "quant", "evidence": "The issue relates to the PT2E quantization process and the implementation of torch.histc for half precision on XPU."}, "report_date": {"update": "2025-05-23", "evidence": "The issue was last updated on 2025-05-23."}, "last_update": {"update": "2025-05-23", "evidence": "The issue was last updated on 2025-05-23."}}
{"issue_number": 1678, "issue_description": {"score": 2, "evidence": "The issue describes a problem with `model.share_memory()` on XPU devices, providing a detailed error message and steps to reproduce."}, "error_message": {"score": 2, "evidence": "The error message is clearly provided: `RuntimeError: _share_fd_: only available on CPU`."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The user provides a GitHub link with the example code and instructions to run it using `pip install -r requirements.txt` and `python main.py --accel`."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0.dev20250515+xpu is specified along with other relevant library versions."}, "platform": {"score": 2, "evidence": "The platform details include Ubuntu 22.04.5 LTS, Python 3.12.10, and hardware information."}}, "reporter": "jafraustro", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No information provided about the resolution process or timeline."}, "root_cause": {"score": 0, "evidence": "No information provided about the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue affects the functionality of `model.share_memory()` on XPU, potentially impacting distributed training or multi-process data loading."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue relates to tensor operations and memory sharing, which is part of the core functionality of PyTorch."}, "predicted_module": {"module": "Core", "evidence": "The problem occurs during a basic operation (`share_memory()`), suggesting it's a core issue."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1674, "issue_description": {"score": 2, "evidence": "The issue description is concise and provides details about the bug, including the test case, error message, and stack trace."}, "error_message": {"score": 2, "evidence": "The error message clearly indicates the test failure with specific tensor comparison issues."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "Reproduce steps are mentioned but not very clear. The user is instructed to run a specific test but not the exact command or setup needed."}, "software_version": {"score": 2, "evidence": "PyTorch version and other relevant software versions are provided in detail."}, "platform": {"score": 2, "evidence": "Comprehensive platform information, including OS, CPU, and libraries, is given."}}, "reporter": "PenghuiCheng", "assignee": "githubsgi", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause information is provided."}, "impact": {"score": 2, "evidence": "The issue affects the accuracy of distributed tensor operations, which is critical for distributed training."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The label explicitly states 'bug.module: distributed.'"}, "predicted_module": {"module": "distributed", "evidence": "The test case is in the distributed/tensor directory, indicating it's related to distributed computing."}, "report_date": {"update": "2025-05-20", "evidence": "The issue was last updated on 2025-05-20."}, "last_update": {"update": "2025-05-20", "evidence": "The issue was last updated on 2025-05-20."}}
{"issue_number": 1668, "issue_description": {"score": 2, "evidence": "The issue description is clear and concise, detailing the bug in the distributed module related to accuracy issues in the _composable compile tests."}, "error_message": {"score": 2, "evidence": "The error message includes detailed tensor comparisons, indicating a clear problem with tensor values during the test."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides specific test commands to reproduce the bug, making it straightforward to replicate the issue."}, "software_version": {"score": 2, "evidence": "PyTorch version and oneAPI version are clearly provided, aiding in accurate reproduction."}, "platform": {"score": 2, "evidence": "The OS, CPU, and other hardware details are included, which are essential for reproducing the issue."}}, "reporter": "PenghuiCheng", "assignee": "zhangxiaoli73", "resolution": {"score": 0, "evidence": "No information is provided about potential resolutions or fixes."}, "root_cause": {"score": 0, "evidence": "The issue does not delve into possible causes, so no evidence is available."}, "impact": {"score": 2, "evidence": "The bug affects the accuracy of distributed training, which is critical for model performance and deployment."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under the 'bug.module: distributed' category, clearly indicating the module."}, "predicted_module": {"module": "distributed", "evidence": "The test case classifications and the issue description point to the distributed module as the source of the problem."}, "report_date": {"update": "2025-05-15", "evidence": "The issue was last updated on 2025-05-15, providing a clear timeline for the problem."}, "last_update": {"update": "2025-05-15", "evidence": "The issue was last updated on 2025-05-15, indicating recent activity."}}
{"issue_number": 1667, "issue_description": {"score": 2, "evidence": "The issue is titled '[distributed] AssertionError: 'setattr() on Tensor.requires_grad' not found in 'Attempted to call function marked as skipped', which clearly indicates the problem is related to a failed assertion in a distributed test involving Tensor attribute setting. The body provides detailed error logs and context about the test failure, making the issue description concise and clear."}, "error_message": {"score": 2, "evidence": "The error message is 'AssertionError: 'setattr() on Tensor.requires_grad' not found in 'Attempted to call function marked as skipped...' which is specific and provides context about the test failure. The stack trace and logs are included, making it clear where the issue occurred."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes a test file and test name, but specific steps to reproduce are not clearly outlined. The user is instructed to run a specific test, but the exact commands are not detailed in a reproducible way."}, "software_version": {"score": 2, "evidence": "The PyTorch version is provided as 2.8.0a0+gita2b2fc6, and the platform details are included, making it clear which version and environment the issue occurs in."}, "platform": {"score": 2, "evidence": "The OS, Python version, and hardware details are provided, allowing for accurate reproduction setup."}}, "reporter": "PenghuiCheng", "assignee": "newtdms", "resolution": {"score": 0, "evidence": "No information is provided about any attempted resolutions or fixes."}, "root_cause": {"score": 0, "evidence": "The issue does not specify the root cause of the problem, only the symptoms and error logs."}, "impact": {"score": 0, "evidence": "The impact of the issue on users or the system is not discussed or quantified."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled with 'bug.module: distributed', clearly indicating the affected module."}, "predicted_module": {"module": "distributed", "evidence": "The test case is in 'test_dynamo_distributed.py', which relates to distributed computing functionality. The error message involves setting attributes on tensors, which is part of the distributed processing logic."}, "report_date": {"update": "2025-05-20", "evidence": "The issue was last updated on 2025-05-20, indicating recent activity."}, "last_update": {"update": "2025-05-20", "evidence": "The issue was last updated on 2025-05-20, showing the most recent activity."}}
{"issue_number": 1666, "issue_description": {"score": 2, "evidence": "The issue describes a bug in FSDP checkpoint tests, providing details about the failing tests and error messages."}, "error_message": {"score": 2, "evidence": "The error message includes a traceback and specific tensor comparison failure details, indicating a discrepancy in model outputs."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue provides a code snippet for the failing test but lacks a clear, step-by-step guide on how to reproduce the issue."}, "software_version": {"score": 2, "evidence": "The PyTorch version and oneAPI version are provided, along with the operating system and hardware details."}, "platform": {"score": 2, "evidence": "The platform information includes OS, CPU details, and installed libraries, which are essential for reproduction."}}, "reporter": "PenghuiCheng", "assignee": "githubsgi", "resolution": {"score": 0, "evidence": "No information is provided about the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue affects FSDP checkpoint functionality, potentially causing test failures and reliability issues in distributed training."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under the 'bug.module: distributed' category."}, "predicted_module": {"module": "distributed", "evidence": "The test case is related to FSDP, which falls under the distributed training module."}, "report_date": {"update": "2025-05-15", "evidence": "The issue was last updated on 2025-05-15, indicating recent activity."}, "last_update": {"update": "2025-05-15", "evidence": "The issue was last updated on 2025-05-15."}}
{"issue_number": 1665, "issue_description": {"score": 2, "evidence": "The issue describes a bug in PyTorch where the BackendCompilerFailed exception is raised during FSDP2 compilation using the 'inductor' backend. It provides the test case that failed and the error message indicating an unexpected number of `fsdp.copy_` operations."}, "error_message": {"score": 2, "evidence": "The error message is 'AssertionError: Scalars are not equal! Expected 4 but got 0.' and 'Unexpected number of `fsdp.copy_` ops (expected 4, got 0) in graph: ..."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The steps to reproduce the issue are provided via a test script and specific commands to run the test."}, "software_version": {"score": 2, "evidence": "PyTorch version is mentioned as 'https://github.com/daisyden/pytorch/tree/distributed_2.8' and oneAPI version is '2025.1.1'."}, "platform": {"score": 2, "evidence": "The issue occurred on a platform using oneAPI and Intel's torch-xpu-ops."}}, "reporter": "PenghuiCheng", "assignee": "zhangxiaoli73", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects the FSDP2 compilation process, potentially causing failures in distributed training scenarios using the inductor backend."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under 'bug.module: distributed'."}, "predicted_module": {"module": "distributed", "evidence": "The test case relates to FSDP2, which is part of the distributed computing module in PyTorch."}, "report_date": {"update": "2025-05-19", "evidence": "The issue was last updated on 2025-05-19."}, "last_update": {"update": "2025-05-19", "evidence": "The issue was last updated on 2025-05-19."}}
{"issue_number": 1663, "issue_description": {"score": 2, "evidence": "The issue description provides details about the bug, including the test case, error messages, and relevant logs. It mentions the test `test_dp_state_dict_cpu_offload` failing with exit code -9, which is clear and concise."}, "error_message": {"score": 2, "evidence": "The error message states 'Expected zero exit code but got -9', which is specific and to the point."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The test command `pytest -vs test/distributed/_composable/fsdp/test_fully_shard_state_dict.py::TestFullyShardStateDictMultiProcess::test_dp_state_dict_cpu_offload` is provided, allowing for reproduction."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0a0+gita2b2fc6 is specified, along with the platform details."}, "platform": {"score": 2, "evidence": "The OS is Ubuntu 22.04.4 LTS, with specific CPU and other hardware details provided."}}, "reporter": "PenghuiCheng", "assignee": "pkourdis", "resolution": {"score": 0, "evidence": "No information provided about the resolution."}, "root_cause": {"score": 0, "evidence": "No information provided about the root cause."}, "impact": {"score": 1, "evidence": "The issue affects distributed training, potentially causing test failures and impacting the reliability of the FSDP implementation."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled with `bug.module: distributed`, indicating the affected module."}, "predicted_module": {"module": "distributed", "evidence": "The test case pertains to distributed functionality, specifically FSDP (Fully Sharded Data Parallel), which falls under the distributed module."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1656, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the test case, the discrepancy between XPU and CPU results, and the version information."}, "error_message": {"score": 2, "evidence": "The issue includes specific error messages indicating that tensors do not match within tolerance, which clearly describes the problem."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides a Python script snippet for reproducing the issue, including the setup of tensors and the test case."}, "software_version": {"score": 2, "evidence": "PyTorch version 2.8.0.dev20250512+xpu is clearly stated."}, "platform": {"score": 2, "evidence": "The platform details include the OS, CPU, and other environment information."}}, "reporter": "hoshibara", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No information provided on the resolution."}, "root_cause": {"score": 0, "evidence": "No information provided on the root cause."}, "impact": {"score": 2, "evidence": "The issue affects the correctness of the Flex Attention operation, which is critical for models relying on it."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under dependency component: oneDNN, indicating it relates to dependencies."}, "predicted_module": {"module": "UT", "evidence": "The issue arises from running unit tests, specifically for Flex Attention, suggesting it's related to the test suite."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1649, "issue_description": {"score": 2, "evidence": "The issue describes a bug where using inconsistent OneAPI versions leads to an ImportError. The description is clear and concise."}, "error_message": {"score": 1, "evidence": "The error message mentions a missing version 'LIBUR_LOADER_0.10' but does not explicitly state the issue of inconsistent OneAPI versions."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps are provided to reproduce the issue."}, "software_version": {"score": 1, "evidence": "The PyTorch version and other relevant library versions are mentioned but not in a clear reproduce steps section."}, "platform": {"score": 2, "evidence": "The OS, GCC, Python, and other system details are provided, making the platform information clear."}}, "reporter": "ZhaoqiongZ", "assignee": "dvrogozh", "resolution": {"score": 0, "evidence": "No resolution information is provided in the issue."}, "root_cause": {"score": 1, "evidence": "The root cause is hinted at being version mismatch but not clearly explained."}, "impact": {"score": 1, "evidence": "The issue impacts the functionality of C++ extensions in PyTorch when OneAPI versions are inconsistent, but the impact is not fully detailed."}, "state": "open", "labeled_module": {"module": "build", "evidence": "The issue is labeled under 'module: build', indicating it relates to the build process."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue involves version incompatibility between OneAPI versions, which falls under dependency management."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, but the exact creation date is provided as 2025-05-09."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1641, "issue_description": {"score": 0, "evidence": "The issue description is missing detailed information about the problem, error messages, or steps to reproduce."}, "error_message": {"score": 0, "evidence": "No specific error message is provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No steps to reproduce the issue are provided."}, "software_version": {"score": 0, "evidence": "No information about the PyTorch version is provided."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "xytintel", "assignee": "chunhuanMeng", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause information is provided."}, "impact": {"score": 0, "evidence": "No information about the impact of the issue is provided."}, "state": "open", "labeled_module": {"module": "N/A", "evidence": "No labels are provided in the issue."}, "predicted_module": {"module": "N/A", "evidence": "The issue lacks sufficient details to predict a module."}, "report_date": {"update": "2025-05-15", "evidence": "Last updated on 2025-05-15, created on 2025-05-07."}, "last_update": {"update": "2025-05-15", "evidence": "Last updated on 2025-05-15, created on 2025-05-07."}}
{"issue_number": 1636, "issue_description": {"score": 2, "evidence": "The issue description clearly states that three FSDP test cases failed with specific AssertionError messages, including details about the tests and the reproduce command."}, "error_message": {"score": 2, "evidence": "The error messages are specific, mentioning 'AssertionError: Tensor-likes are not close!' and 'Scalars are not equal!', which are clear and reproducible."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue provides a 'reproduce cmd' with pytest command, but it lacks detailed setup steps and environment specifics, making it somewhat unclear for full reproducibility."}, "software_version": {"score": 1, "evidence": "The pytorch version is not explicitly mentioned, though the test cases imply a specific setup. Platform information is also missing."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "daisyden", "assignee": "zhangxiaoli73", "resolution": {"score": 0, "evidence": "No information is provided about the resolution process or timeline."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects specific FSDP test cases, impacting the distributed training functionality, but the broader impact is not clearly detailed."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under 'bug.module: distributed'."}, "predicted_module": {"module": "distributed", "evidence": "The test cases involve FSDP, which is part of PyTorch's distributed training functionality."}, "report_date": {"update": "2025-05-13", "evidence": "The issue was last updated on 2025-05-13."}, "last_update": {"update": "2025-05-13", "evidence": "The issue was last updated on 2025-05-13."}}
{"issue_number": 1634, "issue_description": {"score": 2, "evidence": "The issue is titled 'test_linear_row_wise_parallel failed with weight not equal between dist and non-dist' and the body provides details about the test failure, including error messages about tensor-like mismatches and differences in weights between distributed and non-distributed setups."}, "error_message": {"score": 2, "evidence": "The error message indicates that tensors are not close, with specific differences in elements and indices, and mentions warnings about converting tensors with requires_grad=True to scalars."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions that the test can be run with 'PYTORCH_TEST_WITH_SLOW=1 python test/distributed/tensor/parallel/test_parallelize_api.py TensorParallelAPITests.test_linear_row_wise_parallel', but no clear step-by-step instructions are provided."}, "software_version": {"score": 0, "evidence": "No specific version of PyTorch is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "daisyden", "assignee": "syedshahbaaz", "resolution": {"score": 0, "evidence": "No information about the resolution is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue description."}, "impact": {"score": 0, "evidence": "The impact of the issue is not described, but since it's a test failure in distributed tensor parallelism, it could affect the correctness of distributed training in PyTorch."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The label is explicitly given as 'bug.module: distributed.'"}, "predicted_module": {"module": "distributed", "evidence": "The test case is related to distributed tensor parallelism, and the error is in the distributed setup."}, "report_date": {"update": "2025-05-12", "evidence": "The issue was last updated on 2025-05-12 20:42:15+00:00."}, "last_update": {"update": "2025-05-12", "evidence": "The issue was last updated on 2025-05-12 20:42:15+00:00."}}
{"issue_number": 1626, "issue_description": {"score": 2, "evidence": "The issue describes a problem with a custom C++ extension in PyTorch where the opcheck test fails due to an autograd registration issue on XPU devices. The error message indicates that autograd registration is not implemented for devices other than CPU and CUDA, specifically encountering {'xpu'}. The reporter followed the example from the PyTorch extension-cpp repository but encountered this error during testing. The issue includes detailed reproduce steps, environment setup, and the specific error messages observed during the test runs. The information is clear and concise, providing enough context to understand the problem and how to reproduce it."}, "error_message": {"score": 2, "evidence": "The error message clearly states the NotImplementedError: autograd_registration_check: NYI devices other than CPU/CUDA, got {'xpu'}. Additionally, the stack trace is provided, which helps in pinpointing where the failure occurs in the code. The message is specific and directly indicates the issue with autograd registration for XPU devices."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides a comprehensive set of steps to reproduce the problem, including setting up a conda environment, installing necessary packages, cloning the repository, building, and running the test. The commands are detailed and executable, making it easy for others to replicate the issue. This is crucial for debugging and verifying potential fixes."}, "software_version": {"score": 2, "evidence": "The versions of PyTorch, Torchvision, Torchaudio, and other relevant libraries are provided. The specific PyTorch version is 2.8.0.dev20250427+xpu, and the platform is Ubuntu 24.10. This information is essential for understanding the environment where the issue occurs and ensuring that any fixes are compatible with the same versions."}, "platform": {"score": 2, "evidence": "The platform details include the CPU architecture, OS version, GCC and Clang versions, CMake version, and other system specifics. This information is vital for troubleshooting as it provides context about the environment in which the issue is encountered."}}, "reporter": "ZhaoqiongZ", "assignee": "dvrogozh", "resolution": {"score": 1, "evidence": "While the comments discuss the feasibility and challenges of adding support, they do not provide a clear resolution plan or timeline. There is a mention of a PR that started the work but no indication of its current status or next steps."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as the use of a deprecated API (`torch._custom_ops`) which is planned for removal in PyTorch 2.6. This is a significant factor in the difficulty of implementing support for XPU devices in `torch.library.opcheck()`."}, "impact": {"score": 2, "evidence": "The issue impacts the development of custom PyTorch operators, particularly for XPU devices. If autograd is not properly registered, it can hinder the training of neural networks that rely on gradients computed by PyTorch's autograd system. This can affect the usability and performance of PyTorch for developers using XPU hardware."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue relates to autograd registration, which is a core part of PyTorch's computation graph and differentiation mechanism. The failure in autograd_registration_check suggests a problem in the core autograd system when handling non-CPU/CUDA devices like XPU."}, "predicted_module": {"module": "Core", "evidence": "The issue is related to the core functionality of PyTorch's autograd system, which is responsible for automatic differentiation. The error occurs during the registration check, indicating a problem within the core modules handling device support for autograd."}, "report_date": {"update": "2025-05-07", "evidence": "The issue was last updated on 2025-05-07, and it was created on 2025-04-29. This indicates that the issue is relatively new and is still being addressed as of the last update date."}, "last_update": {"update": "2025-05-07", "evidence": "The issue was last updated on 2025-05-07, which is recent, suggesting that the maintainers are actively looking into the problem or that there have been recent discussions or updates related to it."}}
{"issue_number": 1623, "issue_description": {"score": 2, "evidence": "The issue describes a problem with low scaling efficiency in distributed training using TorchTune, providing specific test cases and performance metrics."}, "error_message": {"score": 1, "evidence": "The user provided performance metrics but no specific error messages or logs."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The user provided links to test scripts, but the steps are not clearly outlined in the issue description."}, "software_version": {"score": 2, "evidence": "PyTorch version and other relevant software versions are provided in detail."}, "platform": {"score": 2, "evidence": "Detailed platform information, including CPU specs and OS, is provided."}}, "reporter": "zxd1997066", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": "No information provided on the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No information provided on the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue affects the scaling efficiency of distributed training, impacting performance in multi-GPU setups."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue has the label 'bug.module: distributed'."}, "predicted_module": {"module": "distributed", "evidence": "Based on the label and test case classifications, the issue likely pertains to the distributed module."}, "report_date": {"update": "2025-05-22", "evidence": "The issue was last updated on 2025-05-22."}, "last_update": {"update": "2025-05-22", "evidence": "The issue was last updated on 2025-05-22."}}
{"issue_number": 1618, "issue_description": {"score": 2, "evidence": "The issue describes an error where the function `current_stream` is marked as skipped by Dynamo, leading to an exception during autograd in distributed training with FSDP. The error occurs in multiple test cases related to FSDP and autograd compilation."}, "error_message": {"score": 2, "evidence": "The error message clearly states `torch._dynamo.exc.Unsupported: Attempted to call function marked as skipped` and provides a traceback indicating the problematic function and its location."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes a test command but lacks detailed steps to reproduce the issue, such as specific environment setups or code snippets."}, "software_version": {"score": 2, "evidence": "The PyTorch version is specified as `2.8.0a0+gite558eaa`, and the platform is Ubuntu 22.04.5 LTS with Python 3.10.17."}, "platform": {"score": 2, "evidence": "The platform details include OS, CPU, and other system information which is helpful for reproduction."}}, "reporter": "zxd1997066", "assignee": "guangyey", "resolution": {"score": 2, "evidence": "The comments indicate that adding `torch.xpu.current_stream` and `torch.xpu.stream` to `torch_non_c_binding_in_graph_functions` resolves the issue, and the root cause was the exclusion of these functions in the trace rules."}, "root_cause": {"score": 1, "evidence": "The error points to a conflict between Dynamo and the `current_stream` function in the XPU module, suggesting that Dynamo is skipping a function it shouldn't or the function is not properly handled."}, "impact": {"score": 1, "evidence": "The issue affects multiple distributed training tests with FSDP, potentially breaking distributed training functionality when using Dynamo for autograd compilation."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under `bug.module: distributed`."}, "predicted_module": {"module": "distributed", "evidence": "The test cases failing are related to distributed FSDP and autograd, indicating the issue is within the distributed module."}, "report_date": {"update": "2025-05-19", "evidence": "The issue was last updated on 2025-05-19."}, "last_update": {"update": "2025-05-19", "evidence": "The issue was last updated on 2025-05-19."}}
{"issue_number": 1617, "issue_description": {"score": 2, "evidence": "The issue has a clear title and body describing the problem with detailed test cases and error messages."}, "error_message": {"score": 2, "evidence": "RuntimeError: eof (this error originated at tensorpipe/transport/shm/connection_impl.cc:259)"}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps provided to reproduce the issue."}, "software_version": {"score": 0, "evidence": "No information about PyTorch version or platform provided."}, "platform": {"score": 0, "evidence": "No platform information given."}}, "reporter": "PenghuiCheng", "assignee": "syedshahbaaz", "resolution": {"score": 2, "evidence": "The comments suggest that the issue is being worked on and that the root cause is identified."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as the UTs not being ported to XPU."}, "impact": {"score": 1, "evidence": "The issue affects distributed training, causing test failures with timeout errors."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The label explicitly states 'bug.module: distributed'."}, "predicted_module": {"module": "distributed", "evidence": "The test cases are in the distributed module and the error relates to tensorpipe transport which is part of distributed communication."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1616, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the bug encountered during the test of ShardedTensor."}, "error_message": {"score": 2, "evidence": "The error message indicates an attempt to send a tensor with an unexpected device type 'xpu:3'."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue does not provide explicit steps to reproduce the issue, but the test case mentioned is test_init_from_local_shards in test_sharded_tensor.py."}, "software_version": {"score": 1, "evidence": "The issue mentions Python 3.10.16 and PyTorch, but the exact version is unclear."}, "platform": {"score": 1, "evidence": "The platform is Linux, but specific details are not provided."}}, "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified."}, "impact": {"score": 1, "evidence": "The issue impacts the distributed training functionality, potentially affecting the reliability of ShardedTensor usage."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The label explicitly states the module as 'distributed'."}, "predicted_module": {"module": "distributed", "evidence": "The test case is related to distributed tensors and the error occurs in the distributed RPC component."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1614, "issue_description": {"score": 2, "evidence": "The issue describes a performance drop in models like BartForCausalLM when using oneDNN v3.8-rc compared to v3.7.1. The reporter provided benchmarking commands and performance metrics showing significant slowdowns."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The reporter provided a Python command to reproduce the issue: `python benchmarks/dynamo/huggingface.py --performance --amp --amp-dtype bfloat16 -d xpu -n10 --inference  --only BartForCausalLM --backend=inductor`"}, "software_version": {"score": 2, "evidence": "oneDNN version v3.8-rc and v3.7.1 are specified, along with PyTorch version through the use of `--backend=inductor` which suggests PyTorch with Inductor."}, "platform": {"score": 2, "evidence": "The issue mentions XPU devices (`-d xpu`) and the use of Hugging Face benchmarks, indicating the platform is related to Intel's hardware."}}, "reporter": "mengfei25", "assignee": "LuFinch", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 2, "evidence": "The performance drop is significant (~70%) and affects specific models, impacting the efficiency of neural network inference."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue involves models from Hugging Face transformers, specifically BartForCausalLM."}, "predicted_module": {"module": "transformers", "evidence": "The test case involves a transformer model (BartForCausalLM), suggesting the issue relates to the transformers module."}, "report_date": {"update": "2025-05-07", "evidence": "The issue was last updated on 2025-05-07."}, "last_update": {"update": "2025-05-07", "evidence": "The issue was last updated on 2025-05-07."}}
{"issue_number": 1606, "issue_description": {"score": 2, "evidence": "The issue describes a performance regression when using oneDNN v3.8 compared to v3.7.1. It provides benchmark results from both versions, showing a significant drop in performance. The description is concise and clear."}, "error_message": {"score": 0, "evidence": "No specific error message is provided."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides the Python command to reproduce the issue, which is `python benchmarks/dynamo/huggingface.py --performance --amp --amp-dtype bfloat16 -d xpu -n10 --inference  --only GPT2ForSequenceClassification --backend=inductor`. The steps are clear and concise."}, "software_version": {"score": 2, "evidence": "The PyTorch version used is specified as `release/2.7`."}, "platform": {"score": 2, "evidence": "The platform used is specified as `xpu`."}}, "reporter": "mengfei25", "assignee": "LuFinch", "resolution": {"score": 2, "evidence": "The comment provides a clear link to a PR that fixes the regression issue, indicating a resolution path."}, "root_cause": {"score": 1, "evidence": "The comment links to a PR but does not explicitly state the root cause of the regression."}, "impact": {"score": 2, "evidence": "The issue mentions a significant performance drop, which could impact the efficiency of machine learning models using oneDNN v3.8."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under E2E.performance.hw: PVC.regression, which suggests it relates to hardware performance regression."}, "predicted_module": {"module": "distributed", "evidence": "The issue involves oneDNN performance regression, which is related to the distributed computing aspect of PyTorch."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1605, "issue_description": {"score": 2, "evidence": "The issue reports an AssertionError in the test_fully_shard_memory test, specifically comparing memory usage which exceeds expected limits."}, "error_message": {"score": 2, "evidence": "AssertionError: 177 not less than or equal to 163.904256"}, "reproduce_steps": {"steps": {"score": 1, "evidence": "To execute this test, run the following from the base repo dir: python test/distributed/_composable/fsdp/test_fully_shard_memory.py TestFullyShardMemory.test_fully_shard_training_memory"}, "software_version": {"score": 2, "evidence": "PyTorch version: 2.8.0a0+git624be3a, CUDA: None, OS: SUSE Linux, Python: 3.10.16"}, "platform": {"score": 2, "evidence": "CPU: Intel Xeon Max 9470C, 208 cores, 4 NUMA nodes, 210 MiB L3 cache, Architecture: x86_64"}}, "reporter": "ratnampa", "assignee": "ratnampa", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 1, "evidence": "The test failure indicates a potential issue with memory management in distributed training, affecting the FSDP functionality."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "Labels include bug.module: distributed."}, "predicted_module": {"module": "distributed", "evidence": "Test case is related to fully sharded memory in distributed training, which falls under distributed module."}, "report_date": {"update": "2025-05-16", "evidence": "Last updated at 2025-05-16 06:12:57+00:00"}, "last_update": {"update": "2025-05-16", "evidence": "Last updated at 2025-05-16 06:12:57+00:00"}}
{"issue_number": 1604, "issue_description": {"score": 2, "evidence": "The issue describes a bug where a test involving distributed operations hangs due to batch_isend_irecv() async P2P ops. It provides context about the PyTorch and Torch-xpu-ops versions and the internal JIRA link."}, "error_message": {"score": 0, "evidence": "No specific error message is provided in the issue description."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps are provided to reproduce the issue."}, "software_version": {"score": 2, "evidence": "PyTorch version is specified as 2.8.0a0+git624be3a, along with OS, GCC, Python, and CPU information."}, "platform": {"score": 2, "evidence": "Detailed platform info includes OS, version, CPU details, and other system configurations."}}, "reporter": "ratnampa", "assignee": "ratnampa", "resolution": {"score": 0, "evidence": "No information is provided about potential resolutions."}, "root_cause": {"score": 0, "evidence": "No specific root cause is identified or discussed in the issue."}, "impact": {"score": 0, "evidence": "No explicit information about the impact of the issue is provided."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under 'bug.module: distributed'."}, "predicted_module": {"module": "distributed", "evidence": "The issue involves test cases related to distributed operations, specifically mentioning 'test.distributed.tensor.test_experimental_ops.DistOtherOpsTest.test_bernoulli' which falls under distributed functionality."}, "report_date": {"update": "2025-05-16", "evidence": "Last updated at 2025-05-16 06:11:56+00:00."}, "last_update": {"update": "2025-05-16", "evidence": "Last updated at 2025-05-16 06:11:56+00:00."}}
{"issue_number": 1597, "issue_description": {"score": 2, "evidence": "The issue is to implement aten::_linalg_solve_ex.result on XPU. The reporter mentions that this op will fallback to CPU and is used by comfyUI text to video generation. The issue includes a link to another PyTorch issue (154182)."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "jianyizh", "assignee": "huiyan2021", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 2, "evidence": "The issue mentions that the op is used by comfyUI text to video generation, which implies it's critical for that use case and could impact performance if not implemented on XPU."}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "The label is module: op impl."}, "predicted_module": {"module": "OP impl", "evidence": "The issue is about implementing a specific PyTorch operation on XPU, which falls under operation implementation."}, "report_date": {"update": "2025-05-23", "evidence": "Last updated on 2025-05-23"}, "last_update": {"update": "2025-05-23", "evidence": "Last updated on 2025-05-23"}}
{"issue_number": 1594, "issue_description": {"score": 2, "evidence": "The issue describes the need to continuously check for building warnings, providing specific examples of warnings encountered."}, "error_message": {"score": 2, "evidence": "Multiple warnings are listed, such as deprecation notices and unused variables."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps provided to reproduce the issue."}, "software_version": {"score": 0, "evidence": "No information on PyTorch version or platform details."}, "platform": {"score": 0, "evidence": "No platform information provided."}}, "reporter": "xytintel", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No resolution provided."}, "root_cause": {"score": 0, "evidence": "No root cause analysis provided."}, "impact": {"score": 1, "evidence": "The issue relates to build warnings that could affect code quality and future deprecations."}, "state": "open", "labeled_module": {"module": "build", "evidence": "Warnings are related to the build process and code deprecations."}, "predicted_module": {"module": "build", "evidence": "The issue focuses on build warnings and deprecated code usage."}, "report_date": {"update": "2025-05-06", "evidence": "Last updated on 2025-05-06."}, "last_update": {"update": "2025-05-06", "evidence": "Last update date is provided."}}
{"issue_number": 1592, "issue_description": {"score": 2, "evidence": "The issue describes a bug where an AssertionError occurs due to tensor-like mismatch in distributed operations, specifically in the test case test_layer_norm_bwd_req_grad. It mentions it's a regression issue that passes with certain environment variables set. The error message details the tensor mismatch percentages and specific indices where the difference occurs, providing clear information about the problem and its impact on the test case. The provided versions and environment details are comprehensive, aiding in reproducing the issue. However, specific reproduce steps using Python or shell commands are missing, which slightly reduces clarity but the overall information is concise and clear."}, "error_message": {"score": 2, "evidence": "The error message 'AssertionError:Tensor-likes are not close!' is clear and provides specific details about the tensor mismatch, including the percentage of mismatched elements, the greatest absolute and relative differences, and their respective indices. This information is precise and directly indicates the nature of the issue."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue does not explicitly provide Python, pytest, or shell commands to reproduce the issue, which is a minor shortcoming. However, it does mention that setting specific environment variables (CCL_SEND and CCL_RECV to 'direct') makes the test pass, which implies that these variables might influence the behavior of the distributed operations, providing indirect guidance on how to manipulate the environment to observe the issue or its resolution."}, "software_version": {"score": 2, "evidence": "The PyTorch version is specified as 2.8.0a0+git41475ac, and the platform details include the OS (Ubuntu 22.04.5 LTS), Python version (3.10.17), and hardware information (Intel Xeon Platinum 8468). This provides a clear environment context, making it easier to replicate the issue if the exact setup is available."}, "platform": {"score": 2, "evidence": "Comprehensive platform details are provided, including OS, Python version, hardware specifications, and CPU information, which are essential for reproducing the issue in a similar environment."}}, "reporter": "PenghuiCheng", "assignee": "zxd1997066", "resolution": {"score": 0, "evidence": "No information is provided regarding the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 2, "evidence": "The issue impacts the distributed operations in PyTorch, specifically affecting the backward pass of layer normalization with gradients. This can lead to incorrect model training in distributed training setups, which is critical for many machine learning applications relying on distributed computing."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The label explicitly states 'bug.module: distributed', indicating that the issue pertains to the distributed module of PyTorch."}, "predicted_module": {"module": "distributed", "evidence": "The test case mentioned, test_layer_norm_bwd_req_grad, falls under distributed operations, specifically involving layer normalization in distributed training. This aligns with the labeled module, reinforcing the prediction."}, "report_date": {"update": "2025-04-25", "evidence": "The issue was last updated on 2025-04-25, indicating recent activity."}, "last_update": {"update": "2025-04-25", "evidence": "The issue was last updated on 2025-04-25, showing that there has been recent interaction or activity related to this issue."}}
{"issue_number": 1587, "issue_description": {"score": 2, "evidence": "The issue description mentions the need to keep up with the latest CUDA optimizations and provides specific PR links related to gather, loops kernels, and H2D operations."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "xytintel", "assignee": "xytintel", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue mentions kernel optimizations and provides PR links related to PyTorch's optimizations, which is related to the torch-xpu-ops repository's optimizations."}, "predicted_module": {"module": "torchbench", "evidence": "The issue is about tracking CUDA optimizations, which impacts performance benchmarks."}, "report_date": {"update": "2025-05-09", "evidence": "Last updated at 2025-05-09 02:45:45+00:00"}, "last_update": {"update": "2025-05-09", "evidence": "Last updated at 2025-05-09 02:45:45+00:00"}}
{"issue_number": 1581, "issue_description": {"score": 2, "evidence": "The issue describes a fatal Python error: Segmentation fault in distributed tests, including specific test cases and error logs."}, "error_message": {"score": 2, "evidence": "Fatal Python error: Segmentation fault, with specific test failures and warnings related to collective operations and unwaited collective calls."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes test cases but lacks explicit Python, pytest, or shell commands to reproduce the issue."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version or platform information is provided."}, "platform": {"score": 0, "evidence": "No platform information is given."}}, "reporter": "PenghuiCheng", "assignee": "githubsgi", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue description."}, "impact": {"score": 1, "evidence": "The issue causes test failures in distributed collective operations, potentially affecting distributed training and data parallelism in PyTorch."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under the 'bug.module: distributed' category."}, "predicted_module": {"module": "distributed", "evidence": "The issue involves test cases related to distributed collective operations, indicating the distributed module."}, "report_date": {"update": "2025-05-16", "evidence": "The issue was last updated on 2025-05-16."}, "last_update": {"update": "2025-05-16", "evidence": "The issue was last updated on 2025-05-16."}}
{"issue_number": 1574, "issue_description": {"score": 2, "evidence": "The operator 'aten::_grouped_mm' is not currently implemented for the XPU device."}, "error_message": {"score": 2, "evidence": "NotImplementedError: The operator 'aten::_grouped_mm' is not currently implemented for the XPU device."}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 1, "evidence": "pytorch master and xpu-ops master."}, "platform": {"score": 0, "evidence": ""}}, "reporter": "githubsgi", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue is related to the implementation of a specific operator, which falls under the core functionality of PyTorch."}, "predicted_module": {"module": "Core", "evidence": "The issue describes a missing implementation of an operator, which is part of the core functionality."}, "report_date": {"update": "2025-04-23", "evidence": "2025-04-23 08:26:05+00:00"}, "last_update": {"update": "2025-04-23", "evidence": "2025-04-23 08:26:05+00:00"}}
{"issue_number": 1571, "issue_description": {"score": 2, "evidence": "The issue title and body provide clear and concise information about the bug encountered during the test, including the error message and the context in which it occurred."}, "error_message": {"score": 2, "evidence": "The error message 'ValueError: Cannot use ReduceOp.PREMUL_SUM with XCCL' is specific and directly points to the problematic operation and the communication backend involved."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue does not provide explicit reproduce steps, but the test case mentioned (test_set_reduce_scatter_divide_factor) and the use of ReduceOp.PREMUL_SUM with XCCL suggest potential steps. However, detailed steps are missing."}, "software_version": {"score": 2, "evidence": "The PyTorch version and environment details are provided, including the versions of oneCCL, OpenZEN, and other relevant libraries."}, "platform": {"score": 2, "evidence": "Comprehensive platform information is given, including the CPU model, architecture, and OS details."}}, "reporter": "daisyden", "assignee": "zhangxiaoli73", "resolution": {"score": 1, "evidence": "It is mentioned that the issue is pending on oneCCL support, but no specific resolution plan or timeline is provided."}, "root_cause": {"score": 0, "evidence": "No analysis or proposed root cause is provided in the issue description."}, "impact": {"score": 1, "evidence": "The issue impacts the distributed training functionality, specifically when using ReduceOp.PREMUL_SUM with XCCL, but the extent of the impact is not fully detailed."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under the 'module: distributed' category."}, "predicted_module": {"module": "distributed", "evidence": "The test case involves distributed communication (test_fully_shard_comm) and the error relates to XCCL, indicating the issue is within the distributed module."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, and created on 2025-04-11."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1555, "issue_description": {"score": 1, "evidence": "The issue description mentions a bug in the distributed module but lacks specific details about the problem's impact or reproduction steps."}, "error_message": {"score": 1, "evidence": "RuntimeError: aten.add.Tensor: got mixed torch.Tensor and DTensor, need to convert all torch.Tensor to DTensor before calling distributed operators!"}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps provided to reproduce the issue."}, "software_version": {"score": 0, "evidence": "No information about PyTorch version or platform."}, "platform": {"score": 0, "evidence": "No platform information provided."}}, "reporter": "PenghuiCheng", "assignee": "githubsgi", "resolution": {"score": 0, "evidence": "No resolution information available."}, "root_cause": {"score": 2, "evidence": "The root cause is the mask tensor, which is not converted to DTensor."}, "impact": {"score": 1, "evidence": "The issue affects distributed tensor operations, which is critical for large-scale distributed training."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under the 'module: distributed' category."}, "predicted_module": {"module": "distributed", "evidence": "The test cases involve distributed tensor operations, indicating the issue is related to the distributed module."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1554, "issue_description": {"score": 2, "evidence": "The issue describes encountering a PermissionError during multi-threaded compilation when multiple threads try to open the same file simultaneously using 'with open(filename, \"w\")'. The error causes the compilation to fail. The reporter suggests using a mutex lock to prevent concurrent access."}, "error_message": {"score": 2, "evidence": "PermissionError: [Errno 13] Permission denied"}, "reproduce_steps": {"steps": {"score": 1, "evidence": "Steps to reproduce include building PyTorch with a specific commit. Missing specific commands or environment details for reproduction."}, "software_version": {"score": 1, "evidence": "Python version 3.10.16, CMake version 3.31.6, but no specific PyTorch version is mentioned. Platform is Windows 10."}, "platform": {"score": 2, "evidence": "Platform is Windows 10."}}, "reporter": "chunhuanMeng", "assignee": "chunhuanMeng", "resolution": {"score": 0, "evidence": "No resolution provided in the issue."}, "root_cause": {"score": 1, "evidence": "Possible root cause is multiple threads accessing the same file without proper synchronization, as suggested by the reporter."}, "impact": {"score": 1, "evidence": "The error causes the compilation to fail, which could hinder the build process and prevent successful installation or usage of PyTorch with XPU support."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under 'os: Windows' and relates to file operations during compilation, which might be a dependency or build system issue."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue involves multi-threaded file operations during compilation, suggesting a dependency or build system problem."}, "report_date": {"update": "2025-05-07", "evidence": "Last updated on 2025-05-07."}, "last_update": {"update": "2025-05-07", "evidence": "Last updated on 2025-05-07."}}
{"issue_number": 1551, "issue_description": {"score": 2, "evidence": "The issue describes a NotImplementedError for the operator 'symm_mem::fused_scaled_matmul_reduce_scatter' on XPU device, with specific test cases failing. The description is concise and clear."}, "error_message": {"score": 2, "evidence": "The error message clearly states the operator not implemented for XPU device."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes test cases but lacks explicit reproduce steps using specific commands or scripts."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version or platform info provided."}, "platform": {"score": 0, "evidence": "No platform information is given."}}, "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": {"score": 1, "evidence": "The issue is open, but there's no information on the resolution process or timeline."}, "root_cause": {"score": 1, "evidence": "The error is due to the missing implementation of a specific operator for XPU, but the root cause analysis is not detailed."}, "impact": {"score": 1, "evidence": "The issue affects multiple test cases in distributed tensor parallelism, indicating potential impact on distributed training functionality."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under 'module: distributed'."}, "predicted_module": {"module": "distributed", "evidence": "The test cases belong to distributed tensor parallelism."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on this date."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on this date."}}
{"issue_number": 1550, "issue_description": {"score": 2, "evidence": "The issue describes a NotImplementedError for the '_scaled_mm.out' operator on XPU device, with test failures in distributed tensor parallelism tests."}, "error_message": {"score": 2, "evidence": "NotImplementedError: The operator 'aten::_scaled_mm.out' is not currently implemented for the XPU device."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue body includes test failures but lacks explicit steps to reproduce the issue, though it mentions pytest and specific test files."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "PenghuiCheng", "assignee": "xytintel", "resolution": {"score": 1, "evidence": "The issue is open, but no specific resolution plan is detailed."}, "root_cause": {"score": 1, "evidence": "The root cause is identified as the missing implementation of the operator, but no further details are provided."}, "impact": {"score": 2, "evidence": "The issue affects distributed tensor parallelism tests, potentially impacting the functionality of distributed training in PyTorch."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under 'bug.module: distributed'."}, "predicted_module": {"module": "distributed", "evidence": "The test failures are in the distributed tensor parallelism tests."}, "report_date": {"update": "2025-05-07", "evidence": "Issue last updated on 2025-05-07."}, "last_update": {"update": "2025-05-07", "evidence": "Issue last updated on 2025-05-07."}}
{"issue_number": 1549, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the error message and the test cases affected."}, "error_message": {"score": 2, "evidence": "The error message 'AssertionError: 'fused_all_gather_scaled_matmul' not found in 'graph():\\"}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes the test cases that can be used to reproduce the issue, such as 'test_fuse_all_gather_scaled_matmul_A_dims_2_gather_dim_0_return_A_False'."}, "software_version": {"score": 2, "evidence": "The issue does not explicitly mention the PyTorch version, but the test cases suggest it's related to distributed tensor operations in PyTorch."}, "platform": {"score": 2, "evidence": "The platform is Linux with Python 3.10.16, as indicated by the test logs."}}, "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": {"score": 1, "evidence": "The issue is open, so resolution information is not available, but the user is encouraged to provide more details."}, "root_cause": {"score": 1, "evidence": "The root cause is likely related to the missing implementation of the 'fused_matmul_reduce_scatter' operator for XPU, but more details are needed."}, "impact": {"score": 2, "evidence": "The issue affects multiple test cases related to distributed tensor operations, which could impact the functionality of distributed training in PyTorch."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under the 'module: distributed' category."}, "predicted_module": {"module": "distributed", "evidence": "The test cases and error message indicate the issue is related to distributed tensor operations."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1548, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the error message and the test cases that failed."}, "error_message": {"score": 2, "evidence": "The error message is 'AssertionError: 'fused_all_gather_matmul' not found in '# AOT ID: [\\'2_inference\\']\". This is clear and specific."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions the test cases but does not provide detailed steps to reproduce the issue, such as specific commands or environment setup."}, "software_version": {"score": 1, "evidence": "The issue does not explicitly state the PyTorch version used, though it can be inferred from the test cases."}, "platform": {"score": 1, "evidence": "The platform information is not explicitly provided but might be inferred from the test logs."}}, "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": {"score": 1, "evidence": "No resolution information is provided in the issue description."}, "root_cause": {"score": 1, "evidence": "The root cause is likely related to the missing implementation of the 'fused_matmul_reduce_scatter' operator on XPU devices. However, the issue description doesn't explicitly state this; it's inferred from the error messages and the context of the test case."}, "impact": {"score": 1, "evidence": "The issue affects multiple test cases related to distributed tensor operations, indicating a potential problem in the distributed training functionality."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under the 'module: distributed' category."}, "predicted_module": {"module": "distributed", "evidence": "The test cases are related to distributed tensor parallel operations, which falls under the distributed module."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, indicating recent activity."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, indicating recent activity."}}
{"issue_number": 1547, "issue_description": {"score": 2, "evidence": "The issue reports a NotImplementedError for the operator 'symm_mem::fused_matmul_reduce_scatter' on XPU device, affecting several test cases in distributed tensor parallelism."}, "error_message": {"score": 2, "evidence": "NotImplementedError: The operator 'symm_mem::fused_matmul_reduce_scatter' is not currently implemented for the XPU device."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes test cases but lacks explicit reproduce steps using specific commands or scripts."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects multiple test cases related to distributed tensor parallelism, indicating potential impact on distributed training and model parallelism functionalities."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under 'module: distributed'."}, "predicted_module": {"module": "distributed", "evidence": "The failing test cases are within the distributed tensor parallelism tests."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1543, "issue_description": {"score": 2, "evidence": "The issue describes a memory allocation discrepancy between XPU and CUDA during fine-tuning, with XPU reserving 8GB more VRAM. The problem is clearly explained, including steps to reproduce and logs from both torchtune and PyTorch sides."}, "error_message": {"score": 2, "evidence": "The user provided logs showing 8GB extra allocation on XPU which is not expected, indicating an issue with memory management."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Detailed steps provided, including cloning a specific branch, running a command with arguments, and code snippets for hook registration and memory stats printing."}, "software_version": {"score": 1, "evidence": "No specific version of PyTorch or other libraries is mentioned, though the user modified the XPU allocator, suggesting a specific version might be in use."}, "platform": {"score": 2, "evidence": "The issue pertains to XPU and CUDA, so platform info is clear."}}, "reporter": "airMeng", "assignee": "guangyey", "resolution": {"score": 2, "evidence": "The assignee provided a clear resolution plan, mentioning the refactoring of the XPU allocator to align with CUDA's core logic, with an expected completion before Q3."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as subtle differences between the XPU and CUDA allocators."}, "impact": {"score": 2, "evidence": "The issue affects memory usage during training, potentially causing resource exhaustion or longer training times on XPU compared to CUDA."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "Labels include 'module: core' indicating the core functionality is affected."}, "predicted_module": {"module": "Core", "evidence": "The issue relates to memory management, which is a core functionality of any GPU/TPU runtime."}, "report_date": {"update": "2025-05-14", "evidence": "Last updated at 2025-05-14."}, "last_update": {"update": "2025-05-14", "evidence": "The issue was last updated on 2025-05-14."}}
{"issue_number": 1536, "issue_description": {"score": 2, "evidence": "The issue describes a bug where the test_distributed_checkpoint.py test fails randomly with an atl_status: FAILURE error. It provides details about the environment, including the branch, build, and test commands, along with logs showing warnings and errors related to distributed training and CCL issues. The description is concise and clear, providing sufficient context for understanding the problem."}, "error_message": {"score": 2, "evidence": "The error message includes specific warnings and errors, such as 'CCL_WARN' and 'CCL_ERROR', along with stack traces pointing to issues in the CCL library and FSDP initialization. These messages are clear and provide actionable information for debugging."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides the exact pytest command used to reproduce the failure, including the test file and the specific test method. This is clear and sufficient for others to replicate the issue."}, "software_version": {"score": 2, "evidence": "The PyTorch version, CUDA, and other relevant software versions are clearly specified. This helps in identifying if the issue is version-specific."}, "platform": {"score": 2, "evidence": "The OS, CPU architecture, and other platform details are provided, which are essential for understanding the environment where the issue occurs."}}, "reporter": "daisyden", "assignee": "ratnampa", "resolution": {"score": 1, "evidence": "While the issue is still open and randomly failing, there are mentions of testing with newer versions (2025.1.1) and plans for more retries to check for random failures. No explicit resolution or root cause is provided, but the issue is being actively worked on."}, "root_cause": {"score": 1, "evidence": "The gdb backtrace indicates a problem in the CCL allgather function, which is part of the oneCCL library. The failure occurs during a collective communication operation in PyTorch's distributed training code, suggesting a possible issue in the oneCCL implementation or its interaction with PyTorch's distributed training framework."}, "impact": {"score": 2, "evidence": "The issue affects the distributed checkpointing functionality, which is critical for training large models. Random failures in this area can lead to unreliable training runs and wasted computational resources."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under the 'bug.module: distributed' category, clearly indicating the affected module."}, "predicted_module": {"module": "distributed", "evidence": "The test case is related to distributed checkpointing, which falls under the distributed module. The error messages also point to issues in the distributed communication layer, reinforcing this prediction."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, and it was created on 2025-04-02."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1525, "issue_description": {"score": 2, "evidence": "The issue describes a ValueError when initializing the default process group twice in distributed testing."}, "error_message": {"score": 2, "evidence": "ValueError: trying to initialize the default process group twice!"}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides the test file and the specific lines where the error occurs, including the setUp function in test_c10d_functional_native.py."}, "software_version": {"score": 2, "evidence": "PyTorch version: 2.8.0a0+git44d55b9 and other relevant library versions are provided."}, "platform": {"score": 2, "evidence": "Ubuntu 22.04.4 LTS (x86_64), CUDA and ROCM versions are specified."}}, "reporter": "PenghuiCheng", "assignee": "Chao1Han", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause information is provided."}, "impact": {"score": 1, "evidence": "The issue causes test failures in distributed testing, but the broader impact on PyTorch's functionality is not detailed."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The label is explicitly 'bug.module: distributed'."}, "predicted_module": {"module": "distributed", "evidence": "The issue involves initializing the default process group, which is part of the distributed module in PyTorch."}, "report_date": {"update": "2025-05-13", "evidence": "Last updated on 2025-05-13"}, "last_update": {"update": "2025-05-13", "evidence": "Last updated on 2025-05-13"}}
{"issue_number": 1521, "issue_description": {"score": 2, "evidence": "The issue description provides a clear explanation of the bug encountered when enabling PyTorch Flex Attention on XPU, including the error message and code snippet."}, "error_message": {"score": 2, "evidence": "The error message clearly states the issue with 'Torch not compiled with CUDA enabled' and the target function 'flex_attention'."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes the code snippet but lacks detailed reproduce steps, such as specific commands or configurations."}, "software_version": {"score": 2, "evidence": "The PyTorch version is specified as 'torch head.'"}, "platform": {"score": 2, "evidence": "The labels include 'os: Ubuntu' and 'hw: PVC'."}}, "reporter": "githubsgi", "assignee": "liangan1", "resolution": {"score": 0, "evidence": "No information provided about the resolution."}, "root_cause": {"score": 0, "evidence": "No information provided about the root cause."}, "impact": {"score": 1, "evidence": "The issue affects the functionality of PyTorch Flex Attention on XPU, but the severity and specific impact are not detailed."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue relates to PyTorch Flex Attention, which is part of the transformers module."}, "predicted_module": {"module": "transformers", "evidence": "Based on the issue description, the test case classifications and the use of flex_attention, the issue is likely related to the transformers module."}, "report_date": {"update": "2025-04-22", "evidence": "The issue was last updated on 2025-04-22."}, "last_update": {"update": "2025-04-22", "evidence": "The issue was last updated on 2025-04-22."}}
{"issue_number": 1519, "issue_description": {"score": 2, "evidence": "The issue describes two failing test cases: test_max_pool_nan_inf_xpu_float32 and test_dtypes__refs_nn_functional_pdist_xpu. It provides details about the environment and steps to reproduce."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Command: 'export ZE_AFFINITY_MASK=2; pytest -sv test_pooling_xpu.py -k test_max_pool_nan_inf'"}, "software_version": {"score": 2, "evidence": "Pytorch/torchvision/torchaudio versions provided"}, "platform": {"score": 2, "evidence": "Hardware: Max 1550; Driver details; Basekit version; Compiler details."}}, "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 2, "evidence": "The tests are failing in both IPEX and PyTorch, affecting the functionality of the pooling operations and pdist function."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue involves test cases in test_pooling_xpu.py and test_ops_xpu.py, which are part of the unit tests."}, "predicted_module": {"module": "UT", "evidence": "The failing tests are part of the unit test suite for Torch-XPU-ops."}, "report_date": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 13:11:51+00:00"}, "last_update": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 13:11:51+00:00"}}
{"issue_number": 1513, "issue_description": {"score": 2, "evidence": "The issue describes that adding Inductor UT test into nightly test causes an issue where the Github action stage fails to exit normally after completing the test. The test results and logs are provided, indicating the problem occurs during the UT test execution."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "RUIJIEZHONG66166", "assignee": "RUIJIEZHONG66166", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 2, "evidence": "The issue affects the nightly test suite by preventing Inductor UT tests from completing successfully, which could delay CI/CD pipelines and hinder timely identification of issues in the Inductor module."}, "state": "open", "labeled_module": {"module": "inductor", "evidence": "The labels include 'inductor.module', which directly indicates the module related to the issue."}, "predicted_module": {"module": "inductor", "evidence": "The issue involves Inductor UT tests, so the predicted module is Inductor."}, "report_date": {"update": "2025-04-17", "evidence": "Last updated at 2025-04-17 01:43:42+00:00"}, "last_update": {"update": "2025-04-17", "evidence": "Last updated at 2025-04-17 01:43:42+00:00"}}
{"issue_number": 1512, "issue_description": {"score": 2, "evidence": "The issue is titled '[Windows] First run takes long time on ARC/BMG' and the body provides a detailed description of the problem, including steps to reproduce and timing data for different PyTorch versions and environments."}, "error_message": {"score": 0, "evidence": "No specific error message is provided in the issue description."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides clear steps to reproduce, including creating a new conda environment, installing specific PyTorch wheels, and running a sample script. The steps are concise and repeatable."}, "software_version": {"score": 2, "evidence": "The issue specifies the PyTorch versions (v2.6, v2.7.0_0312, v2.7_0326) and the platform (Windows)."}, "platform": {"score": 2, "evidence": "The issue explicitly mentions the platform as Windows and the hardware (ARC/BMG)."}}, "reporter": "ZhaoqiongZ", "assignee": "LuFinch", "resolution": {"score": 0, "evidence": "No resolution information is provided in the issue."}, "root_cause": {"score": 0, "evidence": "No root cause is identified or discussed in the issue."}, "impact": {"score": 2, "evidence": "The issue describes the impact as the first run taking significantly longer on Windows compared to Linux, which could affect user experience and productivity."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under 'client.os: Windows', indicating a dependency or platform-specific issue."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue pertains to the Windows platform, suggesting a dependency or OS-related module."}, "report_date": {"update": "2025-05-23", "evidence": "The issue was last updated on 2025-05-23."}, "last_update": {"update": "2025-05-23", "evidence": "The issue was last updated on 2025-05-23."}}
{"issue_number": 1510, "issue_description": {"score": 2, "evidence": "The issue describes that some test cases in test/xpu are hanging, specifically test_linspace_xpu_complex128. The description is clear and concise."}, "error_message": {"score": 2, "evidence": "The error message indicates that once a test fails, subsequent tests also fail, and rerunning the failed test individually passes. This provides a clear indication of the issue."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "No specific steps are provided, but it can be inferred that running the tests in a specific environment would reproduce the issue."}, "software_version": {"score": 2, "evidence": "The PyTorch version is specified as 2.7.0+xpu, and the OS is Ubuntu 24.10. This is clear and sufficient."}, "platform": {"score": 2, "evidence": "The platform is BMG on Ubuntu 24.10 with specific driver versions. This is clear and sufficient."}}, "reporter": "mengfei25", "assignee": "Stonepia", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause information is provided."}, "impact": {"score": 1, "evidence": "The issue affects multiple test cases, potentially impacting the reliability of the XPU operations in PyTorch."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled under the 'module: ut' label, indicating it is related to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The issue involves test cases hanging, which points to potential issues in the testing framework or environment, aligning with the UT module."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}, "last_update": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}}
{"issue_number": 1509, "issue_description": {"score": 2, "evidence": "The issue describes a bug where x.sum().backward() causes a RuntimeError: Data corruption detected. The test case is test_multi_threaded_pg.py, line 336."}, "error_message": {"score": 2, "evidence": "RuntimeError: Data corruption detected"}, "reproduce_steps": {"steps": {"score": 2, "evidence": "pytest -v test/distributed/test_multi_threaded_pg.py"}, "software_version": {"score": 2, "evidence": "PyTorch version: 2.8.0a0+gitfd4037d"}, "platform": {"score": 2, "evidence": "Ubuntu 22.04.5 LTS (x86_64)"}}, "reporter": "PenghuiCheng", "assignee": "syedshahbaaz", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 1, "evidence": "The root cause is mentioned to be related to oneCCL not fully supporting multi-threading, but the details are not clear. The comments suggest that the issue occurs when running multiple ranks on a single device, which might indicate a concurrency issue with oneCCL. However, the explanation is not thorough."}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "bug.module: distributed"}, "predicted_module": {"module": "distributed", "evidence": "The test case is in test_multi_threaded_pg.py, which suggests it's related to distributed computing."}, "report_date": {"update": "2025-05-12", "evidence": "last updated at 2025-05-12 20:50:27+00:00"}, "last_update": {"update": "2025-05-12", "evidence": "last updated at 2025-05-12 20:50:27+00:00"}}
{"issue_number": 1508, "issue_description": {"score": 2, "evidence": "The issue is titled '[distributed] RuntimeError: oneCCL: coll_param.cpp:455 validate: EXCEPTION: average operation is not supported for the scheduler path' and the body provides a detailed error message and test cases."}, "error_message": {"score": 2, "evidence": "RuntimeError: oneCCL: coll_param.cpp:455 validate: EXCEPTION: average operation is not supported for the scheduler path"}, "reproduce_steps": {"steps": {"score": 2, "evidence": "pytest -vs test/distributed/test_c10d_functional_native.py -k test_reduce_scatter_tensor_coalesced and other test cases"}, "software_version": {"score": 2, "evidence": "Python 3.10.16, pytest-7.3.2, pluggy-1.5.0"}, "platform": {"score": 2, "evidence": "linux"}}, "reporter": "PenghuiCheng", "assignee": "ratnampa", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 1, "evidence": "The issue affects distributed training and data parallelism, potentially breaking critical functionalities in PyTorch."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "bug.module: distributed"}, "predicted_module": {"module": "distributed", "evidence": "test cases related to distributed functionality"}, "report_date": {"update": "2025-05-16", "evidence": "created at 2025-03-25, last updated at 2025-05-16"}, "last_update": {"update": "2025-05-16", "evidence": "last updated at 2025-05-16 03:50:05+00:00"}}
{"issue_number": 1506, "issue_description": {"score": 2, "evidence": "The issue describes that several E2E models are failing accuracy tests on BMG and LNL, with specific model names and commands provided."}, "error_message": {"score": 2, "evidence": "The issue includes detailed error messages from the test runs, showing which models failed."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides Python commands used to run the benchmarks, which can be used to reproduce the issue."}, "software_version": {"score": 2, "evidence": "The issue specifies the PyTorch and torch-xpu-ops versions, as well as the driver version."}, "platform": {"score": 2, "evidence": "The issue mentions the client OS is Windows, and specific hardware details like BMG and LNL."}}, "reporter": "libohao1201", "assignee": "Stonepia", "resolution": {"score": 2, "evidence": "The comment suggests checking with a newer version of transformers, indicating a possible resolution path."}, "root_cause": {"score": 2, "evidence": "The comment references a previous issue (1216), suggesting the root cause may be similar."}, "impact": {"score": 2, "evidence": "The issue affects the accuracy of several models in production and training, impacting performance and reliability."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue involves benchmarking scripts in torchbench, specifically in the timm_models and huggingface benchmarks."}, "predicted_module": {"module": "torchbench", "evidence": "The issue is related to model benchmarking and accuracy tests, which fall under the torchbench category."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1505, "issue_description": {"score": 2, "evidence": "The issue describes that 14 Timm models are failing accuracy tests on ARC-WSL using bfloat16 training with torch-xpu-ops."}, "error_message": {"score": 0, "evidence": "No specific error message is provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps are provided."}, "software_version": {"score": 2, "evidence": "PyTorch version and torch-xpu-ops commit are specified."}, "platform": {"score": 2, "evidence": "Machine details including driver version and WSL are provided."}}, "reporter": "libohao1201", "assignee": "", "resolution": {"score": 0, "evidence": "No resolution provided."}, "root_cause": {"score": 0, "evidence": "No root cause analysis provided."}, "impact": {"score": 0, "evidence": "No impact analysis provided."}, "state": "open", "labeled_module": {"module": "build", "evidence": "Label is 'bug.client.os: Windows'."}, "predicted_module": {"module": "quant", "evidence": "The test uses bfloat16, which relates to quantization."}, "report_date": {"update": "2025-03-26", "evidence": "Issue last updated at 2025-03-26 01:19:59+00:00."}, "last_update": {"update": "2025-03-26", "evidence": "Issue last updated at 2025-03-26 01:19:59+00:00."}}
{"issue_number": 1504, "issue_description": {"score": 2, "evidence": "The issue describes accuracy gaps in FSDP tests on XELink, with specific test cases failing due to discrepancies in losses, total norms, and tensor values. The description is concise and clear, providing specific test failures and error messages."}, "error_message": {"score": 2, "evidence": "Multiple test cases show assertion errors with specific numerical discrepancies, indicating potential precision issues in distributed training."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides detailed Python commands to reproduce the tests, such as `python test/distributed/fsdp/test_fsdp_fine_tune.py -k test_hooks_multi_traversal_xpu` and others, making it clear how to replicate the problem."}, "software_version": {"score": 1, "evidence": "The PyTorch version is not explicitly mentioned, but the error messages and test commands suggest a recent version based on the file paths and Python environment."}, "platform": {"score": 2, "evidence": "Tests were run on PVC 1550 and 1100 machines with specific ZE_AFFINITY_MASK settings, providing clear platform details."}}, "reporter": "daisyden", "assignee": "pkourdis", "resolution": {"score": 2, "evidence": "It is mentioned that setting `CCL_OP_SYNC=1` resolves the accuracy issues, but the root cause is linked to an internal Jira ticket without further details."}, "root_cause": {"score": 2, "evidence": "The issue is linked to an internal Jira ticket `PYTORCHDGQ-6190`, but no further details are provided."}, "impact": {"score": 1, "evidence": "The issue affects FSDP functionality, which is critical for distributed training, potentially impacting model accuracy and training reliability."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The label explicitly states 'bug.module: distributed', and the test failures are related to FSDP (Fair Scalable Data Parallel), which is part of the distributed training module."}, "predicted_module": {"module": "distributed", "evidence": "The test failures are within the FSDP tests, which are part of the distributed training functionality in PyTorch."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, indicating recent activity."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, showing the most recent activity."}}
{"issue_number": 1502, "issue_description": {"score": 2, "evidence": "The issue describes that WSL crashes when running torchbench, providing a clear problem statement."}, "error_message": {"score": 1, "evidence": "No specific error message is provided, but the issue mentions WSL crashing."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Detailed steps including changing iterations and running a script are provided."}, "software_version": {"score": 2, "evidence": "PyTorch version, torch-xpu-ops commit, driver version, and Conda info are specified."}, "platform": {"score": 2, "evidence": "Machine details including OS and hardware are provided."}}, "reporter": "libohao1201", "assignee": "", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue."}, "impact": {"score": 1, "evidence": "The issue impacts WSL users running torchbench."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue involves core functionality during testing."}, "predicted_module": {"module": "Core", "evidence": "The test case relates to core functionality of PyTorch and torch-xpu-ops."}, "report_date": {"update": "2025-03-26", "evidence": "Last updated date is provided."}, "last_update": {"update": "2025-03-26", "evidence": "Last updated date is provided."}}
{"issue_number": 1500, "issue_description": {"score": 2, "evidence": "The issue describes a bug where the operator 'aten::_slow_conv2d_forward' is not implemented for the XPU device. The reporter provides a code snippet and environment details."}, "error_message": {"score": 2, "evidence": "The error message is clear and specific: 'The operator 'aten::_slow_conv2d_forward' is not currently implemented for the XPU device.'"}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The reporter provides a Python code snippet that reproduces the issue, including importing necessary libraries and running a model with an image input."}, "software_version": {"score": 2, "evidence": "The versions of PyTorch, transformers, and other libraries are clearly listed, along with platform and hardware information."}, "platform": {"score": 2, "evidence": "Detailed platform information, including OS, CPU, and XPU device details, is provided."}}, "reporter": "kaixuanliu", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No information provided about the resolution status."}, "root_cause": {"score": 0, "evidence": "No information provided about the root cause."}, "impact": {"score": 2, "evidence": "The issue affects the functionality of the model when using the XPU device, potentially impacting performance and usability for users relying on XPU acceleration."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue relates to the implementation of a specific PyTorch operator, suggesting it falls under the torchbench module."}, "predicted_module": {"module": "torchbench", "evidence": "The issue involves a missing implementation of a PyTorch operator, which is typically handled under the torchbench category."}, "report_date": {"update": "2025-04-24", "evidence": "The issue was last updated on 2025-04-24."}, "last_update": {"update": "2025-04-24", "evidence": "The last update date is provided."}}
{"issue_number": 1498, "issue_description": {"score": 2, "evidence": "The issue describes that extended UTs failed with a specific RuntimeError related to the Native API returning an error code 29 (UR_RESULT_ERROR_INVALID_KERNEL_NAME)."}, "error_message": {"score": 2, "evidence": "RuntimeError: Native API failed. Native API returns: 29 (UR_RESULT_ERROR_INVALID_KERNEL_NAME)"}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides detailed reproduce steps using pytest commands for specific test cases."}, "software_version": {"score": 2, "evidence": "Includes PyTorch version, torch-xpu-ops commit, Conda environment, and driver version."}, "platform": {"score": 2, "evidence": "Mentions the machine is running on ARC - Win11 with WSL."}}, "reporter": "libohao1201", "assignee": "gaopengff", "resolution": {"score": 0, "evidence": "No information provided on the resolution status or steps taken to fix the issue."}, "root_cause": {"score": 0, "evidence": "No information provided on the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue affects the extended UTs, but the broader impact on the project or users isn't clearly specified."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled with 'ut' and the test cases are part of the extended UTs."}, "predicted_module": {"module": "UT", "evidence": "The issue involves test cases (test_backward_bernoulli_xpu_float32, etc.), which fall under the UT module."}, "report_date": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 07:45:22+00:00"}, "last_update": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 07:45:22+00:00"}}
{"issue_number": 1496, "issue_description": {"score": 2, "evidence": "The issue describes an application error when running E2E inductor on LNL, with a memory access error. The description is concise and clear."}, "error_message": {"score": 2, "evidence": "The error message details the specific memory address causing the issue, indicating a read error. It is clear and specific."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific reproduce steps are provided in the issue."}, "software_version": {"score": 2, "evidence": "Python version 3.10, PyTorch version, and driver details are provided. The information is clear and sufficient."}, "platform": {"score": 2, "evidence": "The issue is reported on Windows using LNL hardware. This is clearly specified."}}, "reporter": "libohao1201", "assignee": "", "resolution": {"score": 1, "evidence": "The comment suggests a possible root cause related to the driver's overcommit feature but doesn't provide a clear resolution. It mentions tracking another issue but doesn't outline a specific fix or plan."}, "root_cause": {"score": 1, "evidence": "The comment suggests a possible root cause related to the driver's overcommit feature but doesn't provide a clear or concise explanation."}, "impact": {"score": 1, "evidence": "The issue impacts the E2E inductor test on LNL, causing application crashes. While the impact is mentioned, it could be more detailed."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The label includes 'dependency bug', indicating a dependency-related issue."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue involves a memory access error, possibly due to incorrect driver or library dependencies. This aligns with the 'dependency bug' label."}, "report_date": {"update": "2025-03-21", "evidence": "The issue was last updated on March 21, 2025."}, "last_update": {"update": "2025-03-21", "evidence": "The issue was last updated on March 21, 2025."}}
{"issue_number": 1483, "issue_description": {"score": 2, "evidence": "The issue description is clear and concise, explaining the problem encountered with the Segmentation fault on Rolling but not on LTS."}, "error_message": {"score": 2, "evidence": "The error message is detailed, showing the specific Segmentation fault and related warnings during the test execution."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The reproduce steps are provided with the specific command line arguments and environment setup, including Python and PyTorch versions."}, "software_version": {"score": 2, "evidence": "PyTorch version and platform information are clearly stated, including the driver versions."}, "platform": {"score": 2, "evidence": "Platform details include OS (Ubuntu) and hardware (PVC), which are essential for reproducing the issue."}}, "reporter": "mengfei25", "assignee": "jianyizh", "resolution": {"score": 2, "evidence": "The issue was resolved in oneDNN v3.8, as mentioned in the comments."}, "root_cause": {"score": 1, "evidence": "The root cause is mentioned to be related to sdpa and _sfdp_init(), but the exact reason is not fully explained."}, "impact": {"score": 2, "evidence": "The issue impacts the functionality of the SAM model during inference, causing the test to fail with a Segmentation fault on the Rolling driver."}, "state": "open", "labeled_module": {"module": "inductor", "evidence": "The issue involves the 'backend=inductor' flag, indicating it relates to the Inductor module."}, "predicted_module": {"module": "inductor", "evidence": "The test case uses the 'backend=inductor', which is part of the Inductor module."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The last update was on 2025-05-21."}}
{"issue_number": 1475, "issue_description": {"score": 2, "evidence": "The issue describes random failures in test cases of test_fsdp_core.py, specifically mentioning two test cases and their error messages. The error involves a scalar comparison failure where the expected value is 0 but got -11.2025-03-13T07:14:07.6794758Z. The error occurs during the _join_processes function in a distributed test setup, indicating potential issues in process coordination or data synchronization."}, "error_message": {"score": 2, "evidence": "The error message clearly states 'AssertionError: Scalars are not equal! Expected 0 but got -11.2025-03-13T07:14:07.6794758Z. Absolute difference: 112025-03-13T07:14:07.6794932Z. Relative difference: inf'. This provides specific details about the discrepancy between expected and actual values, aiding in pinpointing the issue."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions the test cases but does not provide explicit steps to reproduce the issue. However, it references the test file and the functions involved, which could be used to infer the necessary commands."}, "software_version": {"score": 1, "evidence": "The PyTorch version is provided as 'Torch c208f217917929a9f780a81a8c7f788b4c03ee05', which is specific but not in a standard versioning format, making it somewhat unclear."}, "platform": {"score": 1, "evidence": "The issue mentions 'xputest' which suggests the use of Intel's XPU, but specific platform details are not provided."}}, "reporter": "daisyden", "assignee": "ashokei", "resolution": {"score": 0, "evidence": "No information is provided about the resolution status or potential fixes."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects specific test cases in the distributed module, potentially indicating a problem with FSDP (Fully Sharded Data Parallel) functionality, which could impact the reliability of distributed training in PyTorch."}, "state": "open", "labeled_module": {"module": "distributed", "evidence": "The issue is labeled under 'bug.module: distributed' and the test cases pertain to FSDP, which is part of the distributed training features."}, "predicted_module": {"module": "distributed", "evidence": "The test cases failing are related to FSDP, which is a distributed training feature in PyTorch. The error occurs during process joining, which is a critical part of distributed execution."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21."}}
{"issue_number": 1468, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the bug, including the test case and results comparison between XPU, CPU, and CUDA."}, "error_message": {"score": 2, "evidence": "The issue includes specific error messages and test results showing discrepancies between XPU and CPU/CUDA outputs."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides the pytest command and sample input tensors, allowing the issue to be reproduced."}, "software_version": {"score": 2, "evidence": "The issue mentions using PyTorch with oneAPI 2025.1."}, "platform": {"score": 2, "evidence": "The issue specifies the platform as XPU with device 'xpu:0'."}}, "reporter": "daisyden", "assignee": "Stonepia", "resolution": {"score": 1, "evidence": "The comment provides a timeline for the expected resolution (2025.2) but does not specify the exact resolution steps or the root cause of the issue."}, "root_cause": {"score": 1, "evidence": "The comment suggests that the issue is related to oneAPI but does not provide a specific root cause or detailed explanation."}, "impact": {"score": 2, "evidence": "The issue affects the correctness of the argmin function for specific integer types, which could impact applications relying on accurate minima computation."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under 'bug.dependency component: oneAPI.'"}, "predicted_module": {"module": "quant", "evidence": "The issue relates to integer types and potential quantization issues in the implementation."}, "report_date": {"update": "2025-05-14", "evidence": "The issue was last updated on 2025-05-14."}, "last_update": {"update": "2025-05-14", "evidence": "The issue was last updated on 2025-05-14."}}
{"issue_number": 1465, "issue_description": {"score": 2, "evidence": "The issue describes a bug where non-uniform work-groups are not supported by the target device when using the interpolate function with bicubic mode on BMG hardware."}, "error_message": {"score": 2, "evidence": "RuntimeError: Non-uniform work-groups are not supported by the target device."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes code snippets but lacks a clear, step-by-step guide on how to reproduce the problem. The user would need to infer the exact setup from the provided context."}, "software_version": {"score": 1, "evidence": "Mentions oneAPI 2025.1 but does not specify PyTorch version or other relevant software versions."}, "platform": {"score": 2, "evidence": "The issue is reported on BMG hardware with Windows OS."}}, "reporter": "daisyden", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No information provided about the resolution status or process."}, "root_cause": {"score": 0, "evidence": "No information about the root cause is provided in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects the functionality of the interpolate operation on BMG hardware, potentially causing tests to fail and preventing proper image resizing or interpolation tasks."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue is labeled under 'test' and 'os: Windows.hw: BMG' suggesting it relates to testing and a specific hardware setup."}, "predicted_module": {"module": "torchbench", "evidence": "The issue involves a test case for the interpolate function, which is part of the core functionality, but the test setup suggests it's within the testing framework."}, "report_date": {"update": "2025-05-14", "evidence": "The issue was last updated on 2025-05-14."}, "last_update": {"update": "2025-05-14", "evidence": "The issue was last updated on 2025-05-14."}}
{"issue_number": 1453, "issue_description": {"score": 2, "evidence": "The issue is described as the BMG machine crashing when running HuggingFace performance mode, with specific commands provided."}, "error_message": {"score": 2, "evidence": "The comments explain that the issue arises due to high memory pressure and incorrect memory release after using shared GPU memory. The fix involves using only dedicated GPU memory to align with CUDA behavior."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The Python command to reproduce the issue is provided."}, "software_version": {"score": 2, "evidence": "Includes specific versions of Triton, PyTorch, and drivers."}, "platform": {"score": 2, "evidence": "Mentions Windows, XPU devices, and specific driver versions."}}, "reporter": "libohao1201", "assignee": "Stonepia", "resolution": {"score": 2, "evidence": "The resolution involves updating the driver and ensuring only dedicated GPU memory is used. The fix is tracked internally and scheduled for release 2.7."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as high memory pressure and improper memory management when using shared GPU memory, leading to system crashes."}, "impact": {"score": 1, "evidence": "The issue causes machine crashes when running performance mode, but the impact on other scenarios is unclear."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "Labels include 'bug.client.hw' and 'BMG.dependency' component."}, "predicted_module": {"module": "dependency bug", "evidence": "Issue relates to hardware dependency and BMG component."}, "report_date": {"update": "2025-04-16", "evidence": "Last updated date provided."}, "last_update": {"update": "2025-04-16", "evidence": "Last updated date provided."}}
{"issue_number": 1432, "issue_description": {"score": 2, "evidence": "The issue is titled 'SDPA cases failed after XPU enabled in stock pytorch' and the body provides details about the failed tests and the error message."}, "error_message": {"score": 2, "evidence": "The error message indicates a shape mismatch between the meta and real implementations of the scaled dot product attention function."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes the commit hash and the test cases that failed, but specific steps to reproduce are not clearly outlined."}, "software_version": {"score": 2, "evidence": "The versions of PyTorch and torch-xpu-ops are provided."}, "platform": {"score": 2, "evidence": "The issue mentions XPU and the specific device used in the test."}}, "reporter": "daisyden", "assignee": "LuFinch", "resolution": {"score": 0, "evidence": "No information about the resolution is provided."}, "root_cause": {"score": 0, "evidence": "No information about the root cause is provided."}, "impact": {"score": 1, "evidence": "The issue affects the transformer tests in PyTorch, specifically the scaled dot product attention functionality."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The failed tests are in test_transformers_xpu.py, indicating the issue is related to the transformers module."}, "predicted_module": {"module": "transformers", "evidence": "The test cases involve transformer encoder and multihead attention, which are part of the transformers module."}, "report_date": {"update": "2025-03-20", "evidence": "The issue was last updated on 2025-03-20."}, "last_update": {"update": "2025-03-20", "evidence": "The last update date is provided."}}
{"issue_number": 1401, "issue_description": {"score": 2, "evidence": "The issue is about a test failure in test_weight_norm.py where the tensor-like values are not close, indicating a discrepancy between CPU and XPU computations."}, "error_message": {"score": 2, "evidence": "AssertionError: Tensor-likes are not close!\", with details about mismatched elements and differences in tensor values."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The test can be reproduced using the command `python test_weight_norm.py TestNNMethod.test_weight_norm_different_type`."}, "software_version": {"score": 0, "evidence": "PyTorch version is not specified in the issue."}, "platform": {"score": 2, "evidence": "The issue is reported on Windows with hardware ARC."}}, "reporter": "huaiyuzh", "assignee": "daisyden", "resolution": {"score": 0, "evidence": "No resolution information is provided in the issue."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue."}, "impact": {"score": 2, "evidence": "The test failure could indicate a bug in the weight normalization implementation affecting CPU and XPU compatibility."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled under the 'UT' category, which stands for Unit Tests."}, "predicted_module": {"module": "UT", "evidence": "The test case is specifically a unit test for the weight normalization functionality."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, and created on 2025-02-24."}, "last_update": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 07:42:55+00:00."}}
{"issue_number": 1400, "issue_description": {"score": 2, "evidence": "The issue is about a test failure in the RMSNorm test on Windows using the ARC platform. The error is an AssertionError due to tensor-like values not being close."}, "error_message": {"score": 2, "evidence": "AssertionError: Tensor-likes are not close! The tensors have a 0.8% mismatch with significant differences in elements."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The test can be reproduced using pytest with the command `python test_rms_norm.py TestNNMethod.test_rms_norm_bw`."}, "software_version": {"score": 2, "evidence": "PyTorch version is not explicitly mentioned, but the test uses PyTorch functions and is in the context of Intel's XPU operations."}, "platform": {"score": 2, "evidence": "Test is run on Windows with the ARC platform."}}, "reporter": "huaiyuzh", "assignee": "PenghuiCheng", "resolution": {"score": 2, "evidence": "The comments provide a clear resolution path by identifying the issue as related to IPEX ops and suggesting that the problem can only be reproduced on ARC. This helps in pinpointing the root cause and the steps needed for further investigation."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as being related to IPEX ops, specifically `torch.xpu.IpexRmsNorm`, and that the issue can only be reproduced on ARC. This provides a clear understanding of where the problem lies."}, "impact": {"score": 2, "evidence": "The failure in the RMSNorm test affects the correctness of the neural network layers, potentially impacting training and inference using RMSNorm."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is labeled under 'client.hw: Arc' and involves unit testing."}, "predicted_module": {"module": "UT", "evidence": "The test is part of the unit testing framework for RMSNorm operations."}, "report_date": {"update": "2025-05-20", "evidence": "The issue was last updated on 2025-05-20."}, "last_update": {"update": "2025-05-20", "evidence": "The issue was last updated on 2025-05-20."}}
{"issue_number": 1381, "issue_description": {"score": 2, "evidence": "The issue describes a performance regression of up to 76% caused by pad_sequence and gru.input in molan."}, "error_message": {"score": 0, "evidence": "No specific error message provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No steps provided to reproduce the issue."}, "software_version": {"score": 0, "evidence": "No pytorch version or platform info mentioned."}, "platform": {"score": 0, "evidence": "No platform information provided."}}, "reporter": "huaiyuzh", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No information on resolution provided."}, "root_cause": {"score": 0, "evidence": "No root cause analysis provided."}, "impact": {"score": 2, "evidence": "Performance regression up to 76% in molan."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue is labeled with performance, which may relate to performance optimizations or benchmarks."}, "predicted_module": {"module": "torchbench", "evidence": "The issue relates to performance regression in specific functions, suggesting it's related to performance benchmarking or optimizations."}, "report_date": {"update": "2025-05-21", "evidence": "Issue last updated at 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "Issue last updated at 2025-05-21."}}
{"issue_number": 1352, "issue_description": {"score": 2, "evidence": "The issue is clearly described with details about the error occurring on iGPU on Windows and passing on BMG, along with the error message and versions."}, "error_message": {"score": 2, "evidence": "The error message is provided clearly, including the traceback and the specific RuntimeError."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps are provided to reproduce the issue."}, "software_version": {"score": 2, "evidence": "PyTorch version and XPU version are clearly stated."}, "platform": {"score": 2, "evidence": "Platform information includes Windows OS and Intel Arc 140V GPU details."}}, "reporter": "Stonepia", "assignee": "Stonepia", "resolution": {"score": 2, "evidence": "The issue is noted as non-blocking with a PyTorch warning, and internal tracking shows readiness for deployment."}, "root_cause": {"score": 2, "evidence": "The root cause is explained as a driver limitation on integrated platforms, with specific references to UR and Sysman modules."}, "impact": {"score": 1, "evidence": "The impact is somewhat clear as it affects memory information retrieval on iGPU under specific conditions."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "Label explicitly mentions 'dependency bug' and 'driver'."}, "predicted_module": {"module": "dependency bug", "evidence": "Issue relates to missing aspect in device which might be a driver or dependency issue."}, "report_date": {"update": "2025-04-16", "evidence": "Last update date is provided."}, "last_update": {"update": "2025-04-16", "evidence": "Last update date is provided."}}
{"issue_number": 1329, "issue_description": {"score": 2, "evidence": "The issue describes a NotImplementedError when using torch.ao.quantization.quantize_dynamic with XPU device. It provides a code snippet that reproduces the error and logs from the execution."}, "error_message": {"score": 2, "evidence": "The error message clearly states that 'quantized::linear_dynamic' is not implemented for XPU and provides a fallback option."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The user provided a Python code snippet that can be used to reproduce the issue."}, "software_version": {"score": 2, "evidence": "The versions of relevant libraries are listed, including torch 2.5.0a0, intel_extension_for_pytorch 2.5.10, etc."}, "platform": {"score": 2, "evidence": "The issue involves XPU device and oneDNN version information is provided."}}, "reporter": "gurwinderintel", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No information provided on the resolution."}, "root_cause": {"score": 0, "evidence": "No information provided on the root cause."}, "impact": {"score": 2, "evidence": "The issue affects the functionality of quantization on XPU, potentially impacting models using dynamic quantization."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue is labeled under 'module: quant' and the description relates to quantization."}, "predicted_module": {"module": "quant", "evidence": "The issue involves the use of quantize_dynamic from torch.ao.quantization, which is part of the quantization module."}, "report_date": {"update": "2025-05-14", "evidence": "Last updated date is provided."}, "last_update": {"update": "2025-05-14", "evidence": "Last updated date is provided."}}
{"issue_number": 1324, "issue_description": {"score": 2, "evidence": "The issue describes a bug where running models on Windows with XPU devices leads to UR errors when the model runs out of memory. It provides detailed steps to reproduce the issue, including code snippets that cause an OutOfMemoryError and subsequent UR errors. The description is clear and concise, making it easy to understand the problem."}, "error_message": {"score": 2, "evidence": "The error messages include specific UR backend errors (e.g., UR_RESULT_ERROR_OUT_OF_RESOURCES) and tracebacks that help pinpoint where the failure occurs. These messages are detailed and directly relevant to the issue."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides clear reproduce steps, including Python code snippets that fill GPU memory and trigger the OOM and UR errors. The steps are well-documented and easy to follow."}, "software_version": {"score": 2, "evidence": "The versions section includes PyTorch and platform information, which is essential for reproducing the issue. It specifies the environment details, including the use of a specific Python environment."}, "platform": {"score": 2, "evidence": "The issue mentions the platform (Windows) and the specific hardware (Intel XPU), which are crucial for understanding the context of the problem."}}, "reporter": "Stonepia", "assignee": "guangyey", "resolution": {"score": 0, "evidence": "No information is provided about the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 2, "evidence": "The issue highlights that the tensor context becomes broken after an OOM error, which can lead to further runtime errors and instability in applications using PyTorch with XPU devices. This impact is clearly described and demonstrates the severity of the problem."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under the 'dependency bug' module, which relates to issues with dependencies or third-party components. This is supported by the label 'dependency component: driver'."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue involves problems with memory management and tensor operations, which can be linked to dependency issues, especially given the mention of the driver component. Therefore, the predicted module aligns with the labeled module."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23, which is provided in the issue details."}, "last_update": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23, which is provided in the issue details."}}
{"issue_number": 1305, "issue_description": {"score": 2, "evidence": "The issue describes that models fail accuracy on BMG but pass on PVC, providing specific test cases and accuracy results."}, "error_message": {"score": 2, "evidence": "The issue provides error messages such as 'fail_accuracy' for multiple models and test cases."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes a shell command to run the benchmarks but lacks detailed reproduce steps for different scenarios."}, "software_version": {"score": 2, "evidence": "Versions of all relevant software and hardware are provided, including PyTorch, Torch-xpu-ops, and others."}, "platform": {"score": 2, "evidence": "The issue specifies the OS as Ubuntu 24.10 and device as BMG with specific driver versions."}}, "reporter": "mengfei25", "assignee": "Stonepia", "resolution": {"score": 2, "evidence": "The resolution plan involves aligning the optimizer behavior with CUDA's and submitting a PR. This is clearly stated in the comments."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as the fallback to SGD optimizer for certain models, as explained in the comments."}, "impact": {"score": 2, "evidence": "The issue affects multiple models and training configurations, impacting the accuracy of the models on BMG hardware."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue involves failing accuracy tests in torchbench and timm_models."}, "predicted_module": {"module": "torchbench", "evidence": "The issue primarily affects models tested through torchbench and timm, indicating a possible issue in the torchbench or related modules."}, "report_date": {"update": "2025-04-02", "evidence": "The issue was last updated on 2025-04-02 and created on 2025-01-20."}, "last_update": {"update": "2025-04-02", "evidence": "Last updated at 2025-04-02 07:39:12+00:00."}}
{"issue_number": 1276, "issue_description": {"score": 2, "evidence": "The issue describes that during inference, an out-of-memory error occurs with Hf_T5_base, while training passes. The error message indicates insufficient memory allocation on the XPU device."}, "error_message": {"score": 2, "evidence": "The error message is clear: 'torch.OutOfMemoryError: XPU out of memory...' and provides details about memory allocation."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue provides the command line to reproduce: `python benchmarks/dynamo/torchbench.py --accuracy --bfloat16 -d xpu -n10 --inference --only hf_T5_base --backend=inductor`"}, "software_version": {"score": 2, "evidence": "Includes Python version, PyTorch commit IDs, TRITON, TorchVision, TorchAudio, and driver versions."}, "platform": {"score": 2, "evidence": "Device is PVC 1100, OS is Ubuntu 22.04.2 LTS."}}, "reporter": "mengfei25", "assignee": "LuFinch", "resolution": {"score": 2, "evidence": "The comment suggests a potential resolution by indicating that non-fused SDPA should only take 768M or 2x/3x of it, implying that adjusting the memory allocation or optimizing the operation might resolve the OOM issue."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as the failure of `nn.functional.softmax` in eager mode to fuse with the SDPA operator, leading to higher memory usage and subsequent OOM errors."}, "impact": {"score": 1, "evidence": "The issue affects the ability to run inference with Hf_T5_base model, causing OOM errors on XPU."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue involves the T5 model from the transformers library."}, "predicted_module": {"module": "transformers", "evidence": "The test case is related to the Hf_T5_base model, which is part of the transformers module."}, "report_date": {"update": "2025-03-20", "evidence": "Last updated on 2025-03-20."}, "last_update": {"update": "2025-03-20", "evidence": "Last updated on 2025-03-20."}}
{"issue_number": 1261, "issue_description": {"score": 2, "evidence": "The issue describes a problem with Stable_diffusion_unet running out of XPU memory during inference, specifically mentioning that fp16 and bf16 work but others fail."}, "error_message": {"score": 2, "evidence": "The error message clearly states 'XPU out of memory' with details about memory allocation attempts and current usage."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides a specific Python command to reproduce the issue, which is detailed and executable."}, "software_version": {"score": 2, "evidence": "Versions of Python, PyTorch, and other relevant packages are provided, making it clear which versions are in use."}, "platform": {"score": 2, "evidence": "The platform details, including device (PVC 1100), driver version, kernel version, etc., are provided."}}, "reporter": "mengfei25", "assignee": "tye1", "resolution": {"score": 0, "evidence": "No information is provided about potential resolutions or fixes."}, "root_cause": {"score": 0, "evidence": "No information is provided about the root cause of the issue."}, "impact": {"score": 1, "evidence": "The issue affects the functionality of the Stable_diffusion_unet model, potentially limiting its usability on XPU devices, but the extent of the impact is not fully detailed."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue is labeled with E2E.Accuracy.module: torchbench, indicating the module it's related to."}, "predicted_module": {"module": "transformers", "evidence": "The issue involves the Stable_diffusion_unet model, which is part of the transformers module in PyTorch."}, "report_date": {"update": "2025-04-22", "evidence": "Last updated on 2025-04-22, indicating recent activity on the issue."}, "last_update": {"update": "2025-04-22", "evidence": "The issue was last updated on 2025-04-22."}}
{"issue_number": 1256, "issue_description": {"score": 2, "evidence": "The issue describes that HF/Timm/Torchbench models are encountering 'eager_two_runs_differ'. It lists affected models under different categories and provides version details. This is clear and concise."}, "error_message": {"score": 0, "evidence": "No specific error message is provided in the issue."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No detailed steps to reproduce the issue are given."}, "software_version": {"score": 2, "evidence": "PyTorch and related packages versions are specified, along with commit hashes. This is clear and concise."}, "platform": {"score": 2, "evidence": "Driver versions, Conda environment, and transformer details are provided."}}, "reporter": "libohao1201", "assignee": "Stonepia", "resolution": {"score": 0, "evidence": "No resolution details are provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue."}, "impact": {"score": 2, "evidence": "The issue affects multiple models across different frameworks, which could impact performance and reliability of these models when using XPU operations."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue is labeled under E2E.client.hw: BMG, suggesting it relates to hardware or benchmarking."}, "predicted_module": {"module": "torchbench", "evidence": "The issue involves benchmarking models where 'eager_two_runs_differ' indicates discrepancies in performance metrics across runs, which is a concern in benchmarking."}, "report_date": {"update": "2025-03-12", "evidence": "The issue was last updated on 2025-03-12."}, "last_update": {"update": "2025-03-12", "evidence": "The issue was last updated on 2025-03-12."}}
{"issue_number": 1214, "issue_description": {"score": 2, "evidence": "The issue describes random test failures in precision tests with AssertionError messages."}, "error_message": {"score": 2, "evidence": "AssertionError: Tensor-likes are not close!"}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions specific test cases but lacks clear reproduce steps using Python, pytest, or shell commands."}, "software_version": {"score": 2, "evidence": "PyTorch version: N/A, Python version: 3.10.15, OS: Ubuntu 22.04.2 LTS."}, "platform": {"score": 2, "evidence": "Running on Intel(R) Xeon(R) Platinum 8468 with specific CPU features."}}, "reporter": "PenghuiCheng", "assignee": "daisyden", "resolution": {"score": 0, "evidence": "No resolution information provided."}, "root_cause": {"score": 0, "evidence": "No root cause information provided."}, "impact": {"score": 1, "evidence": "Random test failures in precision tests affecting specific test cases."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "Issue labels: triaged.module: op, impl.dtype: complex."}, "predicted_module": {"module": "UT", "evidence": "Test cases are part of unit tests in test_ops_xpu.py."}, "report_date": {"update": "2025-05-14", "evidence": "Last updated on 2025-05-14."}, "last_update": {"update": "2025-05-14", "evidence": "Last updated on 2025-05-14."}}
{"issue_number": 1195, "issue_description": {"score": 2, "evidence": "The issue describes that the reporter encounters NaN values when using complex dtypes in PyTorch with XPU operations. The problem is related to the torch-xpu-ops library, and the reporter provided multiple test cases that fail due to NaN outputs."}, "error_message": {"score": 2, "evidence": "The error message is 'NaN with complex dtype', which clearly indicates that the issue is about the occurrence of NaN values when using complex data types."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides detailed steps to reproduce the issue using specific test commands. These steps are clear and can be directly used to replicate the problem."}, "software_version": {"score": 2, "evidence": "The PyTorch version and commit hash are provided, along with the install command, making it easy to identify the exact versions used."}, "platform": {"score": 2, "evidence": "The platform information includes the OS (Windows 11) and the machine type (LNL), which is sufficient for reproducibility."}}, "reporter": "Stonepia", "assignee": "Stonepia", "resolution": {"score": 0, "evidence": "No information is provided about the resolution process or status of the issue."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue description."}, "impact": {"score": 2, "evidence": "The issue impacts multiple test cases related to unary ufuncs with complex dtypes, which could affect the correctness of computations involving complex numbers in PyTorch on XPU devices."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under 'client.module: dependency bug.dependency component: oneAPI.dtype: complex', indicating that it relates to dependencies and complex dtypes."}, "predicted_module": {"module": "dependency bug", "evidence": "Based on the test cases and labels, the issue is primarily related to dependencies and complex dtype handling, which falls under the 'dependency bug' module."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23, and created on 2024-12-23."}, "last_update": {"update": "2025-04-23", "evidence": "The last update date is provided in the issue details."}}
{"issue_number": 1173, "issue_description": {"score": 2, "evidence": "The issue describes a fatal error 'Illegal instruction' occurring during pytest execution of a specific test case involving torch.grid_sampler_2d with float64 dtype. The error only occurs when running via pytest, not when running the code directly. It's reproducible with a provided test case and affects both XPU package versions. The issue is linked to the Windows platform and specific PyTorch and torch-xpu-ops versions."}, "error_message": {"score": 2, "evidence": "The error message is 'Fatal Python error: Illegal instruction' with a traceback pointing to the test case execution in pytest."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "A minimal test case is provided that reproduces the issue when run with pytest using float64 dtype."}, "software_version": {"score": 2, "evidence": "PyTorch version and torch-xpu-ops commit are specified, along with the installation command."}, "platform": {"score": 2, "evidence": "The issue occurs on Windows 11 with an LNL machine."}}, "reporter": "daisyden", "assignee": "xuhancn", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause information is provided."}, "impact": {"score": 2, "evidence": "The issue causes test failures in extended UT, potentially affecting the reliability and functionality of the XPU package for Windows users."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "Labels include 'client.os: Windows.hw : LNL.dependency component: MSVC.' indicating a dependency-related issue."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue is linked to Windows and MSVC, suggesting it's related to dependencies or compilation issues."}, "report_date": {"update": "2025-05-22", "evidence": "Last updated on 2025-05-22."}, "last_update": {"update": "2025-05-22", "evidence": "Last updated on 2025-05-22."}}
{"issue_number": 1171, "issue_description": {"score": 2, "evidence": "The issue describes an unexpected error message on Windows with LNL, specifically mentioning the test case and the error message encountered."}, "error_message": {"score": 2, "evidence": "'Assertion `maxind >= 0 && maxind < outputImageSize` failed' not found in the test output."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides a command to execute the test: `python test\\nn\\test_pooling.py TestPoolingNNDeviceTypeXPU.test_MaxUnpool_index_errors_case2_xpu`."}, "software_version": {"score": 2, "evidence": "20241202 wheel is mentioned."}, "platform": {"score": 2, "evidence": "Windows.hw : LNL."}}, "reporter": "daisyden", "assignee": "gaopengff", "resolution": {"score": 0, "evidence": "No information provided on resolution status or steps taken."}, "root_cause": {"score": 0, "evidence": "No information provided on the root cause of the issue."}, "impact": {"score": 2, "evidence": "The issue impacts the test case related to MaxUnpool on Windows, potentially affecting functionality in PyTorch XPU operations."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is related to a test case in the test_pooling_xpu.py file, indicating it pertains to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The test case is part of the unit testing framework for pooling operations."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}, "last_update": {"update": "2025-04-23", "evidence": "Last updated at 2025-04-23 07:52:47+00:00."}}
{"issue_number": 1165, "issue_description": {"score": 2, "evidence": "The issue description mentions adding a test for PyTorch XPU with Huggingface Transformers, with clear goals and approach."}, "error_message": {"score": 0, "evidence": "No specific error message provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No steps to reproduce the issue are provided."}, "software_version": {"score": 0, "evidence": "No specific versions mentioned."}, "platform": {"score": 0, "evidence": "No platform details provided."}}, "reporter": "dvrogozh", "assignee": "RUIJIEZHONG66166", "resolution": {"score": 0, "evidence": "No resolution information available."}, "root_cause": {"score": 0, "evidence": "No root cause information provided."}, "impact": {"score": 0, "evidence": "No impact assessment provided."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "Issue labeled with module: transformers."}, "predicted_module": {"module": "transformers", "evidence": "Issue involves integrating Huggingface Transformers with PyTorch XPU."}, "report_date": {"update": "2025-03-12", "evidence": "Issue last updated on 2025-03-12."}, "last_update": {"update": "2025-03-12", "evidence": "Issue last updated on 2025-03-12."}}
{"issue_number": 1159, "issue_description": {"score": 2, "evidence": "The issue describes a runtime error when using Huggingface models DebertaForQuestionAnswering and DebertaV2ForMaskedLM with the error message 'RuntimeError: value cannot be converted to type at::BFloat16 without overflow'. The description is concise and provides enough context about the problem."}, "error_message": {"score": 2, "evidence": "The error message is specific: 'RuntimeError: value cannot be converted to type at::BFloat16 without overflow'. It clearly indicates the type conversion issue during model inference."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides a specific Python script command to reproduce the issue, including the use of certain parameters and flags. This makes it clear how to reproduce the problem."}, "software_version": {"score": 2, "evidence": "The versions section provides detailed information about the PyTorch version, Torch-XPU-Ops, driver version, and Conda setup, which is essential for reproducibility."}, "platform": {"score": 2, "evidence": "The platform is clearly stated as 'LNL - Win11', providing the environment details needed for reproduction."}}, "reporter": "libohao1201", "assignee": "Stonepia", "resolution": {"score": 1, "evidence": "The comments mention that the issue will be updated when the transformer is fixed, but they do not provide a detailed resolution or root cause analysis."}, "root_cause": {"score": 0, "evidence": "No analysis or potential root causes are provided in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects the functionality of specific Huggingface models on Windows, but the broader impact on other models or use cases is not fully described."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under 'dependency bug.dependency: third_party packages', indicating a dependency-related issue."}, "predicted_module": {"module": "transformers", "evidence": "The issue involves Huggingface models, specifically DebertaForQuestionAnswering and DebertaV2ForMaskedLM, which are part of the transformers library. The error occurs during model inference, pointing towards a problem in the model implementation or interaction with PyTorch."}, "report_date": {"update": "2025-03-12", "evidence": "The issue was last updated on 2025-03-12 and created on 2024-12-11, providing clear date information."}, "last_update": {"update": "2025-03-12", "evidence": "The last update date is provided, indicating recent activity on the issue."}}
{"issue_number": 1124, "issue_description": {"score": 2, "evidence": "The issue describes precision issues in extreme value processing with Numpy and XPU results being inconsistent, affecting std operations on complex operands. It also lists several test cases that have been fixed or are failing, providing clear context about the problem's scope and impact."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "daisyden", "assignee": "daisyden", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 2, "evidence": "The issue affects multiple mathematical functions (exp2, tanhshrink, acos, acosh, asin, log, tanh) across different data types (complex64, complex128, complex32), potentially leading to incorrect results in applications relying on precise computations with complex numbers."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The label is 'triaged.dependency component: oneAPI.' which indicates the issue is related to dependencies or components under oneAPI, which is a dependency management system."}, "predicted_module": {"module": "dependency bug", "evidence": "The issue relates to oneAPI, a dependency management system, and the problem stems from compiler dependencies affecting numerical precision, which falls under dependency management issues."}, "report_date": {"update": "2025-04-22", "evidence": "Last updated at 2025-04-22 19:03:27+00:00"}, "last_update": {"update": "2025-04-22", "evidence": "Last updated at 2025-04-22 19:03:27+00:00"}}
{"issue_number": 1121, "issue_description": {"score": 2, "evidence": "The issue describes a bug where the kernel bundle is not device specific under a specific platform context, leading to incorrect kernel execution on a device. The reporter suggests reverting to the original usage of sycl::get_kernel_bundle without specifying the device, which resolves the issue."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "OP impl", "evidence": "The issue is labeled under 'module: op impl'."}, "predicted_module": {"module": "OP impl", "evidence": "The issue relates to the implementation of SYCL kernel bundles and their device specificity, which falls under the 'op impl' module."}, "report_date": {"update": "2025-05-14", "evidence": "Last updated at 2025-05-14 07:41:57+00:00"}, "last_update": {"update": "2025-05-14", "evidence": "Last updated at 2025-05-14 07:41:57+00:00"}}
{"issue_number": 1059, "issue_description": {"score": 2, "evidence": "Issue title and body provide clear and concise information about the feature request, including existing code references and TODO items."}, "error_message": {"score": 0, "evidence": "No error message is provided in the issue."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific reproduce steps are provided."}, "software_version": {"score": 0, "evidence": "No information about PyTorch version or platform is provided."}, "platform": {"score": 0, "evidence": "No platform information is given."}}, "reporter": "fengyuan14", "assignee": "majing921201", "resolution": {"score": 0, "evidence": "No resolution information is available."}, "root_cause": {"score": 0, "evidence": "No root cause is discussed in the issue."}, "impact": {"score": 1, "evidence": "The issue mentions a dependency component, indicating potential impact on the project, but specifics are unclear."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The issue is labeled under 'dependency component: oneAPI.'"}, "predicted_module": {"module": "dependency bug", "evidence": "The issue relates to SYCL RT and kernel-specific max work group size, which falls under dependency components."}, "report_date": {"update": "2025-05-21", "evidence": "Last updated date is provided."}, "last_update": {"update": "2025-05-21", "evidence": "Last updated date is provided."}}
{"issue_number": 1016, "issue_description": {"score": 2, "evidence": "The issue describes a performance problem with severe host overhead in sycl::get_kernel_bundle. The reporter provides a code snippet and performance data, explaining the impact on kernel launches and comparing to CUDA performance."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 1, "evidence": "Mentions torch-xpu-ops: latest main and Intel DPC++ compiler/rt version."}, "platform": {"score": 0, "evidence": ""}}, "reporter": "fengyuan14", "assignee": "majing921201", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 2, "evidence": "Explains two main impacts: 40us overhead not acceptable for some single batch inference, and compares to CUDA's 6us kernel launch."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "Performance issue in op impl."}, "predicted_module": {"module": "torchbench", "evidence": "Performance impact on kernel launches suggests testing in benchmarking framework."}, "report_date": {"update": "2025-03-12", "evidence": "Last updated at 2025-03-12 08:20:33+00:00"}, "last_update": {"update": "2025-03-12", "evidence": "Last updated at 2025-03-12 08:20:33+00:00"}}
{"issue_number": 1005, "issue_description": {"score": 2, "evidence": "The issue includes a detailed description of integrating oneDNN GEMM INT4 kernels for Torchao LLM usage, including passing unit tests and example workloads."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "riverliuintel", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue has the label 'module: quant.'"}, "predicted_module": {"module": "quant", "evidence": "The issue description mentions integrating GEMM INT4 kernels, which relates to quantization."}, "report_date": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 06:19:20+00:00"}, "last_update": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 06:19:20+00:00"}}
{"issue_number": 970, "issue_description": {"score": 2, "evidence": "The issue describes that the CPU time for aten::sum is worse when using torch-xpu-ops compared to IPEX. The reporter provides performance metrics comparing override and non-override scenarios."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 2, "evidence": "Mentions 'Latest torch-xpu-ops vs IPEX 2.3 implementation'"}, "platform": {"score": 0, "evidence": ""}}, "reporter": "fengyuan14", "assignee": "majing921201", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 2, "evidence": "The comment by majing921201 on 2024-10-29 provides a link to the root cause in another issue (https://github.com/intel/llvm/issues/15824), which is concise and clear."}, "impact": {"score": 2, "evidence": "The performance issue affects the CPU time for reduction operations, which could impact overall application performance."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue is labeled under 'performance' and discusses reduction operations, which relates to performance benchmarks."}, "predicted_module": {"module": "torchbench", "evidence": "The issue discusses performance metrics of reduction operations, indicating it's related to benchmarking."}, "report_date": {"update": "2025-04-23", "evidence": "Last updated on 2025-04-23"}, "last_update": {"update": "2025-04-23", "evidence": "Last updated on 2025-04-23"}}
{"issue_number": 969, "issue_description": {"score": 2, "evidence": "The issue describes that the performance of the nonzero function in torch-xpu-ops is worse than IPEX in terms of host overhead, with specific performance metrics provided. This is a clear and concise description of the problem."}, "error_message": {"score": 1, "evidence": "No error message was provided in the comments."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific reproduce steps are provided in the issue description."}, "software_version": {"score": 1, "evidence": "The versions of torch-xpu-ops and IPEX 2.3 are mentioned, but specific PyTorch version and platform information are not provided."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "fengyuan14", "assignee": "majing921201", "resolution": {"score": 1, "evidence": "The comment mentions a possible resolution by filing an issue with the compiler, but it's not clearly stated."}, "root_cause": {"score": 1, "evidence": "The root cause is mentioned as the SYCL API used for querying kernel-specific max work group size, but it's not clearly explained."}, "impact": {"score": 1, "evidence": "The issue mentions that the host overhead is worse, which affects performance, but the specific impact is not clearly detailed."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The label is 'performance', which suggests it's related to performance metrics, typically associated with benchmarking modules."}, "predicted_module": {"module": "torchbench", "evidence": "The issue focuses on performance metrics of the nonzero function, which is likely part of the benchmarking module."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}, "last_update": {"update": "2025-04-23", "evidence": "The last activity on the issue is on 2025-04-23."}}
{"issue_number": 964, "issue_description": {"score": 0, "evidence": "Missing detailed issue description, only mentions the need to port unit tests for CUDA coverage."}, "error_message": {"score": 0, "evidence": "No error message provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps provided."}, "software_version": {"score": 0, "evidence": "No specific versions mentioned."}, "platform": {"score": 0, "evidence": "No platform information provided."}}, "reporter": "xytintel", "assignee": "daisyden", "resolution": {"score": 0, "evidence": "No resolution information provided."}, "root_cause": {"score": 0, "evidence": "No root cause information provided."}, "impact": {"score": 0, "evidence": "No impact described."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "Issue is about unit tests."}, "predicted_module": {"module": "UT", "evidence": "Issue involves porting unit tests from test_cuda.py, which relates to unit tests."}, "report_date": {"update": "2025-03-12", "evidence": "Last updated at 2025-03-12 08:02:32+00:00"}, "last_update": {"update": "2025-03-12", "evidence": "Last updated at 2025-03-12 08:02:32+00:00"}}
{"issue_number": 954, "issue_description": {"score": 2, "evidence": "The comments provide concise information about the root cause and resolution. The issue is related to duplicate symbols when using Clang and the inability to work with the lld linker. The comments also mention that the `torchgen.gen` headers in `torch-xpu-ops` cause duplicate symbols with `pytorch` when using Clang, and that `torch-xpu-ops` cannot work with the lld linker due to duplicate symbols. The last updated date is 2025-05-21 06:22:51+00:00."}, "error_message": {"score": 2, "evidence": "The comments do not provide specific information about an error message. The issue is about duplicate symbols and linker issues, but no specific error message is mentioned."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps are provided in the issue content."}, "software_version": {"score": 0, "evidence": "No software version is provided in the issue content."}, "platform": {"score": 0, "evidence": "No platform information is provided in the issue content."}}, "reporter": "gglin001", "assignee": "fengyuan14", "resolution": {"score": 2, "evidence": "The comments suggest that enabling Clang might require slight changes and that there are issues with duplicate symbols and the lld linker. The resolution involves addressing these issues, possibly by modifying the build process or the way headers are generated to avoid duplicate symbols."}, "root_cause": {"score": 2, "evidence": "The root cause is identified as duplicate symbols in the generated headers from `torchgen.gen` in `torch-xpu-ops` and the inability to work with the lld linker. The duplicate symbols cause Clang to fail, and the lld linker reports similar issues."}, "impact": {"score": 0, "evidence": "No impact information is provided in the issue content."}, "state": "open", "labeled_module": {"module": "build", "evidence": "The label is explicitly provided as 'module: build'."}, "predicted_module": {"module": "build", "evidence": "The issue is about building with clang as the host compiler, which relates to the build process."}, "report_date": {"update": "2025-05-21", "evidence": "The last updated date is provided."}, "last_update": {"update": "2025-05-21", "evidence": "The last updated date is provided."}}
{"issue_number": 781, "issue_description": {"score": 2, "evidence": "The issue describes a discrepancy in the results of the square function between CPU and XPU for a complex64 tensor. The CPU returns a positive imaginary part, while the XPU returns -inf. The reporter also provided a code snippet to reproduce the issue and mentioned that the problem is related to the std::pow function implementation in the compiler."}, "error_message": {"score": 2, "evidence": "The error messages show that the CPU returns an imaginary part of 1.0020e+23 while the expected is -1.0020e+23, and the XPU returns -inf for both real and imaginary parts."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "A Python code snippet is provided to reproduce the issue using PyTorch tensors on CPU and XPU devices."}, "software_version": {"score": 2, "evidence": "PyTorch version is mentioned as 'latest'."}, "platform": {"score": 2, "evidence": "The issue involves both CPU and XPU (Intel's oneAPI) platforms."}}, "reporter": "daisyden", "assignee": "daisyden", "resolution": {"score": 0, "evidence": "No information about the resolution status or steps is provided in the issue."}, "root_cause": {"score": 0, "evidence": "No detailed root cause analysis is provided, but the issue is linked to the std::pow function implementation in the compiler."}, "impact": {"score": 2, "evidence": "The discrepancy in results could lead to incorrect computations in applications relying on complex number operations, affecting the reliability of PyTorch's implementation on XPU devices."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The label includes 'triaged.dependency component: oneAPI.dtype: complex.' indicating a dependency issue related to complex number handling."}, "predicted_module": {"module": "dependency bug", "evidence": "Based on the test case involving complex numbers and the use of oneAPI, the issue likely falls under dependency management or component-specific bugs."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23 and was created on 2024-08-20."}, "last_update": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}}
{"issue_number": 772, "issue_description": {"score": 2, "evidence": "The issue is titled '[Evalated] Support of QuantizedXPU // Issues of test__view_ops_xpy.py' and the body describes the need for quantization support and the specific test failures. The description is concise and clear, providing enough context about the problem."}, "error_message": {"score": 2, "evidence": "The error message is 'NotImplementedError: Could not run 'aten::_empty_affine_quantized' with arguments from the 'QuantizedXPU' backend.' which is specific and provides clear information about the issue."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions the test cases 'test_flatten_xpu' and 'test_ravel_xpu' but does not provide detailed steps to reproduce the issue, such as the exact commands or setup required."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version or platform information is provided in the issue."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "PenghuiCheng", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No information is provided about any resolution attempts or updates after the initial report."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue description."}, "impact": {"score": 1, "evidence": "The issue mentions that the tests are failing, which suggests that the quantization support is incomplete or missing for certain operations, potentially affecting the functionality of the QuantizedXPU backend."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue has the label 'triaged.module: quant.' indicating it's related to quantization."}, "predicted_module": {"module": "quant", "evidence": "The issue is about supporting quantized operations, specifically mentioning the '_empty_affine_quantized' function, which is related to quantization."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23 and was created on 2024-08-16."}, "last_update": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}}
{"issue_number": 761, "issue_description": {"score": 2, "evidence": "The issue provides a detailed description of the problem, including specific error messages and test cases that are failing. It also mentions the lack of support for certain backends and datatypes."}, "error_message": {"score": 2, "evidence": "The error messages are clearly listed, such as 'NotImplementedError: Could not run 'aten::_to_copy'...' and 'AssertionError: False is not true'."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue lists the test cases that are failing, but it does not provide explicit steps to reproduce the issue beyond mentioning the test names."}, "software_version": {"score": 0, "evidence": "No specific PyTorch version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "PenghuiCheng", "assignee": "PenghuiCheng", "resolution": {"score": 0, "evidence": "No information is provided about the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No root cause is identified in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects multiple test cases related to the transformer encoder and scaled dot product attention, indicating potential problems with the XPU implementation."}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "The issue is labeled under 'triaged.module: op impl' and involves transformer encoder layer and scaled dot product attention tests."}, "predicted_module": {"module": "transformers", "evidence": "The failing test cases are related to the transformer encoder and scaled dot product attention, which are part of the transformers module."}, "report_date": {"update": "2025-05-14", "evidence": "The issue was last updated on 2025-05-14."}, "last_update": {"update": "2025-05-14", "evidence": "The issue was last updated on 2025-05-14."}}
{"issue_number": 754, "issue_description": {"score": 2, "evidence": "Issue describes failures caused by precision errors due to different compiler or package implementations."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "daisyden", "assignee": "daisyden", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "transformers", "evidence": "Issue relates to test cases for polygamma and nll_loss, which are part of the distributions and nn modules."}, "predicted_module": {"module": "distributed", "evidence": "The test cases involve numerical computations which are part of the distributions module."}, "report_date": {"update": "2025-04-22", "evidence": "Last updated on 2025-04-22"}, "last_update": {"update": "2025-04-22", "evidence": "Last updated on 2025-04-22"}}
{"issue_number": 725, "issue_description": {"score": 2, "evidence": "The issue describes that the training accuracy of detectron2_fcos_r_50_fpn fails when using torchbench with amp_bf16. The error message indicates that FCOS training is not supported by upstream detectron2, and there is a fallback from XPU to CPU which may impact performance. Additionally, the issue mentions a trace of the error and the versions of torch-xpu-ops and PyTorch used."}, "error_message": {"score": 2, "evidence": "The traceback includes a NotImplementedError stating that FCOS train is not supported by detectron2 and provides a link to another GitHub issue. There's also a warning about Aten Op fallback from XPU to CPU."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue includes a command line trace but lacks explicit reproduce steps such as specific Python or shell commands. The user would need to infer the steps from the context provided."}, "software_version": {"score": 2, "evidence": "The versions of torch-xpu-ops and PyTorch are provided, along with device information."}, "platform": {"score": 2, "evidence": "Device details including PVC 1100, bundle version, and driver version are included."}}, "reporter": "mengfei25", "assignee": "retonym", "resolution": {"score": 0, "evidence": "No information is provided about the resolution of the issue."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue description."}, "impact": {"score": 2, "evidence": "The issue affects the training accuracy of a specific model, which could impact the reliability of the framework for detection tasks."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The labels include torchbench and dtype-related tags, indicating the issue is related to the benchmarking module."}, "predicted_module": {"module": "torchbench", "evidence": "The issue involves a benchmark test case for detectron2, which falls under the torchbench module."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}, "last_update": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}}
{"issue_number": 711, "issue_description": {"score": 2, "evidence": "The issue describes that certain quantized models (resnet50_quantized_qat and mobilenet_v2_quantized_qat) are failing with a NotImplementedError indicating that the eval test only supports CPU."}, "error_message": {"score": 2, "evidence": "Traceback shows the error occurs during model loading, specifically in the __init__ method of the model, raising NotImplementedError: The eval test only supports CPU."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue does not provide explicit steps to reproduce the issue, but it mentions the models and the error trace which could be used to infer the steps."}, "software_version": {"score": 2, "evidence": "PyTorch version d224857b3af5c9d5a3c7a48401475c09d90db296 and torch-xpu-ops version from commit 1d70431c072db889d9a47ea4956049fe340a426d are provided."}, "platform": {"score": 2, "evidence": "Device information: pvc 1100, bundle 0.5.3, driver 803.61."}}, "reporter": "mengfei25", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No information provided about the resolution."}, "root_cause": {"score": 0, "evidence": "No information provided about the root cause."}, "impact": {"score": 1, "evidence": "The issue affects the ability to run specific quantized models on devices other than CPU, potentially limiting the functionality of the framework for users requiring GPU or other hardware support."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue involves quantized models (resnet50_quantized_qat and mobilenet_v2_quantized_qat), suggesting it relates to quantization functionality."}, "predicted_module": {"module": "quant", "evidence": "The test case involves quantized models, which are part of the quantization module."}, "report_date": {"update": "2025-04-23", "evidence": "Last updated at 2025-04-23 07:41:29+00:00"}, "last_update": {"update": "2025-04-23", "evidence": "Last updated at 2025-04-23 07:41:29+00:00"}}
{"issue_number": 685, "issue_description": {"score": 2, "evidence": "The issue description is concise and clear, explaining the need to enhance the reduction kernel with data type dynamic cast support for better performance."}, "error_message": {"score": 0, "evidence": "No error message is provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No reproduce steps are provided."}, "software_version": {"score": 0, "evidence": "No PyTorch version is mentioned."}, "platform": {"score": 0, "evidence": "No platform information is provided."}}, "reporter": "fengyuan14", "assignee": "xytintel", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is mentioned."}, "impact": {"score": 0, "evidence": "No impact is specified."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue is labeled with 'performance', which typically relates to the torchbench module."}, "predicted_module": {"module": "torchbench", "evidence": "The issue pertains to enhancing reduction kernels, which is related to performance optimization."}, "report_date": {"update": "2025-03-19", "evidence": "The issue was last updated on 2025-03-19."}, "last_update": {"update": "2025-03-19", "evidence": "The issue was last updated on 2025-03-19."}}
{"issue_number": 632, "issue_description": {"score": 2, "evidence": "The issue describes a problem with Squeezenet1_1 model accuracy when using certain configurations, including the provided code snippet which sets up the model and the testing environment. The issue is clearly explained with the necessary context."}, "error_message": {"score": 2, "evidence": "The user provided a detailed code snippet that includes the setup of the model, the forward pass, and the testing configuration, which helps in understanding the error context. However, specific error messages are not explicitly mentioned, which could have been helpful for pinpointing the issue."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "Reproduction steps are somewhat provided through the code, but they are not clearly listed as separate steps. The code would need to be run as-is to replicate the issue, which may not be straightforward for others to follow without additional instructions."}, "software_version": {"score": 2, "evidence": "The versions of torch-xpu-ops and PyTorch are clearly provided, along with device information, making it easy to replicate the environment."}, "platform": {"score": 2, "evidence": "Device details are provided, including the specific hardware (PVC 1100, 881, 2024.2), which is essential for reproduction."}}, "reporter": "retonym", "assignee": "weishi-deng", "resolution": {"score": 1, "evidence": "The resolution involves reverting a specific PyTorch PR, but the reasoning behind this decision is not clearly explained."}, "root_cause": {"score": 1, "evidence": "The root cause is related to the combination of conv, relu, and adaptive_avgpool in the backward pattern, but more details are needed."}, "impact": {"score": 1, "evidence": "While the issue affects the accuracy of the Squeezenet1_1 model, the specific impact on the system or workflow isn't clearly detailed, making it harder to prioritize the issue."}, "state": "open", "labeled_module": {"module": "Core", "evidence": "The issue is related to the core functionality of the model's accuracy, which falls under the Core module."}, "predicted_module": {"module": "Core", "evidence": "The issue involves the core functionality of the model's accuracy when using specific configurations, which aligns with the Core module."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, indicating recent activity."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, showing that there has been recent interaction with the issue."}}
{"issue_number": 544, "issue_description": {"score": 2, "evidence": "The issue describes numerical differences in log, log1p, and log2 functions between CPU and XPU results, with specific test failures and discrepancies in expected vs actual values. The issue also mentions difficulty in determining which result is more accurate."}, "error_message": {"score": 2, "evidence": "The issue includes specific error messages like 'test_python_ref__refs_log2_xpu_complex128' and 'test_log1p_complex_xpu_complex64', with numerical differences and indices where the discrepancy occurs."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "No specific steps are provided, but the test cases imply that certain Python or pytest commands might be used, though not explicitly detailed."}, "software_version": {"score": 1, "evidence": "PyTorch version is mentioned but not explicitly stated. Platform information is not detailed beyond the test cases."}, "platform": {"score": 1, "evidence": "Only that the tests are run on XPU and CPU, but specific hardware details are missing."}}, "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": "No information about resolution steps or status provided beyond the issue being open."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue description."}, "impact": {"score": 1, "evidence": "The issue affects the accuracy of mathematical functions (log, log1p, log2) for complex numbers on XPU, potentially impacting applications relying on these functions for correct numerical results."}, "state": "open", "labeled_module": {"module": "dependency bug", "evidence": "The label is 'dependency component: oneAPI.dtype: complex', indicating a dependency-related issue in handling complex data types."}, "predicted_module": {"module": "UT", "evidence": "The issue involves test failures in specific test cases, suggesting it relates to unit tests."}, "report_date": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, indicating recent activity."}, "last_update": {"update": "2025-05-21", "evidence": "The issue was last updated on 2025-05-21, indicating recent activity."}}
{"issue_number": 506, "issue_description": {"score": 2, "evidence": "The issue describes a RuntimeError during the training of the demucs model using torchbench_bfloat16_trainingxpu. The error message indicates a type mismatch between the input (float) and bias (BFloat16) in a Convolutional layer."}, "error_message": {"score": 2, "evidence": "RuntimeError: Input type (float) and bias type (c10::BFloat16) should be the same"}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue includes the command `torchbench_bfloat16_trainingxpu train demucs`, which suggests it's part of the PyTorch benchmark suite."}, "software_version": {"score": 2, "evidence": "PyTorch version: 0f81473d7b4a1bf09246410712df22541be7caf3 + PRs: 127277,129120; torch-xpu-ops version: commit 31c400195d63064940242220dc9100322d36bac4"}, "platform": {"score": 2, "evidence": "Device: PVC 1100, 803.61, 0.5.1."}}, "reporter": "mengfei25", "assignee": "jianyizh", "resolution": {"score": 0, "evidence": "No information provided on how to resolve the issue."}, "root_cause": {"score": 0, "evidence": "No information provided on the root cause of the issue."}, "impact": {"score": 0, "evidence": "No information provided on the impact of the issue."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue is labeled with E2E.Accuracy.module: torchbench.training.dtype: float16.dtype: bfloat16.triaged."}, "predicted_module": {"module": "quant", "evidence": "The issue involves mixed data types (float and BFloat16) in a convolutional layer, suggesting it's related to quantization."}, "report_date": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 08:33:55+00:00"}, "last_update": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 08:33:55+00:00"}}
{"issue_number": 492, "issue_description": {"score": 2, "evidence": "The issue describes that the Timm_efficientdet model raises a NotImplementedError due to forcing the use of CUDA during training with AMP FP16 on XPU devices."}, "error_message": {"score": 2, "evidence": "Traceback shows a NotImplementedError with the message: 'The original model code forces the use of CUDA.'"}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue mentions running torchbench_amp_fp16_training but lacks explicit steps like specific commands or configurations."}, "software_version": {"score": 2, "evidence": "Provides torch-xpu-ops commit, PyTorch version with PRs, and device details."}, "platform": {"score": 2, "evidence": "Device details: PVC 1100, 803.61, 0.5.1."}}, "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": {"score": 0, "evidence": "No information provided on potential solutions or workarounds."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue description."}, "impact": {"score": 1, "evidence": "The issue prevents training the Timm_efficientdet model on XPU using AMP FP16 due to CUDA dependency."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "From the label: 'E2E.Accuracy.module: torchbench'"}, "predicted_module": {"module": "torchbench", "evidence": "Based on the test case classification, the issue relates to the torchbench module."}, "report_date": {"update": "2025-05-21", "evidence": "Issue last updated on 2025-05-21."}, "last_update": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 08:33:56+00:00."}}
{"issue_number": 489, "issue_description": {"score": 2, "evidence": "The issue title and body provide a clear description of the problem, including the error message and the test case that failed."}, "error_message": {"score": 2, "evidence": "The error message clearly states that 'xpu not supported' and includes the traceback showing where the error occurred."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue does not provide specific reproduce steps, but it mentions the test case 'torchbench_amp_fp16_training' which can be inferred as part of the setup."}, "software_version": {"score": 2, "evidence": "The versions of torch-xpu-ops and PyTorch are provided, along with the device information."}, "platform": {"score": 2, "evidence": "Device details including PVC 1100, 803.61, and 0.5.1 are specified."}}, "reporter": "mengfei25", "assignee": "weishi-deng", "resolution": {"score": 0, "evidence": "No information about the resolution is provided."}, "root_cause": {"score": 0, "evidence": "No information about the root cause is provided."}, "impact": {"score": 1, "evidence": "The issue prevents the Moco model from being trained on XPU using the specified configurations."}, "state": "open", "labeled_module": {"module": "torchbench", "evidence": "The issue is labeled under 'torchbench'."}, "predicted_module": {"module": "torchbench", "evidence": "The issue involves the Moco model training, which is part of the torchbenchmark suite, and the error occurs during model loading."}, "report_date": {"update": "2025-05-08", "evidence": "Last updated date is provided."}, "last_update": {"update": "2025-05-08", "evidence": "Last updated date is provided."}}
{"issue_number": 461, "issue_description": {"score": 2, "evidence": "The issue is about adding support for FP8 data types in index put cases. The problem is that the XPU implementation does not include FP8 support, so the tests are being skipped temporarily."}, "error_message": {"score": 0, "evidence": ""}, "reproduce_steps": {"steps": {"score": 0, "evidence": ""}, "software_version": {"score": 0, "evidence": ""}, "platform": {"score": 0, "evidence": ""}}, "reporter": "fengyuan14", "assignee": "fengyuan14", "resolution": {"score": 0, "evidence": ""}, "root_cause": {"score": 0, "evidence": ""}, "impact": {"score": 0, "evidence": ""}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The label is dtype: float8, which relates to quantization."}, "predicted_module": {"module": "quant", "evidence": "The issue involves FP8 data types, which are related to quantization in PyTorch."}, "report_date": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 06:23:44+00:00"}, "last_update": {"update": "2025-05-21", "evidence": "Last updated at 2025-05-21 06:23:44+00:00"}}
{"issue_number": 322, "issue_description": {"score": 2, "evidence": "The issue describes that some tests are skipped due to FP8 support issues on specific devices and mixed dtype support on SM 8.x. It also provides details about the tests that are skipped and the environment setup."}, "error_message": {"score": 0, "evidence": "No specific error message is provided."}, "reproduce_steps": {"steps": {"score": 0, "evidence": "No specific steps to reproduce the issue are provided."}, "software_version": {"score": 0, "evidence": "PyTorch version is marked as N/A."}, "platform": {"score": 2, "evidence": "The OS, CPU, and other platform details are provided."}}, "reporter": "yuchengliu1", "assignee": "liangan1", "resolution": {"score": 0, "evidence": "No information about resolution is provided."}, "root_cause": {"score": 0, "evidence": "No information about the root cause is provided."}, "impact": {"score": 2, "evidence": "The issue affects FP8 and mixed dtype operations on certain devices, which could impact performance and functionality for users relying on these features."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is related to test cases in test_matmul_cuda.py, indicating it's a unit test issue."}, "predicted_module": {"module": "UT", "evidence": "The issue involves test failures in test cases, pointing towards unit tests."}, "report_date": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}, "last_update": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}}
{"issue_number": 275, "issue_description": {"score": 2, "evidence": "The issue describes a bug in the test_flip_xpu_float32 test case, specifically mentioning the error in using 'aten::empty_quantized' with the XPU backend."}, "error_message": {"score": 2, "evidence": "The traceback shows a NotImplementedError when trying to run 'aten::empty_quantized' on the XPU backend, indicating that this operator isn't supported there."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "The issue provides a command to reproduce the test: PYTORCH_TEST_WITH_SLOW=1 python test/test_shape_ops.py -k TestShapeOpsXPU.test_flip_xpu_float32."}, "software_version": {"score": 2, "evidence": "PyTorch version is 2.4.0a0+git5fb11cd."}, "platform": {"score": 2, "evidence": "The OS is Ubuntu 22.04.3 LTS on an Intel Xeon Platinum 8480+ CPU."}}, "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No resolution information is provided in the issue."}, "root_cause": {"score": 0, "evidence": "No root cause analysis is provided in the issue."}, "impact": {"score": 2, "evidence": "The issue affects the functionality of the XPU backend's quantization operations, potentially breaking related tests and workflows."}, "state": "open", "labeled_module": {"module": "quant", "evidence": "The issue involves quantization operations, specifically in the test_shape_ops_xpu.py file."}, "predicted_module": {"module": "quant", "evidence": "The test case is related to quantized operations, aligning with the quantization module."}, "report_date": {"update": "2025-04-23", "evidence": "The last update was on 2025-04-23."}, "last_update": {"update": "2025-04-23", "evidence": "The issue was last updated on 2025-04-23."}}
{"issue_number": 264, "issue_description": {"score": 2, "evidence": "The issue describes problems with test cases in the XPU backend, including issues with torch.random.fork_rng, unsupported data types, and device affinity issues."}, "error_message": {"score": 2, "evidence": "Traceback shows an AssertionError due to Torch not being compiled with CUDA enabled when using fork_rng."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Test cases and commands like pytest test*.py are provided, but specific commands for reproducing are not explicitly listed."}, "software_version": {"score": 2, "evidence": "Python 3.10.14, PyTorch version not explicitly mentioned, but XPU backend is involved."}, "platform": {"score": 2, "evidence": "Linux platform with specific environment setup."}}, "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No information provided on how to resolve the issue."}, "root_cause": {"score": 0, "evidence": "No detailed analysis of the root cause is provided."}, "impact": {"score": 1, "evidence": "The issue affects multiple test cases and indicates potential problems in XPU backend support, which could impact functionality and reliability of PyTorch with XPU."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue involves test cases in the test_tensor_creation_ops_xpu.py file, indicating it relates to unit tests."}, "predicted_module": {"module": "UT", "evidence": "The issue is about test failures in the XPU backend, which relates to unit testing."}, "report_date": {"update": "2025-05-14", "evidence": "Last updated at 2025-05-14."}, "last_update": {"update": "2025-05-14", "evidence": "Last updated on 2025-05-14."}}
{"issue_number": 253, "issue_description": {"score": 2, "evidence": "The issue describes a problem with TestMathBitsXPU where 200 cases fail due to a RuntimeError regarding double and complex datatype matmul not being supported in oneDNN."}, "error_message": {"score": 2, "evidence": "RuntimeError: Double and complex datatype matmul is not supported in oneDNN."}, "reproduce_steps": {"steps": {"score": 1, "evidence": "The issue provides a command but lacks detailed steps to reproduce the issue, such as specific test cases or configurations."}, "software_version": {"score": 2, "evidence": "The issue mentions PyTorch but does not specify the version."}, "platform": {"score": 2, "evidence": "No specific platform information is provided."}}, "reporter": "daisyden", "assignee": "ZhiweiYan-96", "resolution": {"score": 0, "evidence": "No resolution information is provided."}, "root_cause": {"score": 0, "evidence": "No root cause is identified."}, "impact": {"score": 1, "evidence": "The issue impacts multiple test cases, indicating a potential problem with complex and double data types in matrix operations."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue is related to test cases, suggesting it falls under the Unit Testing category."}, "predicted_module": {"module": "UT", "evidence": "The test cases mentioned are part of the unit testing framework."}, "report_date": {"update": "2025-05-14", "evidence": "The issue was last updated on 2025-05-14."}, "last_update": {"update": "2025-05-14", "evidence": "The issue was last updated on 2025-05-14."}}
{"issue_number": 184, "issue_description": {"score": 2, "evidence": "Issue title and body provide detailed information about accuracy issues in various operations across different data types and devices."}, "error_message": {"score": 2, "evidence": "Multiple error messages are provided, each indicating specific test failures with tracebacks and numerical discrepancies."}, "reproduce_steps": {"steps": {"score": 2, "evidence": "Command to run the tests is provided: `export PYTORCH_TEST_WITH_SLOW=1 && pytest -v test_ops_xpu.py -k 'test_compare_cpu'`"}, "software_version": {"score": 2, "evidence": "PyTorch version is referenced indirectly through the test setup, but exact version isn't specified."}, "platform": {"score": 2, "evidence": "Tests are run on XPU devices with specific data types (bfloat16, float16, complex64, etc.)."}}, "reporter": "daisyden", "assignee": "huaiyuzh", "resolution": {"score": 1, "evidence": "No specific resolution steps are outlined, but the issue is being tracked with a pending state."}, "root_cause": {"score": 1, "evidence": "Possible root causes are hinted at, such as device-specific issues (e.g., Sycl issues) and accuracy gaps in specific operations."}, "impact": {"score": 2, "evidence": "Impact is significant as it affects multiple operations across different data types, potentially leading to incorrect model outputs."}, "state": "open", "labeled_module": {"module": "UT", "evidence": "The issue relates to unit tests for XPU operations, indicating it's within the testing framework."}, "predicted_module": {"module": "UT", "evidence": "The issue is about test failures in various operations, which points to the unit testing module."}, "report_date": {"update": "2025-04-23", "evidence": "Issue last updated on 2025-04-23."}, "last_update": {"update": "2025-04-23", "evidence": "Last updated at 2025-04-23 07:36:21+00:00."}}
https://github.com/intel/torch-xpu-ops/issues/1693 | @xytintel, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version, impact |  | 
https://github.com/intel/torch-xpu-ops/issues/1667 | @PenghuiCheng, please help to improve impact |  | 
https://github.com/intel/torch-xpu-ops/issues/1656 |  |  | @chuanqi129, please help to set module:UT , ,
Cannot find email for NamedUser(login="ZhaoqiongZ")
https://github.com/intel/torch-xpu-ops/issues/1649 | @ZhaoqiongZ, please help to improve reproduce_steps |  | @chuanqi129, please help to set module:dependency bug , ,
https://github.com/intel/torch-xpu-ops/issues/1641 | @xytintel, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version, impact |  | 
https://github.com/intel/torch-xpu-ops/issues/1636 | @daisyden, please help to improve reproduce_platform |  | 
https://github.com/intel/torch-xpu-ops/issues/1634 | @daisyden, please help to improve reproduce_platform, reproduce_software_version, impact |  | 
https://github.com/intel/torch-xpu-ops/issues/1626 |  | @dvrogozh, please help to follow up the issue as it is only updated on 2025-05-07 | 
https://github.com/intel/torch-xpu-ops/issues/1617 | @PenghuiCheng, please help to improve reproduce_steps, reproduce_platform, reproduce_software_version |  | 
https://github.com/intel/torch-xpu-ops/issues/1614 | @mengfei25, please help to improve error_message | @LuFinch, please help to follow up the issue as it is only updated on 2025-05-07 | 
https://github.com/intel/torch-xpu-ops/issues/1606 | @mengfei25, please help to improve error_message |  | 
Cannot find email for NamedUser(login="ratnampa")
https://github.com/intel/torch-xpu-ops/issues/1604 | @ratnampa, please help to improve error_message, reproduce_steps, impact |  | 
https://github.com/intel/torch-xpu-ops/issues/1597 | @jianyizh, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version |  | 
https://github.com/intel/torch-xpu-ops/issues/1594 | @xytintel, please help to improve reproduce_steps, reproduce_platform, reproduce_software_version | @xytintel, please help to follow up the issue as it is only updated on 2025-05-06 | 
Cannot find email for NamedUser(login="zxd1997066")
https://github.com/intel/torch-xpu-ops/issues/1592 |  | @zxd1997066, please help to follow up the issue as it is only updated on 2025-04-25 | 
https://github.com/intel/torch-xpu-ops/issues/1587 | @xytintel, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version, impact | @xytintel, please help to follow up the issue as it is only updated on 2025-05-09 | 
https://github.com/intel/torch-xpu-ops/issues/1581 | @PenghuiCheng, please help to improve reproduce_platform, reproduce_software_version |  | 
Cannot find email for NamedUser(login="githubsgi")
https://github.com/intel/torch-xpu-ops/issues/1574 | @githubsgi, please help to improve reproduce_steps, reproduce_platform, impact | @ZhiweiYan-96, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/1555 | @PenghuiCheng, please help to improve reproduce_steps, reproduce_platform, reproduce_software_version |  | 
Cannot find email for NamedUser(login="chunhuanMeng")
https://github.com/intel/torch-xpu-ops/issues/1554 |  | @chunhuanMeng, please help to follow up the issue as it is only updated on 2025-05-07 | 
https://github.com/intel/torch-xpu-ops/issues/1551 | @PenghuiCheng, please help to improve reproduce_platform, reproduce_software_version |  | 
https://github.com/intel/torch-xpu-ops/issues/1550 | @PenghuiCheng, please help to improve reproduce_platform, reproduce_software_version | @xytintel, please help to follow up the issue as it is only updated on 2025-05-07 | 
https://github.com/intel/torch-xpu-ops/issues/1547 | @PenghuiCheng, please help to improve reproduce_platform, reproduce_software_version |  | 
https://github.com/intel/torch-xpu-ops/issues/1521 |  | @liangan1, please help to follow up the issue as it is only updated on 2025-04-22 | 
https://github.com/intel/torch-xpu-ops/issues/1519 | @huaiyuzh, please help to improve error_message |  | 
Cannot find email for NamedUser(login="RUIJIEZHONG66166")
Cannot find email for NamedUser(login="RUIJIEZHONG66166")
https://github.com/intel/torch-xpu-ops/issues/1513 | @RUIJIEZHONG66166, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version | @RUIJIEZHONG66166, please help to follow up the issue as it is only updated on 2025-04-17 | 
Cannot find email for NamedUser(login="ZhaoqiongZ")
https://github.com/intel/torch-xpu-ops/issues/1512 | @ZhaoqiongZ, please help to improve error_message |  | 
https://github.com/intel/torch-xpu-ops/issues/1510 |  | @Stonepia, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/1509 | @PenghuiCheng, please help to improve impact |  | 
Cannot find email for NamedUser(login="libohao1201")
https://github.com/intel/torch-xpu-ops/issues/1505 | @libohao1201, please help to improve error_message, reproduce_steps, impact | @chuanqi129, please help to assign an owner to follow up the issue as it is only updated on 2025-03-26 | @chuanqi129, please help to set owner, module:quant , ,
https://github.com/intel/torch-xpu-ops/issues/1502 |  | @chuanqi129, please help to assign an owner to follow up the issue as it is only updated on 2025-03-26 | @chuanqi129, please help to set owner , ,
https://github.com/intel/torch-xpu-ops/issues/1500 |  | @ZhiweiYan-96, please help to follow up the issue as it is only updated on 2025-04-24 | 
Cannot find email for NamedUser(login="libohao1201")
https://github.com/intel/torch-xpu-ops/issues/1496 | @libohao1201, please help to improve reproduce_steps | @chuanqi129, please help to assign an owner to follow up the issue as it is only updated on 2025-03-21 | @chuanqi129, please help to set owner , ,
https://github.com/intel/torch-xpu-ops/issues/1468 |  |  | @chuanqi129, please help to set module:quant , ,
https://github.com/intel/torch-xpu-ops/issues/1453 |  | @Stonepia, please help to follow up the issue as it is only updated on 2025-04-16 | 
https://github.com/intel/torch-xpu-ops/issues/1432 |  | @LuFinch, please help to follow up the issue as it is only updated on 2025-03-20 | 
https://github.com/intel/torch-xpu-ops/issues/1401 | @huaiyuzh, please help to improve reproduce_software_version |  | 
https://github.com/intel/torch-xpu-ops/issues/1381 | @huaiyuzh, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version |  | 
https://github.com/intel/torch-xpu-ops/issues/1352 | @Stonepia, please help to improve reproduce_steps | @Stonepia, please help to follow up the issue as it is only updated on 2025-04-16 | 
Cannot find email for NamedUser(login="guangyey")
https://github.com/intel/torch-xpu-ops/issues/1324 |  | @guangyey, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/1305 |  | @Stonepia, please help to follow up the issue as it is only updated on 2025-04-02 | 
https://github.com/intel/torch-xpu-ops/issues/1276 |  | @LuFinch, please help to follow up the issue as it is only updated on 2025-03-20 | 
https://github.com/intel/torch-xpu-ops/issues/1261 |  | @tye1, please help to follow up the issue as it is only updated on 2025-04-22 | @chuanqi129, please help to set module:transformers , ,
Cannot find email for NamedUser(login="libohao1201")
https://github.com/intel/torch-xpu-ops/issues/1256 | @libohao1201, please help to improve error_message, reproduce_steps | @Stonepia, please help to follow up the issue as it is only updated on 2025-03-12 | 
https://github.com/intel/torch-xpu-ops/issues/1214 |  |  | @chuanqi129, please help to set module:UT , ,
https://github.com/intel/torch-xpu-ops/issues/1195 |  | @Stonepia, please help to follow up the issue as it is only updated on 2025-04-23 | 
Cannot find email for NamedUser(login="gaopengff")
https://github.com/intel/torch-xpu-ops/issues/1171 |  | @gaopengff, please help to follow up the issue as it is only updated on 2025-04-23 | 
Cannot find email for NamedUser(login="RUIJIEZHONG66166")
https://github.com/intel/torch-xpu-ops/issues/1165 | @dvrogozh, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version, impact | @RUIJIEZHONG66166, please help to follow up the issue as it is only updated on 2025-03-12 | 
https://github.com/intel/torch-xpu-ops/issues/1159 |  | @Stonepia, please help to follow up the issue as it is only updated on 2025-03-12 | @chuanqi129, please help to set module:transformers , ,
https://github.com/intel/torch-xpu-ops/issues/1124 | @daisyden, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version | @daisyden, please help to follow up the issue as it is only updated on 2025-04-22 | 
https://github.com/intel/torch-xpu-ops/issues/1121 | @fengyuan14, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version, impact |  | 
https://github.com/intel/torch-xpu-ops/issues/1059 | @fengyuan14, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version |  | 
https://github.com/intel/torch-xpu-ops/issues/1016 | @fengyuan14, please help to improve error_message, reproduce_steps, reproduce_platform | @majing921201, please help to follow up the issue as it is only updated on 2025-03-12 | 
https://github.com/intel/torch-xpu-ops/issues/1005 | @riverliuintel, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version, impact |  | 
https://github.com/intel/torch-xpu-ops/issues/970 | @fengyuan14, please help to improve error_message, reproduce_steps, reproduce_platform | @majing921201, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/969 | @fengyuan14, please help to improve reproduce_steps, reproduce_platform | @majing921201, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/964 | @xytintel, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version, impact | @daisyden, please help to follow up the issue as it is only updated on 2025-03-12 | 
https://github.com/intel/torch-xpu-ops/issues/954 | @gglin001, please help to improve reproduce_steps, reproduce_platform, reproduce_software_version, impact |  | 
https://github.com/intel/torch-xpu-ops/issues/781 |  | @daisyden, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/772 | @PenghuiCheng, please help to improve reproduce_platform, reproduce_software_version | @ZhiweiYan-96, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/761 | @PenghuiCheng, please help to improve reproduce_platform, reproduce_software_version |  | 
https://github.com/intel/torch-xpu-ops/issues/754 | @daisyden, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version, impact | @daisyden, please help to follow up the issue as it is only updated on 2025-04-22 | @chuanqi129, please help to set module:distributed , ,
https://github.com/intel/torch-xpu-ops/issues/725 |  | @retonym, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/711 |  | @ZhiweiYan-96, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/685 | @fengyuan14, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version, impact | @xytintel, please help to follow up the issue as it is only updated on 2025-03-19 | 
https://github.com/intel/torch-xpu-ops/issues/544 |  |  | @chuanqi129, please help to set module:UT , ,
https://github.com/intel/torch-xpu-ops/issues/506 | @mengfei25, please help to improve impact |  | @chuanqi129, please help to set module:quant , ,
https://github.com/intel/torch-xpu-ops/issues/489 |  | @weishi-deng, please help to follow up the issue as it is only updated on 2025-05-08 | 
https://github.com/intel/torch-xpu-ops/issues/461 | @fengyuan14, please help to improve error_message, reproduce_steps, reproduce_platform, reproduce_software_version, impact |  | 
https://github.com/intel/torch-xpu-ops/issues/322 | @yuchengliu1, please help to improve error_message, reproduce_steps, reproduce_software_version | @liangan1, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/275 |  | @ZhiweiYan-96, please help to follow up the issue as it is only updated on 2025-04-23 | 
https://github.com/intel/torch-xpu-ops/issues/184 |  | @huaiyuzh, please help to follow up the issue as it is only updated on 2025-04-23 | 
> /home/sdp/daisyden/ds/opea_rag/ci2/issue_checker/issue_checker_report.py(132)<module>()
-> print("mail to {}".format("; ".join(mailing)))
(Pdb) mail to yutao.xu@intel.com; penghui.cheng@intel.com; chuanqi.wang@intel.com; chuanqi.wang@intel.com; yutao.xu@intel.com; daisy.deng@intel.com; daisy.deng@intel.com; dmitry.v.rogozhkin@intel.com; penghui.cheng@intel.com; mengfei.li@Intel.com; fengqing.lu@intel.com; mengfei.li@Intel.com; jianyi.zhang@intel.com; yutao.xu@intel.com; yutao.xu@intel.com; yutao.xu@intel.com; yutao.xu@intel.com; penghui.cheng@intel.com; zhiwei.yan@intel.com; penghui.cheng@intel.com; penghui.cheng@intel.com; penghui.cheng@intel.com; yutao.xu@intel.com; penghui.cheng@intel.com; liangang.zhang@intel.com; huaiyu.zheng@intel.com; tong.su@intel.com; penghui.cheng@intel.com; chuanqi.wang@intel.com; chuanqi.wang@intel.com; zhiwei.yan@intel.com; chuanqi.wang@intel.com; chuanqi.wang@intel.com; tong.su@intel.com; fengqing.lu@intel.com; huaiyu.zheng@intel.com; huaiyu.zheng@intel.com; tong.su@intel.com; tong.su@intel.com; tong.su@intel.com; fengqing.lu@intel.com; ting.ye@intel.com; chuanqi.wang@intel.com; tong.su@intel.com; chuanqi.wang@intel.com; tong.su@intel.com; dmitry.v.rogozhkin@intel.com; tong.su@intel.com; chuanqi.wang@intel.com; daisy.deng@intel.com; daisy.deng@intel.com; feng1.yuan@intel.com; feng1.yuan@intel.com; feng1.yuan@intel.com; jing1.ma@intel.com; river.liu@intel.com; feng1.yuan@intel.com; jing1.ma@intel.com; feng1.yuan@intel.com; jing1.ma@intel.com; yutao.xu@intel.com; daisy.deng@intel.com; guosonglin001@gmail.com; daisy.deng@intel.com; penghui.cheng@intel.com; zhiwei.yan@intel.com; penghui.cheng@intel.com; daisy.deng@intel.com; daisy.deng@intel.com; chuanqi.wang@intel.com; retonym@163.com; zhiwei.yan@intel.com; feng1.yuan@intel.com; yutao.xu@intel.com; chuanqi.wang@intel.com; mengfei.li@Intel.com; chuanqi.wang@intel.com; weishi.deng@intel.com; feng1.yuan@intel.com; yucheng.liu@intel.com; liangang.zhang@intel.com; zhiwei.yan@intel.com; huaiyu.zheng@intel.com
